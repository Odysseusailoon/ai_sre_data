{
    "agent": "gpt-w-shell",
    "session_id": "801a3d1a-ab06-4c82-9f6a-225bbb7b2356",
    "problem_id": "astronomy_shop_payment_service_unreachable-detection-1",
    "start_time": 1755635913.778904,
    "end_time": 1755635967.8950999,
    "trace": [
        {
            "role": "assistant",
            "content": "```\nexec_shell(\"kubectl get pods -n astronomy-shop -o wide\")\n```"
        },
        {
            "role": "env",
            "content": "NAME                               READY   STATUS     RESTARTS      AGE   IP             NODE          NOMINATED NODE   READINESS GATES\naccounting-567f87bbcd-4rbgt        1/1     Running    0             38s   10.244.1.107   kind-worker   <none>           <none>\nad-856c77f595-j2xr8                1/1     Running    0             37s   10.244.1.112   kind-worker   <none>           <none>\ncart-6fd857c656-g5j2s              1/1     Running    0             35s   10.244.1.120   kind-worker   <none>           <none>\ncheckout-5bc54f8cd8-z5n4j          1/1     Running    0             38s   10.244.1.109   kind-worker   <none>           <none>\ncurrency-5864f94d89-vpsr2          1/1     Running    0             37s   10.244.1.111   kind-worker   <none>           <none>\nemail-59d9ffc758-ftjzb             1/1     Running    0             38s   10.244.1.100   kind-worker   <none>           <none>\nflagd-58559659bc-z6ncw             0/2     Init:0/1   0             1s    <none>         kind-worker   <none>           <none>\nflagd-86575d5796-r2dsg             2/2     Running    0             35s   10.244.1.119   kind-worker   <none>           <none>\nfraud-detection-65868bcdb5-q2zwn   1/1     Running    0             38s   10.244.1.99    kind-worker   <none>           <none>\nfrontend-7d6b77c58-2n24s           1/1     Running    0             36s   10.244.1.114   kind-worker   <none>           <none>\nfrontend-proxy-548b4649d7-tq64b    1/1     Running    0             38s   10.244.1.110   kind-worker   <none>           <none>\ngrafana-6b9fb84fb7-clpz2           1/1     Running    0             38s   10.244.1.102   kind-worker   <none>           <none>\nimage-provider-7f5bc6bf-pnwzj      1/1     Running    0             38s   10.244.1.105   kind-worker   <none>           <none>\njaeger-55f499595b-qwqn2            1/1     Running    0             36s   10.244.1.115   kind-worker   <none>           <none>\nkafka-595d889c56-c4dmq             1/1     Running    0             36s   10.244.1.118   kind-worker   <none>           <none>\nload-generator-68b6f899f5-fss4x    1/1     Running    0             38s   10.244.1.98    kind-worker   <none>           <none>\nopensearch-0                       1/1     Running    0             38s   10.244.1.104   kind-worker   <none>           <none>\notel-collector-758b8784c-4qbhv     1/1     Running    0             36s   10.244.1.117   kind-worker   <none>           <none>\npayment-7597577cb5-bt9k6           1/1     Running    0             36s   10.244.1.116   kind-worker   <none>           <none>\nproduct-catalog-6988cd5f49-8hpdq   1/1     Running    1 (21s ago)   38s   10.244.1.101   kind-worker   <none>           <none>\nprometheus-58b7bfb774-nhlbb        1/1     Running    0             35s   10.244.1.121   kind-worker   <none>           <none>\nquote-78bf8788-lxb2m               1/1     Running    0             38s   10.244.1.106   kind-worker   <none>           <none>\nrecommendation-7b49c8cb75-dbwlm    1/1     Running    0             38s   10.244.1.103   kind-worker   <none>           <none>\nshipping-7f445d59dc-bjpkx          1/1     Running    0             37s   10.244.1.113   kind-worker   <none>           <none>\nvalkey-cart-6475dcc67f-g7fbr       1/1     Running    0             38s   10.244.1.108   kind-worker   <none>           <none>\n"
        },
        {
            "role": "assistant",
            "content": "```\nexec_shell(\"kubectl describe pod flagd-58559659bc-z6ncw -n astronomy-shop\")\n```"
        },
        {
            "role": "env",
            "content": "Name:             flagd-58559659bc-z6ncw\nNamespace:        astronomy-shop\nPriority:         0\nService Account:  astronomy-shop\nNode:             kind-worker/192.168.97.2\nStart Time:       Tue, 19 Aug 2025 20:38:33 +0000\nLabels:           app.kubernetes.io/component=flagd\n                  app.kubernetes.io/name=flagd\n                  opentelemetry.io/name=flagd\n                  pod-template-hash=58559659bc\nAnnotations:      kubectl.kubernetes.io/restartedAt: 2025-08-19T13:38:33-07:00\nStatus:           Running\nIP:               10.244.1.122\nIPs:\n  IP:           10.244.1.122\nControlled By:  ReplicaSet/flagd-58559659bc\nInit Containers:\n  init-config:\n    Container ID:  containerd://e2e1d0b12ccec04ad70ff926bad0b7b42b65508cb475337ada18966fb102068f\n    Image:         busybox\n    Image ID:      docker.io/library/busybox@sha256:ab33eacc8251e3807b85bb6dba570e4698c3998eca6f0fc2ccb60575a563ea74\n    Port:          <none>\n    Host Port:     <none>\n    Command:\n      sh\n      -c\n      cp /config-ro/demo.flagd.json /config-rw/demo.flagd.json && cat /config-rw/demo.flagd.json\n    State:          Terminated\n      Reason:       Completed\n      Exit Code:    0\n      Started:      Tue, 19 Aug 2025 20:38:35 +0000\n      Finished:     Tue, 19 Aug 2025 20:38:35 +0000\n    Ready:          True\n    Restart Count:  0\n    Environment:    <none>\n    Mounts:\n      /config-ro from config-ro (rw)\n      /config-rw from config-rw (rw)\n      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-j64dl (ro)\nContainers:\n  flagd:\n    Container ID:  containerd://a4811e9b2102d80c3d16a0185a75fd61a10f8beea630b29156b4542ab8d00b70\n    Image:         ghcr.io/open-feature/flagd:v0.12.8\n    Image ID:      ghcr.io/open-feature/flagd@sha256:634bd05995f5fd0a1b8196822feb8c2d99d37be9e2a25cb17b5d6c4c8fd7e821\n    Ports:         8013/TCP, 8016/TCP\n    Host Ports:    0/TCP, 0/TCP\n    Command:\n      /flagd-build\n      start\n      --port\n      8013\n      --ofrep-port\n      8016\n      --uri\n      file:./etc/flagd/demo.flagd.json\n    State:          Running\n      Started:      Tue, 19 Aug 2025 20:38:36 +0000\n    Ready:          True\n    Restart Count:  0\n    Limits:\n      memory:  75Mi\n    Requests:\n      memory:  75Mi\n    Environment:\n      OTEL_SERVICE_NAME:                                   (v1:metadata.labels['app.kubernetes.io/component'])\n      OTEL_COLLECTOR_NAME:                                otel-collector\n      OTEL_EXPORTER_OTLP_METRICS_TEMPORALITY_PREFERENCE:  cumulative\n      FLAGD_METRICS_EXPORTER:                             otel\n      FLAGD_OTEL_COLLECTOR_URI:                           $(OTEL_COLLECTOR_NAME):4317\n      OTEL_RESOURCE_ATTRIBUTES:                           service.name=$(OTEL_SERVICE_NAME),service.namespace=opentelemetry-demo,service.version=2.0.2\n    Mounts:\n      /etc/flagd from config-rw (rw)\n      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-j64dl (ro)\n  flagd-ui:\n    Container ID:   containerd://d05a51e93ab63a0286f7fb48b1e4e9d30e25177d5f81e8e2e88c44d1b73b9f59\n    Image:          ghcr.io/open-telemetry/demo:2.0.2-flagd-ui\n    Image ID:       ghcr.io/open-telemetry/demo@sha256:655da6a8735b7bb7da2c36c8992387d03ae06420f2a06fbc499f2591889c1d03\n    Port:           4000/TCP\n    Host Port:      0/TCP\n    State:          Running\n      Started:      Tue, 19 Aug 2025 20:38:37 +0000\n    Ready:          True\n    Restart Count:  0\n    Limits:\n      memory:  100Mi\n    Requests:\n      memory:  100Mi\n    Environment:\n      OTEL_SERVICE_NAME:                                   (v1:metadata.labels['app.kubernetes.io/component'])\n      OTEL_COLLECTOR_NAME:                                otel-collector\n      OTEL_EXPORTER_OTLP_METRICS_TEMPORALITY_PREFERENCE:  cumulative\n      FLAGD_METRICS_EXPORTER:                             otel\n      OTEL_EXPORTER_OTLP_ENDPOINT:                        http://$(OTEL_COLLECTOR_NAME):4318\n      OTEL_RESOURCE_ATTRIBUTES:                           service.name=$(OTEL_SERVICE_NAME),service.namespace=opentelemetry-demo,service.version=2.0.2\n    Mounts:\n      /app/data from config-rw (rw)\n      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-j64dl (ro)\nConditions:\n  Type                        Status\n  PodReadyToStartContainers   True \n  Initialized                 True \n  Ready                       True \n  ContainersReady             True \n  PodScheduled                True \nVolumes:\n  config-rw:\n    Type:       EmptyDir (a temporary directory that shares a pod's lifetime)\n    Medium:     \n    SizeLimit:  <unset>\n  config-ro:\n    Type:      ConfigMap (a volume populated by a ConfigMap)\n    Name:      flagd-config\n    Optional:  false\n  kube-api-access-j64dl:\n    Type:                    Projected (a volume that contains injected data from multiple sources)\n    TokenExpirationSeconds:  3607\n    ConfigMapName:           kube-root-ca.crt\n    ConfigMapOptional:       <nil>\n    DownwardAPI:             true\nQoS Class:                   Burstable\nNode-Selectors:              <none>\nTolerations:                 node.kubernetes.io/not-ready:NoExecute op=Exists for 300s\n                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s\nEvents:\n  Type    Reason     Age   From               Message\n  ----    ------     ----  ----               -------\n  Normal  Scheduled  5s    default-scheduler  Successfully assigned astronomy-shop/flagd-58559659bc-z6ncw to kind-worker\n  Normal  Pulling    5s    kubelet            Pulling image \"busybox\"\n  Normal  Pulled     4s    kubelet            Successfully pulled image \"busybox\" in 948ms (948ms including waiting). Image size: 1913387 bytes.\n  Normal  Created    4s    kubelet            Created container: init-config\n  Normal  Started    4s    kubelet            Started container init-config\n  Normal  Pulled     3s    kubelet            Container image \"ghcr.io/open-feature/flagd:v0.12.8\" already present on machine\n  Normal  Created    3s    kubelet            Created container: flagd\n  Normal  Started    3s    kubelet            Started container flagd\n  Normal  Pulled     3s    kubelet            Container image \"ghcr.io/open-telemetry/demo:2.0.2-flagd-ui\" already present on machine\n  Normal  Created    3s    kubelet            Created container: flagd-ui\n  Normal  Started    2s    kubelet            Started container flagd-ui\n"
        },
        {
            "role": "assistant",
            "content": "```\nexec_shell(\"kubectl logs product-catalog-6988cd5f49-8hpdq -n astronomy-shop --all-containers --tail=200\")\n```"
        },
        {
            "role": "env",
            "content": "time=\"2025-08-19T20:38:14Z\" level=info msg=\"Loading Product Catalog...\"\ntime=\"2025-08-19T20:38:14Z\" level=info msg=\"Loaded 10 products\"\ntime=\"2025-08-19T20:38:14Z\" level=info msg=\"Product Catalog reload interval: 10\"\ntime=\"2025-08-19T20:38:14Z\" level=info msg=\"Product Catalog gRPC server started on port: 8080\"\ntime=\"2025-08-19T20:38:25Z\" level=info msg=\"Reloading Product Catalog...\"\n2025/08/19 20:38:45 traces export: context deadline exceeded: rpc error: code = DeadlineExceeded desc = context deadline exceeded\n"
        },
        {
            "role": "assistant",
            "content": "```\nexec_shell(\"kubectl logs otel-collector-758b8784c-4qbhv -n astronomy-shop --tail=200\")\n```"
        },
        {
            "role": "env",
            "content": "\tgo.opentelemetry.io/collector/exporter@v0.131.0/exporterhelper/internal/sender/sender.go:31\ngo.opentelemetry.io/collector/exporter/exporterhelper/internal.(*retrySender).Send\n\tgo.opentelemetry.io/collector/exporter@v0.131.0/exporterhelper/internal/retry_sender.go:91\ngo.opentelemetry.io/collector/exporter/exporterhelper/internal.(*obsReportSender[...]).Send\n\tgo.opentelemetry.io/collector/exporter@v0.131.0/exporterhelper/internal/obs_report_sender.go:92\ngo.opentelemetry.io/collector/exporter/exporterhelper/internal.(*BaseExporter).Send\n\tgo.opentelemetry.io/collector/exporter@v0.131.0/exporterhelper/internal/base_exporter.go:115\ngo.opentelemetry.io/collector/exporter/exporterhelper.NewLogsRequest.newConsumeLogs.func1\n\tgo.opentelemetry.io/collector/exporter@v0.131.0/exporterhelper/logs.go:191\ngo.opentelemetry.io/collector/consumer.ConsumeLogsFunc.ConsumeLogs\n\tgo.opentelemetry.io/collector/consumer@v1.37.0/logs.go:27\ngo.opentelemetry.io/collector/internal/fanoutconsumer.(*logsConsumer).ConsumeLogs\n\tgo.opentelemetry.io/collector/internal/fanoutconsumer@v0.131.0/logs.go:64\ngo.opentelemetry.io/collector/processor/batchprocessor.(*batchLogs).export\n\tgo.opentelemetry.io/collector/processor/batchprocessor@v0.131.0/batch_processor.go:543\ngo.opentelemetry.io/collector/processor/batchprocessor.(*shard[...]).sendItems\n\tgo.opentelemetry.io/collector/processor/batchprocessor@v0.131.0/batch_processor.go:262\ngo.opentelemetry.io/collector/processor/batchprocessor.(*shard[...]).startLoop\n\tgo.opentelemetry.io/collector/processor/batchprocessor@v0.131.0/batch_processor.go:222\n2025-08-19T20:38:15.207Z\terror\tinternal/base_exporter.go:117\tExporting failed. Rejecting data. Try enabling sending_queue to survive temporary failures.\t{\"resource\": {\"service.instance.id\": \"10892f14-25e6-4272-9286-200686bd5fca\", \"service.name\": \"otelcol-contrib\", \"service.version\": \"0.131.0\"}, \"otelcol.component.id\": \"opensearch\", \"otelcol.component.kind\": \"exporter\", \"otelcol.signal\": \"logs\", \"error\": \"not retryable error: Permanent error: Permanent error: flush: dial tcp 10.96.144.4:9200: connect: connection refused\\nPermanent error: Permanent error: flush: dial tcp 10.96.144.4:9200: connect: connection refused\", \"rejected_items\": 679}\ngo.opentelemetry.io/collector/exporter/exporterhelper/internal.(*BaseExporter).Send\n\tgo.opentelemetry.io/collector/exporter@v0.131.0/exporterhelper/internal/base_exporter.go:117\ngo.opentelemetry.io/collector/exporter/exporterhelper.NewLogsRequest.newConsumeLogs.func1\n\tgo.opentelemetry.io/collector/exporter@v0.131.0/exporterhelper/logs.go:191\ngo.opentelemetry.io/collector/consumer.ConsumeLogsFunc.ConsumeLogs\n\tgo.opentelemetry.io/collector/consumer@v1.37.0/logs.go:27\ngo.opentelemetry.io/collector/internal/fanoutconsumer.(*logsConsumer).ConsumeLogs\n\tgo.opentelemetry.io/collector/internal/fanoutconsumer@v0.131.0/logs.go:64\ngo.opentelemetry.io/collector/processor/batchprocessor.(*batchLogs).export\n\tgo.opentelemetry.io/collector/processor/batchprocessor@v0.131.0/batch_processor.go:543\ngo.opentelemetry.io/collector/processor/batchprocessor.(*shard[...]).sendItems\n\tgo.opentelemetry.io/collector/processor/batchprocessor@v0.131.0/batch_processor.go:262\ngo.opentelemetry.io/collector/processor/batchprocessor.(*shard[...]).startLoop\n\tgo.opentelemetry.io/collector/processor/batchprocessor@v0.131.0/batch_processor.go:222\n2025-08-19T20:38:15.208Z\tinfo\tLogs\t{\"resource\": {\"service.instance.id\": \"10892f14-25e6-4272-9286-200686bd5fca\", \"service.name\": \"otelcol-contrib\", \"service.version\": \"0.131.0\"}, \"otelcol.component.id\": \"debug\", \"otelcol.component.kind\": \"exporter\", \"otelcol.signal\": \"logs\", \"resource logs\": 32, \"log records\": 679}\n2025-08-19T20:38:15.208Z\twarn\tbatchprocessor@v0.131.0/batch_processor.go:264\tSender failed\t{\"resource\": {\"service.instance.id\": \"10892f14-25e6-4272-9286-200686bd5fca\", \"service.name\": \"otelcol-contrib\", \"service.version\": \"0.131.0\"}, \"otelcol.component.id\": \"batch\", \"otelcol.component.kind\": \"processor\", \"otelcol.pipeline.id\": \"logs\", \"otelcol.signal\": \"logs\", \"error\": \"not retryable error: Permanent error: Permanent error: flush: dial tcp 10.96.144.4:9200: connect: connection refused\\nPermanent error: Permanent error: flush: dial tcp 10.96.144.4:9200: connect: connection refused\"}\n2025-08-19T20:38:16.415Z\terror\topensearchexporter@v0.131.0/logger.go:36\tRequest failed.\t{\"resource\": {\"service.instance.id\": \"10892f14-25e6-4272-9286-200686bd5fca\", \"service.name\": \"otelcol-contrib\", \"service.version\": \"0.131.0\"}, \"otelcol.component.id\": \"opensearch\", \"otelcol.component.kind\": \"exporter\", \"otelcol.signal\": \"logs\", \"path\": \"/_bulk\", \"method\": \"POST\", \"duration\": 1.003760146, \"reason\": \"dial tcp 10.96.144.4:9200: connect: connection refused\"}\ngithub.com/open-telemetry/opentelemetry-collector-contrib/exporter/opensearchexporter.(*clientLogger).LogRoundTrip\n\tgithub.com/open-telemetry/opentelemetry-collector-contrib/exporter/opensearchexporter@v0.131.0/logger.go:36\ngithub.com/opensearch-project/opensearch-go/v2/opensearchtransport.(*Client).logRoundTrip\n\tgithub.com/opensearch-project/opensearch-go/v2@v2.3.0/opensearchtransport/opensearchtransport.go:498\ngithub.com/opensearch-project/opensearch-go/v2/opensearchtransport.(*Client).Perform\n\tgithub.com/opensearch-project/opensearch-go/v2@v2.3.0/opensearchtransport/opensearchtransport.go:337\ngithub.com/opensearch-project/opensearch-go/v2.(*Client).Perform\n\tgithub.com/opensearch-project/opensearch-go/v2@v2.3.0/opensearch.go:267\ngithub.com/opensearch-project/opensearch-go/v2/opensearchapi.BulkRequest.Do\n\tgithub.com/opensearch-project/opensearch-go/v2@v2.3.0/opensearchapi/api.bulk.go:192\ngithub.com/opensearch-project/opensearch-go/v2/opensearchutil.(*worker).flush\n\tgithub.com/opensearch-project/opensearch-go/v2@v2.3.0/opensearchutil/bulk_indexer.go:552\ngithub.com/opensearch-project/opensearch-go/v2/opensearchutil.(*bulkIndexer).Close\n\tgithub.com/opensearch-project/opensearch-go/v2@v2.3.0/opensearchutil/bulk_indexer.go:299\ngithub.com/open-telemetry/opentelemetry-collector-contrib/exporter/opensearchexporter.(*logBulkIndexer).close\n\tgithub.com/open-telemetry/opentelemetry-collector-contrib/exporter/opensearchexporter@v0.131.0/log_bulk_indexer.go:41\ngithub.com/open-telemetry/opentelemetry-collector-contrib/exporter/opensearchexporter.(*logExporter).pushLogData\n\tgithub.com/open-telemetry/opentelemetry-collector-contrib/exporter/opensearchexporter@v0.131.0/sso_log_exporter.go:88\ngo.opentelemetry.io/collector/consumer.ConsumeLogsFunc.ConsumeLogs\n\tgo.opentelemetry.io/collector/consumer@v1.37.0/logs.go:27\ngo.opentelemetry.io/collector/exporter/exporterhelper.NewLogs.requestConsumeFromLogs.func2\n\tgo.opentelemetry.io/collector/exporter@v0.131.0/exporterhelper/logs.go:136\ngo.opentelemetry.io/collector/exporter/exporterhelper/internal/sender.(*sender[...]).Send\n\tgo.opentelemetry.io/collector/exporter@v0.131.0/exporterhelper/internal/sender/sender.go:31\ngo.opentelemetry.io/collector/exporter/exporterhelper/internal.(*retrySender).Send\n\tgo.opentelemetry.io/collector/exporter@v0.131.0/exporterhelper/internal/retry_sender.go:91\ngo.opentelemetry.io/collector/exporter/exporterhelper/internal.(*obsReportSender[...]).Send\n\tgo.opentelemetry.io/collector/exporter@v0.131.0/exporterhelper/internal/obs_report_sender.go:92\ngo.opentelemetry.io/collector/exporter/exporterhelper/internal.(*BaseExporter).Send\n\tgo.opentelemetry.io/collector/exporter@v0.131.0/exporterhelper/internal/base_exporter.go:115\ngo.opentelemetry.io/collector/exporter/exporterhelper.NewLogsRequest.newConsumeLogs.func1\n\tgo.opentelemetry.io/collector/exporter@v0.131.0/exporterhelper/logs.go:191\ngo.opentelemetry.io/collector/consumer.ConsumeLogsFunc.ConsumeLogs\n\tgo.opentelemetry.io/collector/consumer@v1.37.0/logs.go:27\ngo.opentelemetry.io/collector/internal/fanoutconsumer.(*logsConsumer).ConsumeLogs\n\tgo.opentelemetry.io/collector/internal/fanoutconsumer@v0.131.0/logs.go:64\ngo.opentelemetry.io/collector/processor/batchprocessor.(*batchLogs).export\n\tgo.opentelemetry.io/collector/processor/batchprocessor@v0.131.0/batch_processor.go:543\ngo.opentelemetry.io/collector/processor/batchprocessor.(*shard[...]).sendItems\n\tgo.opentelemetry.io/collector/processor/batchprocessor@v0.131.0/batch_processor.go:262\ngo.opentelemetry.io/collector/processor/batchprocessor.(*shard[...]).startLoop\n\tgo.opentelemetry.io/collector/processor/batchprocessor@v0.131.0/batch_processor.go:222\n2025-08-19T20:38:16.415Z\terror\tinternal/base_exporter.go:117\tExporting failed. Rejecting data. Try enabling sending_queue to survive temporary failures.\t{\"resource\": {\"service.instance.id\": \"10892f14-25e6-4272-9286-200686bd5fca\", \"service.name\": \"otelcol-contrib\", \"service.version\": \"0.131.0\"}, \"otelcol.component.id\": \"opensearch\", \"otelcol.component.kind\": \"exporter\", \"otelcol.signal\": \"logs\", \"error\": \"not retryable error: Permanent error: Permanent error: flush: dial tcp 10.96.144.4:9200: connect: connection refused\\nPermanent error: Permanent error: flush: dial tcp 10.96.144.4:9200: connect: connection refused\", \"rejected_items\": 62}\ngo.opentelemetry.io/collector/exporter/exporterhelper/internal.(*BaseExporter).Send\n\tgo.opentelemetry.io/collector/exporter@v0.131.0/exporterhelper/internal/base_exporter.go:117\ngo.opentelemetry.io/collector/exporter/exporterhelper.NewLogsRequest.newConsumeLogs.func1\n\tgo.opentelemetry.io/collector/exporter@v0.131.0/exporterhelper/logs.go:191\ngo.opentelemetry.io/collector/consumer.ConsumeLogsFunc.ConsumeLogs\n\tgo.opentelemetry.io/collector/consumer@v1.37.0/logs.go:27\ngo.opentelemetry.io/collector/internal/fanoutconsumer.(*logsConsumer).ConsumeLogs\n\tgo.opentelemetry.io/collector/internal/fanoutconsumer@v0.131.0/logs.go:64\ngo.opentelemetry.io/collector/processor/batchprocessor.(*batchLogs).export\n\tgo.opentelemetry.io/collector/processor/batchprocessor@v0.131.0/batch_processor.go:543\ngo.opentelemetry.io/collector/processor/batchprocessor.(*shard[...]).sendItems\n\tgo.opentelemetry.io/collector/processor/batchprocessor@v0.131.0/batch_processor.go:262\ngo.opentelemetry.io/collector/processor/batchprocessor.(*shard[...]).startLoop\n\tgo.opentelemetry.io/collector/processor/batchprocessor@v0.131.0/batch_processor.go:222\n2025-08-19T20:38:16.415Z\tinfo\tLogs\t{\"resource\": {\"service.instance.id\": \"10892f14-25e6-4272-9286-200686bd5fca\", \"service.name\": \"otelcol-contrib\", \"service.version\": \"0.131.0\"}, \"otelcol.component.id\": \"debug\", \"otelcol.component.kind\": \"exporter\", \"otelcol.signal\": \"logs\", \"resource logs\": 15, \"log records\": 62}\n2025-08-19T20:38:16.415Z\twarn\tbatchprocessor@v0.131.0/batch_processor.go:264\tSender failed\t{\"resource\": {\"service.instance.id\": \"10892f14-25e6-4272-9286-200686bd5fca\", \"service.name\": \"otelcol-contrib\", \"service.version\": \"0.131.0\"}, \"otelcol.component.id\": \"batch\", \"otelcol.component.kind\": \"processor\", \"otelcol.pipeline.id\": \"logs\", \"otelcol.signal\": \"logs\", \"error\": \"not retryable error: Permanent error: Permanent error: flush: dial tcp 10.96.144.4:9200: connect: connection refused\\nPermanent error: Permanent error: flush: dial tcp 10.96.144.4:9200: connect: connection refused\"}\n2025-08-19T20:38:17.568Z\tinfo\tinternal/retry_sender.go:133\tExporting failed. Will retry the request after interval.\t{\"resource\": {\"service.instance.id\": \"10892f14-25e6-4272-9286-200686bd5fca\", \"service.name\": \"otelcol-contrib\", \"service.version\": \"0.131.0\"}, \"otelcol.component.id\": \"otlphttp/prometheus\", \"otelcol.component.kind\": \"exporter\", \"otelcol.signal\": \"metrics\", \"error\": \"failed to make an HTTP request: Post \\\"http://prometheus:9090/api/v1/otlp/v1/metrics\\\": dial tcp 10.96.158.72:9090: connect: connection refused\", \"interval\": \"3.364493933s\"}\n2025-08-19T20:38:17.780Z\twarn\tgrpc@v1.74.2/clientconn.go:1414\t[core] [Channel #1 SubChannel #36]grpc: addrConn.createTransport failed to connect to {Addr: \"10.96.231.80:4318\", ServerName: \"otel-collector:4318\", }. Err: connection error: desc = \"error reading server preface: http2: failed reading the frame payload: http2: frame too large, note that the frame header looked like an HTTP/1.1 header\"\t{\"resource\": {\"service.instance.id\": \"10892f14-25e6-4272-9286-200686bd5fca\", \"service.name\": \"otelcol-contrib\", \"service.version\": \"0.131.0\"}, \"grpc_log\": true}\n2025-08-19T20:38:17.946Z\tinfo\tLogs\t{\"resource\": {\"service.instance.id\": \"10892f14-25e6-4272-9286-200686bd5fca\", \"service.name\": \"otelcol-contrib\", \"service.version\": \"0.131.0\"}, \"otelcol.component.id\": \"debug\", \"otelcol.component.kind\": \"exporter\", \"otelcol.signal\": \"logs\", \"resource logs\": 11, \"log records\": 51}\n2025-08-19T20:38:18.263Z\tinfo\tLogs\t{\"resource\": {\"service.instance.id\": \"10892f14-25e6-4272-9286-200686bd5fca\", \"service.name\": \"otelcol-contrib\", \"service.version\": \"0.131.0\"}, \"otelcol.component.id\": \"debug\", \"otelcol.component.kind\": \"exporter\", \"otelcol.signal\": \"logs\", \"resource logs\": 20, \"log records\": 89}\n2025-08-19T20:38:18.519Z\tinfo\tinternal/retry_sender.go:133\tExporting failed. Will retry the request after interval.\t{\"resource\": {\"service.instance.id\": \"10892f14-25e6-4272-9286-200686bd5fca\", \"service.name\": \"otelcol-contrib\", \"service.version\": \"0.131.0\"}, \"otelcol.component.id\": \"otlphttp/prometheus\", \"otelcol.component.kind\": \"exporter\", \"otelcol.signal\": \"metrics\", \"error\": \"failed to make an HTTP request: Post \\\"http://prometheus:9090/api/v1/otlp/v1/metrics\\\": dial tcp 10.96.158.72:9090: connect: connection refused\", \"interval\": \"7.942479992s\"}\n2025-08-19T20:38:18.564Z\tinfo\tLogs\t{\"resource\": {\"service.instance.id\": \"10892f14-25e6-4272-9286-200686bd5fca\", \"service.name\": \"otelcol-contrib\", \"service.version\": \"0.131.0\"}, \"otelcol.component.id\": \"debug\", \"otelcol.component.kind\": \"exporter\", \"otelcol.signal\": \"logs\", \"resource logs\": 3, \"log records\": 20}\n2025-08-19T20:38:18.802Z\tinfo\tLogs\t{\"resource\": {\"service.instance.id\": \"10892f14-25e6-4272-9286-200686bd5fca\", \"service.name\": \"otelcol-contrib\", \"service.version\": \"0.131.0\"}, \"otelcol.component.id\": \"debug\", \"otelcol.component.kind\": \"exporter\", \"otelcol.signal\": \"logs\", \"resource logs\": 12, \"log records\": 89}\n2025-08-19T20:38:19.336Z\tinfo\tLogs\t{\"resource\": {\"service.instance.id\": \"10892f14-25e6-4272-9286-200686bd5fca\", \"service.name\": \"otelcol-contrib\", \"service.version\": \"0.131.0\"}, \"otelcol.component.id\": \"debug\", \"otelcol.component.kind\": \"exporter\", \"otelcol.signal\": \"logs\", \"resource logs\": 1, \"log records\": 25}\n2025-08-19T20:38:20.411Z\tinfo\tMetrics\t{\"resource\": {\"service.instance.id\": \"10892f14-25e6-4272-9286-200686bd5fca\", \"service.name\": \"otelcol-contrib\", \"service.version\": \"0.131.0\"}, \"otelcol.component.id\": \"debug\", \"otelcol.component.kind\": \"exporter\", \"otelcol.signal\": \"metrics\", \"resource metrics\": 1, \"metrics\": 29, \"data points\": 34}\n2025-08-19T20:38:21.059Z\tinfo\tTraces\t{\"resource\": {\"service.instance.id\": \"10892f14-25e6-4272-9286-200686bd5fca\", \"service.name\": \"otelcol-contrib\", \"service.version\": \"0.131.0\"}, \"otelcol.component.id\": \"debug\", \"otelcol.component.kind\": \"exporter\", \"otelcol.signal\": \"traces\", \"resource spans\": 3, \"spans\": 28}\n2025-08-19T20:38:21.661Z\tinfo\tTraces\t{\"resource\": {\"service.instance.id\": \"10892f14-25e6-4272-9286-200686bd5fca\", \"service.name\": \"otelcol-contrib\", \"service.version\": \"0.131.0\"}, \"otelcol.component.id\": \"debug\", \"otelcol.component.kind\": \"exporter\", \"otelcol.signal\": \"traces\", \"resource spans\": 3, \"spans\": 15}\n2025-08-19T20:38:21.863Z\tinfo\tTraces\t{\"resource\": {\"service.instance.id\": \"10892f14-25e6-4272-9286-200686bd5fca\", \"service.name\": \"otelcol-contrib\", \"service.version\": \"0.131.0\"}, \"otelcol.component.id\": \"debug\", \"otelcol.component.kind\": \"exporter\", \"otelcol.signal\": \"traces\", \"resource spans\": 7, \"spans\": 78}\n2025-08-19T20:38:21.976Z\tinfo\tLogs\t{\"resource\": {\"service.instance.id\": \"10892f14-25e6-4272-9286-200686bd5fca\", \"service.name\": \"otelcol-contrib\", \"service.version\": \"0.131.0\"}, \"otelcol.component.id\": \"debug\", \"otelcol.component.kind\": \"exporter\", \"otelcol.signal\": \"logs\", \"resource logs\": 10, \"log records\": 20}\n2025-08-19T20:38:22.002Z\tinfo\tinternal/retry_sender.go:133\tExporting failed. Will retry the request after interval.\t{\"resource\": {\"service.instance.id\": \"10892f14-25e6-4272-9286-200686bd5fca\", \"service.name\": \"otelcol-contrib\", \"service.version\": \"0.131.0\"}, \"otelcol.component.id\": \"otlphttp/prometheus\", \"otelcol.component.kind\": \"exporter\", \"otelcol.signal\": \"metrics\", \"error\": \"failed to make an HTTP request: Post \\\"http://prometheus:9090/api/v1/otlp/v1/metrics\\\": dial tcp 10.96.158.72:9090: connect: connection refused\", \"interval\": \"7.909887118s\"}\n2025-08-19T20:38:22.067Z\tinfo\tTraces\t{\"resource\": {\"service.instance.id\": \"10892f14-25e6-4272-9286-200686bd5fca\", \"service.name\": \"otelcol-contrib\", \"service.version\": \"0.131.0\"}, \"otelcol.component.id\": \"debug\", \"otelcol.component.kind\": \"exporter\", \"otelcol.signal\": \"traces\", \"resource spans\": 8, \"spans\": 36}\n2025-08-19T20:38:22.224Z\tinfo\tLogs\t{\"resource\": {\"service.instance.id\": \"10892f14-25e6-4272-9286-200686bd5fca\", \"service.name\": \"otelcol-contrib\", \"service.version\": \"0.131.0\"}, \"otelcol.component.id\": \"debug\", \"otelcol.component.kind\": \"exporter\", \"otelcol.signal\": \"logs\", \"resource logs\": 1, \"log records\": 1}\n2025-08-19T20:38:22.268Z\tinfo\tTraces\t{\"resource\": {\"service.instance.id\": \"10892f14-25e6-4272-9286-200686bd5fca\", \"service.name\": \"otelcol-contrib\", \"service.version\": \"0.131.0\"}, \"otelcol.component.id\": \"debug\", \"otelcol.component.kind\": \"exporter\", \"otelcol.signal\": \"traces\", \"resource spans\": 4, \"spans\": 16}\n2025-08-19T20:38:22.439Z\tinfo\tLogs\t{\"resource\": {\"service.instance.id\": \"10892f14-25e6-4272-9286-200686bd5fca\", \"service.name\": \"otelcol-contrib\", \"service.version\": \"0.131.0\"}, \"otelcol.component.id\": \"debug\", \"otelcol.component.kind\": \"exporter\", \"otelcol.signal\": \"logs\", \"resource logs\": 2, \"log records\": 2}\n2025-08-19T20:38:22.470Z\tinfo\tTraces\t{\"resource\": {\"service.instance.id\": \"10892f14-25e6-4272-9286-200686bd5fca\", \"service.name\": \"otelcol-contrib\", \"service.version\": \"0.131.0\"}, \"otelcol.component.id\": \"debug\", \"otelcol.component.kind\": \"exporter\", \"otelcol.signal\": \"traces\", \"resource spans\": 4, \"spans\": 81}\n2025-08-19T20:38:22.672Z\tinfo\tTraces\t{\"resource\": {\"service.instance.id\": \"10892f14-25e6-4272-9286-200686bd5fca\", \"service.name\": \"otelcol-contrib\", \"service.version\": \"0.131.0\"}, \"otelcol.component.id\": \"debug\", \"otelcol.component.kind\": \"exporter\", \"otelcol.signal\": \"traces\", \"resource spans\": 1, \"spans\": 5}\n2025-08-19T20:38:22.865Z\tinfo\tLogs\t{\"resource\": {\"service.instance.id\": \"10892f14-25e6-4272-9286-200686bd5fca\", \"service.name\": \"otelcol-contrib\", \"service.version\": \"0.131.0\"}, \"otelcol.component.id\": \"debug\", \"otelcol.component.kind\": \"exporter\", \"otelcol.signal\": \"logs\", \"resource logs\": 10, \"log records\": 35}\n2025-08-19T20:38:22.873Z\tinfo\tTraces\t{\"resource\": {\"service.instance.id\": \"10892f14-25e6-4272-9286-200686bd5fca\", \"service.name\": \"otelcol-contrib\", \"service.version\": \"0.131.0\"}, \"otelcol.component.id\": \"debug\", \"otelcol.component.kind\": \"exporter\", \"otelcol.signal\": \"traces\", \"resource spans\": 16, \"spans\": 62}\n2025-08-19T20:38:23.076Z\tinfo\tTraces\t{\"resource\": {\"service.instance.id\": \"10892f14-25e6-4272-9286-200686bd5fca\", \"service.name\": \"otelcol-contrib\", \"service.version\": \"0.131.0\"}, \"otelcol.component.id\": \"debug\", \"otelcol.component.kind\": \"exporter\", \"otelcol.signal\": \"traces\", \"resource spans\": 1, \"spans\": 78}\n2025-08-19T20:38:23.086Z\tinfo\tLogs\t{\"resource\": {\"service.instance.id\": \"10892f14-25e6-4272-9286-200686bd5fca\", \"service.name\": \"otelcol-contrib\", \"service.version\": \"0.131.0\"}, \"otelcol.component.id\": \"debug\", \"otelcol.component.kind\": \"exporter\", \"otelcol.signal\": \"logs\", \"resource logs\": 1, \"log records\": 2}\n2025-08-19T20:38:23.277Z\tinfo\tTraces\t{\"resource\": {\"service.instance.id\": \"10892f14-25e6-4272-9286-200686bd5fca\", \"service.name\": \"otelcol-contrib\", \"service.version\": \"0.131.0\"}, \"otelcol.component.id\": \"debug\", \"otelcol.component.kind\": \"exporter\", \"otelcol.signal\": \"traces\", \"resource spans\": 3, \"spans\": 15}\n2025-08-19T20:38:23.518Z\tinfo\tLogs\t{\"resource\": {\"service.instance.id\": \"10892f14-25e6-4272-9286-200686bd5fca\", \"service.name\": \"otelcol-contrib\", \"service.version\": \"0.131.0\"}, \"otelcol.component.id\": \"debug\", \"otelcol.component.kind\": \"exporter\", \"otelcol.signal\": \"logs\", \"resource logs\": 2, \"log records\": 5}\n2025-08-19T20:38:23.751Z\tinfo\tLogs\t{\"resource\": {\"service.instance.id\": \"10892f14-25e6-4272-9286-200686bd5fca\", \"service.name\": \"otelcol-contrib\", \"service.version\": \"0.131.0\"}, \"otelcol.component.id\": \"debug\", \"otelcol.component.kind\": \"exporter\", \"otelcol.signal\": \"logs\", \"resource logs\": 1, \"log records\": 1}\n2025-08-19T20:38:23.995Z\tinfo\tLogs\t{\"resource\": {\"service.instance.id\": \"10892f14-25e6-4272-9286-200686bd5fca\", \"service.name\": \"otelcol-contrib\", \"service.version\": \"0.131.0\"}, \"otelcol.component.id\": \"debug\", \"otelcol.component.kind\": \"exporter\", \"otelcol.signal\": \"logs\", \"resource logs\": 13, \"log records\": 57}\n2025/08/19 20:38:24 failed to upload metrics: reader collect and export timeout: rpc error: code = Unavailable desc = connection error: desc = \"error reading server preface: http2: failed reading the frame payload: http2: frame too large, note that the frame header looked like an HTTP/1.1 header\"\n2025-08-19T20:38:24.249Z\tinfo\tLogs\t{\"resource\": {\"service.instance.id\": \"10892f14-25e6-4272-9286-200686bd5fca\", \"service.name\": \"otelcol-contrib\", \"service.version\": \"0.131.0\"}, \"otelcol.component.id\": \"debug\", \"otelcol.component.kind\": \"exporter\", \"otelcol.signal\": \"logs\", \"resource logs\": 2, \"log records\": 25}\n2025-08-19T20:38:24.309Z\twarn\tgrpc@v1.74.2/clientconn.go:1414\t[core] [Channel #1 SubChannel #36]grpc: addrConn.createTransport failed to connect to {Addr: \"10.96.231.80:4318\", ServerName: \"otel-collector:4318\", }. Err: connection error: desc = \"error reading server preface: http2: failed reading the frame payload: http2: frame too large, note that the frame header looked like an HTTP/1.1 header\"\t{\"resource\": {\"service.instance.id\": \"10892f14-25e6-4272-9286-200686bd5fca\", \"service.name\": \"otelcol-contrib\", \"service.version\": \"0.131.0\"}, \"grpc_log\": true}\n2025-08-19T20:38:24.864Z\tinfo\tLogs\t{\"resource\": {\"service.instance.id\": \"10892f14-25e6-4272-9286-200686bd5fca\", \"service.name\": \"otelcol-contrib\", \"service.version\": \"0.131.0\"}, \"otelcol.component.id\": \"debug\", \"otelcol.component.kind\": \"exporter\", \"otelcol.signal\": \"logs\", \"resource logs\": 8, \"log records\": 25}\n2025-08-19T20:38:26.381Z\tinfo\tinternal/retry_sender.go:133\tExporting failed. Will retry the request after interval.\t{\"resource\": {\"service.instance.id\": \"10892f14-25e6-4272-9286-200686bd5fca\", \"service.name\": \"otelcol-contrib\", \"service.version\": \"0.131.0\"}, \"otelcol.component.id\": \"otlphttp/prometheus\", \"otelcol.component.kind\": \"exporter\", \"otelcol.signal\": \"metrics\", \"error\": \"failed to make an HTTP request: Post \\\"http://prometheus:9090/api/v1/otlp/v1/metrics\\\": dial tcp 10.96.158.72:9090: connect: connection refused\", \"interval\": \"5.111834009s\"}\n2025-08-19T20:38:26.464Z\tinfo\tinternal/retry_sender.go:133\tExporting failed. Will retry the request after interval.\t{\"resource\": {\"service.instance.id\": \"10892f14-25e6-4272-9286-200686bd5fca\", \"service.name\": \"otelcol-contrib\", \"service.version\": \"0.131.0\"}, \"otelcol.component.id\": \"otlphttp/prometheus\", \"otelcol.component.kind\": \"exporter\", \"otelcol.signal\": \"metrics\", \"error\": \"failed to make an HTTP request: Post \\\"http://prometheus:9090/api/v1/otlp/v1/metrics\\\": dial tcp 10.96.158.72:9090: connect: connection refused\", \"interval\": \"24.001839781s\"}\n2025-08-19T20:38:29.942Z\tinfo\tinternal/retry_sender.go:133\tExporting failed. Will retry the request after interval.\t{\"resource\": {\"service.instance.id\": \"10892f14-25e6-4272-9286-200686bd5fca\", \"service.name\": \"otelcol-contrib\", \"service.version\": \"0.131.0\"}, \"otelcol.component.id\": \"otlphttp/prometheus\", \"otelcol.component.kind\": \"exporter\", \"otelcol.signal\": \"metrics\", \"error\": \"failed to make an HTTP request: Post \\\"http://prometheus:9090/api/v1/otlp/v1/metrics\\\": dial tcp 10.96.158.72:9090: connect: connection refused\", \"interval\": \"13.709319631s\"}\n2025-08-19T20:38:30.492Z\tinfo\tMetrics\t{\"resource\": {\"service.instance.id\": \"10892f14-25e6-4272-9286-200686bd5fca\", \"service.name\": \"otelcol-contrib\", \"service.version\": \"0.131.0\"}, \"otelcol.component.id\": \"debug\", \"otelcol.component.kind\": \"exporter\", \"otelcol.signal\": \"metrics\", \"resource metrics\": 1, \"metrics\": 29, \"data points\": 34}\n2025-08-19T20:38:30.509Z\tinfo\tinternal/retry_sender.go:133\tExporting failed. Will retry the request after interval.\t{\"resource\": {\"service.instance.id\": \"10892f14-25e6-4272-9286-200686bd5fca\", \"service.name\": \"otelcol-contrib\", \"service.version\": \"0.131.0\"}, \"otelcol.component.id\": \"otlphttp/prometheus\", \"otelcol.component.kind\": \"exporter\", \"otelcol.signal\": \"metrics\", \"error\": \"failed to make an HTTP request: Post \\\"http://prometheus:9090/api/v1/otlp/v1/metrics\\\": dial tcp 10.96.158.72:9090: connect: connection refused\", \"interval\": \"7.318336606s\"}\n2025-08-19T20:38:31.115Z\tinfo\tTraces\t{\"resource\": {\"service.instance.id\": \"10892f14-25e6-4272-9286-200686bd5fca\", \"service.name\": \"otelcol-contrib\", \"service.version\": \"0.131.0\"}, \"otelcol.component.id\": \"debug\", \"otelcol.component.kind\": \"exporter\", \"otelcol.signal\": \"traces\", \"resource spans\": 4, \"spans\": 34}\n2025-08-19T20:38:31.319Z\tinfo\tTraces\t{\"resource\": {\"service.instance.id\": \"10892f14-25e6-4272-9286-200686bd5fca\", \"service.name\": \"otelcol-contrib\", \"service.version\": \"0.131.0\"}, \"otelcol.component.id\": \"debug\", \"otelcol.component.kind\": \"exporter\", \"otelcol.signal\": \"traces\", \"resource spans\": 2, \"spans\": 10}\n2025-08-19T20:38:31.526Z\tinfo\tTraces\t{\"resource\": {\"service.instance.id\": \"10892f14-25e6-4272-9286-200686bd5fca\", \"service.name\": \"otelcol-contrib\", \"service.version\": \"0.131.0\"}, \"otelcol.component.id\": \"debug\", \"otelcol.component.kind\": \"exporter\", \"otelcol.signal\": \"traces\", \"resource spans\": 6, \"spans\": 46}\n2025-08-19T20:38:31.532Z\tinfo\tinternal/retry_sender.go:133\tExporting failed. Will retry the request after interval.\t{\"resource\": {\"service.instance.id\": \"10892f14-25e6-4272-9286-200686bd5fca\", \"service.name\": \"otelcol-contrib\", \"service.version\": \"0.131.0\"}, \"otelcol.component.id\": \"otlphttp/prometheus\", \"otelcol.component.kind\": \"exporter\", \"otelcol.signal\": \"metrics\", \"error\": \"failed to make an HTTP request: Post \\\"http://prometheus:9090/api/v1/otlp/v1/metrics\\\": dial tcp 10.96.158.72:9090: connect: connection refused\", \"interval\": \"7.538677401s\"}\n2025-08-19T20:38:31.732Z\tinfo\tTraces\t{\"resource\": {\"service.instance.id\": \"10892f14-25e6-4272-9286-200686bd5fca\", \"service.name\": \"otelcol-contrib\", \"service.version\": \"0.131.0\"}, \"otelcol.component.id\": \"debug\", \"otelcol.component.kind\": \"exporter\", \"otelcol.signal\": \"traces\", \"resource spans\": 3, \"spans\": 11}\n2025-08-19T20:38:31.944Z\tinfo\tTraces\t{\"resource\": {\"service.instance.id\": \"10892f14-25e6-4272-9286-200686bd5fca\", \"service.name\": \"otelcol-contrib\", \"service.version\": \"0.131.0\"}, \"otelcol.component.id\": \"debug\", \"otelcol.component.kind\": \"exporter\", \"otelcol.signal\": \"traces\", \"resource spans\": 1, \"spans\": 44}\n2025-08-19T20:38:31.994Z\tinfo\tLogs\t{\"resource\": {\"service.instance.id\": \"10892f14-25e6-4272-9286-200686bd5fca\", \"service.name\": \"otelcol-contrib\", \"service.version\": \"0.131.0\"}, \"otelcol.component.id\": \"debug\", \"otelcol.component.kind\": \"exporter\", \"otelcol.signal\": \"logs\", \"resource logs\": 9, \"log records\": 41}\n2025-08-19T20:38:32.145Z\tinfo\tTraces\t{\"resource\": {\"service.instance.id\": \"10892f14-25e6-4272-9286-200686bd5fca\", \"service.name\": \"otelcol-contrib\", \"service.version\": \"0.131.0\"}, \"otelcol.component.id\": \"debug\", \"otelcol.component.kind\": \"exporter\", \"otelcol.signal\": \"traces\", \"resource spans\": 1, \"spans\": 21}\n2025-08-19T20:38:32.249Z\tinfo\tLogs\t{\"resource\": {\"service.instance.id\": \"10892f14-25e6-4272-9286-200686bd5fca\", \"service.name\": \"otelcol-contrib\", \"service.version\": \"0.131.0\"}, \"otelcol.component.id\": \"debug\", \"otelcol.component.kind\": \"exporter\", \"otelcol.signal\": \"logs\", \"resource logs\": 1, \"log records\": 2}\n2025-08-19T20:38:32.346Z\tinfo\tTraces\t{\"resource\": {\"service.instance.id\": \"10892f14-25e6-4272-9286-200686bd5fca\", \"service.name\": \"otelcol-contrib\", \"service.version\": \"0.131.0\"}, \"otelcol.component.id\": \"debug\", \"otelcol.component.kind\": \"exporter\", \"otelcol.signal\": \"traces\", \"resource spans\": 5, \"spans\": 25}\n2025-08-19T20:38:32.550Z\tinfo\tTraces\t{\"resource\": {\"service.instance.id\": \"10892f14-25e6-4272-9286-200686bd5fca\", \"service.name\": \"otelcol-contrib\", \"service.version\": \"0.131.0\"}, \"otelcol.component.id\": \"debug\", \"otelcol.component.kind\": \"exporter\", \"otelcol.signal\": \"traces\", \"resource spans\": 3, \"spans\": 23}\n2025-08-19T20:38:32.751Z\tinfo\tTraces\t{\"resource\": {\"service.instance.id\": \"10892f14-25e6-4272-9286-200686bd5fca\", \"service.name\": \"otelcol-contrib\", \"service.version\": \"0.131.0\"}, \"otelcol.component.id\": \"debug\", \"otelcol.component.kind\": \"exporter\", \"otelcol.signal\": \"traces\", \"resource spans\": 20, \"spans\": 75}\n2025-08-19T20:38:32.879Z\tinfo\tLogs\t{\"resource\": {\"service.instance.id\": \"10892f14-25e6-4272-9286-200686bd5fca\", \"service.name\": \"otelcol-contrib\", \"service.version\": \"0.131.0\"}, \"otelcol.component.id\": \"debug\", \"otelcol.component.kind\": \"exporter\", \"otelcol.signal\": \"logs\", \"resource logs\": 9, \"log records\": 33}\n2025-08-19T20:38:32.954Z\tinfo\tTraces\t{\"resource\": {\"service.instance.id\": \"10892f14-25e6-4272-9286-200686bd5fca\", \"service.name\": \"otelcol-contrib\", \"service.version\": \"0.131.0\"}, \"otelcol.component.id\": \"debug\", \"otelcol.component.kind\": \"exporter\", \"otelcol.signal\": \"traces\", \"resource spans\": 8, \"spans\": 40}\n2025-08-19T20:38:33.096Z\tinfo\tLogs\t{\"resource\": {\"service.instance.id\": \"10892f14-25e6-4272-9286-200686bd5fca\", \"service.name\": \"otelcol-contrib\", \"service.version\": \"0.131.0\"}, \"otelcol.component.id\": \"debug\", \"otelcol.component.kind\": \"exporter\", \"otelcol.signal\": \"logs\", \"resource logs\": 2, \"log records\": 2}\n2025-08-19T20:38:33.511Z\tinfo\tLogs\t{\"resource\": {\"service.instance.id\": \"10892f14-25e6-4272-9286-200686bd5fca\", \"service.name\": \"otelcol-contrib\", \"service.version\": \"0.131.0\"}, \"otelcol.component.id\": \"debug\", \"otelcol.component.kind\": \"exporter\", \"otelcol.signal\": \"logs\", \"resource logs\": 1, \"log records\": 7}\n2025-08-19T20:38:33.760Z\tinfo\tLogs\t{\"resource\": {\"service.instance.id\": \"10892f14-25e6-4272-9286-200686bd5fca\", \"service.name\": \"otelcol-contrib\", \"service.version\": \"0.131.0\"}, \"otelcol.component.id\": \"debug\", \"otelcol.component.kind\": \"exporter\", \"otelcol.signal\": \"logs\", \"resource logs\": 1, \"log records\": 1}\n2025-08-19T20:38:34.025Z\tinfo\tLogs\t{\"resource\": {\"service.instance.id\": \"10892f14-25e6-4272-9286-200686bd5fca\", \"service.name\": \"otelcol-contrib\", \"service.version\": \"0.131.0\"}, \"otelcol.component.id\": \"debug\", \"otelcol.component.kind\": \"exporter\", \"otelcol.signal\": \"logs\", \"resource logs\": 11, \"log records\": 72}\n2025/08/19 20:38:34 failed to upload metrics: reader collect and export timeout: rpc error: code = Unavailable desc = connection error: desc = \"error reading server preface: http2: failed reading the frame payload: http2: frame too large, note that the frame header looked like an HTTP/1.1 header\"\n2025-08-19T20:38:34.266Z\tinfo\tLogs\t{\"resource\": {\"service.instance.id\": \"10892f14-25e6-4272-9286-200686bd5fca\", \"service.name\": \"otelcol-contrib\", \"service.version\": \"0.131.0\"}, \"otelcol.component.id\": \"debug\", \"otelcol.component.kind\": \"exporter\", \"otelcol.signal\": \"logs\", \"resource logs\": 1, \"log records\": 27}\n2025-08-19T20:38:34.696Z\tinfo\tLogs\t{\"resource\": {\"service.instance.id\": \"10892f14-25e6-4272-9286-200686bd5fca\", \"service.name\": \"otelcol-contrib\", \"service.version\": \"0.131.0\"}, \"otelcol.component.id\": \"debug\", \"otelcol.component.kind\": \"exporter\", \"otelcol.signal\": \"logs\", \"resource logs\": 2, \"log records\": 2}\n2025-08-19T20:38:34.923Z\tinfo\tLogs\t{\"resource\": {\"service.instance.id\": \"10892f14-25e6-4272-9286-200686bd5fca\", \"service.name\": \"otelcol-contrib\", \"service.version\": \"0.131.0\"}, \"otelcol.component.id\": \"debug\", \"otelcol.component.kind\": \"exporter\", \"otelcol.signal\": \"logs\", \"resource logs\": 8, \"log records\": 46}\n2025-08-19T20:38:35.852Z\twarn\tgrpc@v1.74.2/clientconn.go:1414\t[core] [Channel #1 SubChannel #36]grpc: addrConn.createTransport failed to connect to {Addr: \"10.96.231.80:4318\", ServerName: \"otel-collector:4318\", }. Err: connection error: desc = \"error reading server preface: http2: failed reading the frame payload: http2: frame too large, note that the frame header looked like an HTTP/1.1 header\"\t{\"resource\": {\"service.instance.id\": \"10892f14-25e6-4272-9286-200686bd5fca\", \"service.name\": \"otelcol-contrib\", \"service.version\": \"0.131.0\"}, \"grpc_log\": true}\n2025-08-19T20:38:40.503Z\tinfo\tMetrics\t{\"resource\": {\"service.instance.id\": \"10892f14-25e6-4272-9286-200686bd5fca\", \"service.name\": \"otelcol-contrib\", \"service.version\": \"0.131.0\"}, \"otelcol.component.id\": \"debug\", \"otelcol.component.kind\": \"exporter\", \"otelcol.signal\": \"metrics\", \"resource metrics\": 1, \"metrics\": 29, \"data points\": 34}\n2025-08-19T20:38:41.196Z\tinfo\tTraces\t{\"resource\": {\"service.instance.id\": \"10892f14-25e6-4272-9286-200686bd5fca\", \"service.name\": \"otelcol-contrib\", \"service.version\": \"0.131.0\"}, \"otelcol.component.id\": \"debug\", \"otelcol.component.kind\": \"exporter\", \"otelcol.signal\": \"traces\", \"resource spans\": 5, \"spans\": 38}\n2025-08-19T20:38:41.425Z\tinfo\tTraces\t{\"resource\": {\"service.instance.id\": \"10892f14-25e6-4272-9286-200686bd5fca\", \"service.name\": \"otelcol-contrib\", \"service.version\": \"0.131.0\"}, \"otelcol.component.id\": \"debug\", \"otelcol.component.kind\": \"exporter\", \"otelcol.signal\": \"traces\", \"resource spans\": 2, \"spans\": 42}\n2025-08-19T20:38:41.641Z\tinfo\tTraces\t{\"resource\": {\"service.instance.id\": \"10892f14-25e6-4272-9286-200686bd5fca\", \"service.name\": \"otelcol-contrib\", \"service.version\": \"0.131.0\"}, \"otelcol.component.id\": \"debug\", \"otelcol.component.kind\": \"exporter\", \"otelcol.signal\": \"traces\", \"resource spans\": 1, \"spans\": 5}\n2025-08-19T20:38:41.854Z\tinfo\tTraces\t{\"resource\": {\"service.instance.id\": \"10892f14-25e6-4272-9286-200686bd5fca\", \"service.name\": \"otelcol-contrib\", \"service.version\": \"0.131.0\"}, \"otelcol.component.id\": \"debug\", \"otelcol.component.kind\": \"exporter\", \"otelcol.signal\": \"traces\", \"resource spans\": 1, \"spans\": 20}\n2025-08-19T20:38:42.091Z\tinfo\tTraces\t{\"resource\": {\"service.instance.id\": \"10892f14-25e6-4272-9286-200686bd5fca\", \"service.name\": \"otelcol-contrib\", \"service.version\": \"0.131.0\"}, \"otelcol.component.id\": \"debug\", \"otelcol.component.kind\": \"exporter\", \"otelcol.signal\": \"traces\", \"resource spans\": 2, \"spans\": 39}\n2025-08-19T20:38:42.083Z\tinfo\tLogs\t{\"resource\": {\"service.instance.id\": \"10892f14-25e6-4272-9286-200686bd5fca\", \"service.name\": \"otelcol-contrib\", \"service.version\": \"0.131.0\"}, \"otelcol.component.id\": \"debug\", \"otelcol.component.kind\": \"exporter\", \"otelcol.signal\": \"logs\", \"resource logs\": 11, \"log records\": 36}\n2025-08-19T20:38:42.307Z\tinfo\tTraces\t{\"resource\": {\"service.instance.id\": \"10892f14-25e6-4272-9286-200686bd5fca\", \"service.name\": \"otelcol-contrib\", \"service.version\": \"0.131.0\"}, \"otelcol.component.id\": \"debug\", \"otelcol.component.kind\": \"exporter\", \"otelcol.signal\": \"traces\", \"resource spans\": 9, \"spans\": 45}\n2025-08-19T20:38:42.522Z\tinfo\tTraces\t{\"resource\": {\"service.instance.id\": \"10892f14-25e6-4272-9286-200686bd5fca\", \"service.name\": \"otelcol-contrib\", \"service.version\": \"0.131.0\"}, \"otelcol.component.id\": \"debug\", \"otelcol.component.kind\": \"exporter\", \"otelcol.signal\": \"traces\", \"resource spans\": 11, \"spans\": 56}\n2025-08-19T20:38:42.667Z\tinfo\tLogs\t{\"resource\": {\"service.instance.id\": \"10892f14-25e6-4272-9286-200686bd5fca\", \"service.name\": \"otelcol-contrib\", \"service.version\": \"0.131.0\"}, \"otelcol.component.id\": \"debug\", \"otelcol.component.kind\": \"exporter\", \"otelcol.signal\": \"logs\", \"resource logs\": 1, \"log records\": 1}\n2025-08-19T20:38:42.761Z\tinfo\tTraces\t{\"resource\": {\"service.instance.id\": \"10892f14-25e6-4272-9286-200686bd5fca\", \"service.name\": \"otelcol-contrib\", \"service.version\": \"0.131.0\"}, \"otelcol.component.id\": \"debug\", \"otelcol.component.kind\": \"exporter\", \"otelcol.signal\": \"traces\", \"resource spans\": 3, \"spans\": 11}\n2025-08-19T20:38:42.978Z\tinfo\tTraces\t{\"resource\": {\"service.instance.id\": \"10892f14-25e6-4272-9286-200686bd5fca\", \"service.name\": \"otelcol-contrib\", \"service.version\": \"0.131.0\"}, \"otelcol.component.id\": \"debug\", \"otelcol.component.kind\": \"exporter\", \"otelcol.signal\": \"traces\", \"resource spans\": 16, \"spans\": 78}\n2025-08-19T20:38:43.014Z\tinfo\tLogs\t{\"resource\": {\"service.instance.id\": \"10892f14-25e6-4272-9286-200686bd5fca\", \"service.name\": \"otelcol-contrib\", \"service.version\": \"0.131.0\"}, \"otelcol.component.id\": \"debug\", \"otelcol.component.kind\": \"exporter\", \"otelcol.signal\": \"logs\", \"resource logs\": 11, \"log records\": 52}\n2025-08-19T20:38:43.210Z\tinfo\tTraces\t{\"resource\": {\"service.instance.id\": \"10892f14-25e6-4272-9286-200686bd5fca\", \"service.name\": \"otelcol-contrib\", \"service.version\": \"0.131.0\"}, \"otelcol.component.id\": \"debug\", \"otelcol.component.kind\": \"exporter\", \"otelcol.signal\": \"traces\", \"resource spans\": 7, \"spans\": 113}\n2025-08-19T20:38:43.270Z\tinfo\tLogs\t{\"resource\": {\"service.instance.id\": \"10892f14-25e6-4272-9286-200686bd5fca\", \"service.name\": \"otelcol-contrib\", \"service.version\": \"0.131.0\"}, \"otelcol.component.id\": \"debug\", \"otelcol.component.kind\": \"exporter\", \"otelcol.signal\": \"logs\", \"resource logs\": 1, \"log records\": 1}\n2025-08-19T20:38:43.532Z\tinfo\tLogs\t{\"resource\": {\"service.instance.id\": \"10892f14-25e6-4272-9286-200686bd5fca\", \"service.name\": \"otelcol-contrib\", \"service.version\": \"0.131.0\"}, \"otelcol.component.id\": \"debug\", \"otelcol.component.kind\": \"exporter\", \"otelcol.signal\": \"logs\", \"resource logs\": 1, \"log records\": 4}\n2025-08-19T20:38:43.987Z\tinfo\tLogs\t{\"resource\": {\"service.instance.id\": \"10892f14-25e6-4272-9286-200686bd5fca\", \"service.name\": \"otelcol-contrib\", \"service.version\": \"0.131.0\"}, \"otelcol.component.id\": \"debug\", \"otelcol.component.kind\": \"exporter\", \"otelcol.signal\": \"logs\", \"resource logs\": 7, \"log records\": 26}\n2025/08/19 20:38:44 failed to upload metrics: reader collect and export timeout: rpc error: code = Unavailable desc = connection error: desc = \"error reading server preface: http2: failed reading the frame payload: http2: frame too large, note that the frame header looked like an HTTP/1.1 header\"\n2025-08-19T20:38:44.402Z\tinfo\tLogs\t{\"resource\": {\"service.instance.id\": \"10892f14-25e6-4272-9286-200686bd5fca\", \"service.name\": \"otelcol-contrib\", \"service.version\": \"0.131.0\"}, \"otelcol.component.id\": \"debug\", \"otelcol.component.kind\": \"exporter\", \"otelcol.signal\": \"logs\", \"resource logs\": 1, \"log records\": 20}\n2025-08-19T20:38:44.679Z\tinfo\tLogs\t{\"resource\": {\"service.instance.id\": \"10892f14-25e6-4272-9286-200686bd5fca\", \"service.name\": \"otelcol-contrib\", \"service.version\": \"0.131.0\"}, \"otelcol.component.id\": \"debug\", \"otelcol.component.kind\": \"exporter\", \"otelcol.signal\": \"logs\", \"resource logs\": 3, \"log records\": 3}\n2025-08-19T20:38:45.123Z\tinfo\tLogs\t{\"resource\": {\"service.instance.id\": \"10892f14-25e6-4272-9286-200686bd5fca\", \"service.name\": \"otelcol-contrib\", \"service.version\": \"0.131.0\"}, \"otelcol.component.id\": \"debug\", \"otelcol.component.kind\": \"exporter\", \"otelcol.signal\": \"logs\", \"resource logs\": 11, \"log records\": 61}\n2025-08-19T20:38:45.421Z\tinfo\tLogs\t{\"resource\": {\"service.instance.id\": \"10892f14-25e6-4272-9286-200686bd5fca\", \"service.name\": \"otelcol-contrib\", \"service.version\": \"0.131.0\"}, \"otelcol.component.id\": \"debug\", \"otelcol.component.kind\": \"exporter\", \"otelcol.signal\": \"logs\", \"resource logs\": 3, \"log records\": 12}\n2025-08-19T20:38:50.396Z\tinfo\tMetrics\t{\"resource\": {\"service.instance.id\": \"10892f14-25e6-4272-9286-200686bd5fca\", \"service.name\": \"otelcol-contrib\", \"service.version\": \"0.131.0\"}, \"otelcol.component.id\": \"debug\", \"otelcol.component.kind\": \"exporter\", \"otelcol.signal\": \"metrics\", \"resource metrics\": 1, \"metrics\": 29, \"data points\": 34}\n2025-08-19T20:38:50.832Z\twarn\tgrpc@v1.74.2/clientconn.go:1414\t[core] [Channel #1 SubChannel #36]grpc: addrConn.createTransport failed to connect to {Addr: \"10.96.231.80:4318\", ServerName: \"otel-collector:4318\", }. Err: connection error: desc = \"error reading server preface: http2: failed reading the frame payload: http2: frame too large, note that the frame header looked like an HTTP/1.1 header\"\t{\"resource\": {\"service.instance.id\": \"10892f14-25e6-4272-9286-200686bd5fca\", \"service.name\": \"otelcol-contrib\", \"service.version\": \"0.131.0\"}, \"grpc_log\": true}\n2025-08-19T20:38:51.220Z\tinfo\tTraces\t{\"resource\": {\"service.instance.id\": \"10892f14-25e6-4272-9286-200686bd5fca\", \"service.name\": \"otelcol-contrib\", \"service.version\": \"0.131.0\"}, \"otelcol.component.id\": \"debug\", \"otelcol.component.kind\": \"exporter\", \"otelcol.signal\": \"traces\", \"resource spans\": 3, \"spans\": 33}\n2025-08-19T20:38:51.468Z\tinfo\tTraces\t{\"resource\": {\"service.instance.id\": \"10892f14-25e6-4272-9286-200686bd5fca\", \"service.name\": \"otelcol-contrib\", \"service.version\": \"0.131.0\"}, \"otelcol.component.id\": \"debug\", \"otelcol.component.kind\": \"exporter\", \"otelcol.signal\": \"traces\", \"resource spans\": 5, \"spans\": 25}\n2025-08-19T20:38:51.878Z\tinfo\tTraces\t{\"resource\": {\"service.instance.id\": \"10892f14-25e6-4272-9286-200686bd5fca\", \"service.name\": \"otelcol-contrib\", \"service.version\": \"0.131.0\"}, \"otelcol.component.id\": \"debug\", \"otelcol.component.kind\": \"exporter\", \"otelcol.signal\": \"traces\", \"resource spans\": 4, \"spans\": 20}\n2025-08-19T20:38:52.090Z\tinfo\tTraces\t{\"resource\": {\"service.instance.id\": \"10892f14-25e6-4272-9286-200686bd5fca\", \"service.name\": \"otelcol-contrib\", \"service.version\": \"0.131.0\"}, \"otelcol.component.id\": \"debug\", \"otelcol.component.kind\": \"exporter\", \"otelcol.signal\": \"traces\", \"resource spans\": 12, \"spans\": 87}\n2025-08-19T20:38:52.230Z\tinfo\tLogs\t{\"resource\": {\"service.instance.id\": \"10892f14-25e6-4272-9286-200686bd5fca\", \"service.name\": \"otelcol-contrib\", \"service.version\": \"0.131.0\"}, \"otelcol.component.id\": \"debug\", \"otelcol.component.kind\": \"exporter\", \"otelcol.signal\": \"logs\", \"resource logs\": 1, \"log records\": 1}\n2025-08-19T20:38:52.299Z\tinfo\tTraces\t{\"resource\": {\"service.instance.id\": \"10892f14-25e6-4272-9286-200686bd5fca\", \"service.name\": \"otelcol-contrib\", \"service.version\": \"0.131.0\"}, \"otelcol.component.id\": \"debug\", \"otelcol.component.kind\": \"exporter\", \"otelcol.signal\": \"traces\", \"resource spans\": 18, \"spans\": 78}\n2025-08-19T20:38:52.571Z\tinfo\tTraces\t{\"resource\": {\"service.instance.id\": \"10892f14-25e6-4272-9286-200686bd5fca\", \"service.name\": \"otelcol-contrib\", \"service.version\": \"0.131.0\"}, \"otelcol.component.id\": \"debug\", \"otelcol.component.kind\": \"exporter\", \"otelcol.signal\": \"traces\", \"resource spans\": 3, \"spans\": 11}\n2025-08-19T20:38:52.618Z\tinfo\tLogs\t{\"resource\": {\"service.instance.id\": \"10892f14-25e6-4272-9286-200686bd5fca\", \"service.name\": \"otelcol-contrib\", \"service.version\": \"0.131.0\"}, \"otelcol.component.id\": \"debug\", \"otelcol.component.kind\": \"exporter\", \"otelcol.signal\": \"logs\", \"resource logs\": 3, \"log records\": 3}\n2025-08-19T20:38:52.808Z\tinfo\tTraces\t{\"resource\": {\"service.instance.id\": \"10892f14-25e6-4272-9286-200686bd5fca\", \"service.name\": \"otelcol-contrib\", \"service.version\": \"0.131.0\"}, \"otelcol.component.id\": \"debug\", \"otelcol.component.kind\": \"exporter\", \"otelcol.signal\": \"traces\", \"resource spans\": 19, \"spans\": 129}\n2025-08-19T20:38:52.908Z\tinfo\tLogs\t{\"resource\": {\"service.instance.id\": \"10892f14-25e6-4272-9286-200686bd5fca\", \"service.name\": \"otelcol-contrib\", \"service.version\": \"0.131.0\"}, \"otelcol.component.id\": \"debug\", \"otelcol.component.kind\": \"exporter\", \"otelcol.signal\": \"logs\", \"resource logs\": 13, \"log records\": 86}\n2025-08-19T20:38:53.218Z\tinfo\tTraces\t{\"resource\": {\"service.instance.id\": \"10892f14-25e6-4272-9286-200686bd5fca\", \"service.name\": \"otelcol-contrib\", \"service.version\": \"0.131.0\"}, \"otelcol.component.id\": \"debug\", \"otelcol.component.kind\": \"exporter\", \"otelcol.signal\": \"traces\", \"resource spans\": 6, \"spans\": 111}\n2025-08-19T20:38:53.389Z\tinfo\tLogs\t{\"resource\": {\"service.instance.id\": \"10892f14-25e6-4272-9286-200686bd5fca\", \"service.name\": \"otelcol-contrib\", \"service.version\": \"0.131.0\"}, \"otelcol.component.id\": \"debug\", \"otelcol.component.kind\": \"exporter\", \"otelcol.signal\": \"logs\", \"resource logs\": 1, \"log records\": 2}\n2025-08-19T20:38:53.425Z\tinfo\tTraces\t{\"resource\": {\"service.instance.id\": \"10892f14-25e6-4272-9286-200686bd5fca\", \"service.name\": \"otelcol-contrib\", \"service.version\": \"0.131.0\"}, \"otelcol.component.id\": \"debug\", \"otelcol.component.kind\": \"exporter\", \"otelcol.signal\": \"traces\", \"resource spans\": 4, \"spans\": 16}\n2025-08-19T20:38:53.634Z\tinfo\tTraces\t{\"resource\": {\"service.instance.id\": \"10892f14-25e6-4272-9286-200686bd5fca\", \"service.name\": \"otelcol-contrib\", \"service.version\": \"0.131.0\"}, \"otelcol.component.id\": \"debug\", \"otelcol.component.kind\": \"exporter\", \"otelcol.signal\": \"traces\", \"resource spans\": 1, \"spans\": 5}\n2025-08-19T20:38:53.660Z\tinfo\tLogs\t{\"resource\": {\"service.instance.id\": \"10892f14-25e6-4272-9286-200686bd5fca\", \"service.name\": \"otelcol-contrib\", \"service.version\": \"0.131.0\"}, \"otelcol.component.id\": \"debug\", \"otelcol.component.kind\": \"exporter\", \"otelcol.signal\": \"logs\", \"resource logs\": 2, \"log records\": 6}\n2025-08-19T20:38:53.926Z\tinfo\tLogs\t{\"resource\": {\"service.instance.id\": \"10892f14-25e6-4272-9286-200686bd5fca\", \"service.name\": \"otelcol-contrib\", \"service.version\": \"0.131.0\"}, \"otelcol.component.id\": \"debug\", \"otelcol.component.kind\": \"exporter\", \"otelcol.signal\": \"logs\", \"resource logs\": 12, \"log records\": 36}\n2025-08-19T20:38:54.171Z\tinfo\tLogs\t{\"resource\": {\"service.instance.id\": \"10892f14-25e6-4272-9286-200686bd5fca\", \"service.name\": \"otelcol-contrib\", \"service.version\": \"0.131.0\"}, \"otelcol.component.id\": \"debug\", \"otelcol.component.kind\": \"exporter\", \"otelcol.signal\": \"logs\", \"resource logs\": 2, \"log records\": 2}\n2025/08/19 20:38:54 failed to upload metrics: reader collect and export timeout: rpc error: code = Unavailable desc = connection error: desc = \"error reading server preface: http2: failed reading the frame payload: http2: frame too large, note that the frame header looked like an HTTP/1.1 header\"\n2025-08-19T20:38:54.459Z\tinfo\tLogs\t{\"resource\": {\"service.instance.id\": \"10892f14-25e6-4272-9286-200686bd5fca\", \"service.name\": \"otelcol-contrib\", \"service.version\": \"0.131.0\"}, \"otelcol.component.id\": \"debug\", \"otelcol.component.kind\": \"exporter\", \"otelcol.signal\": \"logs\", \"resource logs\": 2, \"log records\": 28}\n2025-08-19T20:38:54.948Z\tinfo\tLogs\t{\"resource\": {\"service.instance.id\": \"10892f14-25e6-4272-9286-200686bd5fca\", \"service.name\": \"otelcol-contrib\", \"service.version\": \"0.131.0\"}, \"otelcol.component.id\": \"debug\", \"otelcol.component.kind\": \"exporter\", \"otelcol.signal\": \"logs\", \"resource logs\": 11, \"log records\": 66}\n"
        },
        {
            "role": "assistant",
            "content": "```\nexec_shell(\"kubectl logs opensearch-0 -n astronomy-shop --tail=200\")\n```"
        },
        {
            "role": "env",
            "content": "Defaulted container \"opensearch\" out of: opensearch, configfile (init)\n"
        },
        {
            "role": "assistant",
            "content": "```\nexec_shell(\"kubectl logs opensearch-0 -n astronomy-shop -c opensearch --tail=200\")\n```"
        },
        {
            "role": "env",
            "content": "[2025-08-19T20:38:01,930][INFO ][o.o.p.c.c.PluginSettings ] [opensearch-0] Trying to create directory /dev/shm/performanceanalyzer/.\n[2025-08-19T20:38:01,931][INFO ][o.o.p.c.c.PluginSettings ] [opensearch-0] Config: metricsLocation: /dev/shm/performanceanalyzer/, metricsDeletionInterval: 1, httpsEnabled: false, cleanup-metrics-db-files: true, batch-metrics-retention-period-minutes: 7, rpc-port: 9650, webservice-port 9600\n[2025-08-19T20:38:02,306][INFO ][o.o.i.r.ReindexPlugin    ] [opensearch-0] ReindexPlugin reloadSPI called\n[2025-08-19T20:38:02,306][INFO ][o.o.i.r.ReindexPlugin    ] [opensearch-0] Unable to find any implementation for RemoteReindexExtension\n[2025-08-19T20:38:02,322][INFO ][o.o.j.JobSchedulerPlugin ] [opensearch-0] Loaded scheduler extension: opensearch_time_series_analytics, index: .opendistro-anomaly-detector-jobs\n[2025-08-19T20:38:02,333][INFO ][o.o.j.JobSchedulerPlugin ] [opensearch-0] Loaded scheduler extension: reports-scheduler, index: .opendistro-reports-definitions\n[2025-08-19T20:38:02,333][INFO ][o.o.j.JobSchedulerPlugin ] [opensearch-0] Loaded scheduler extension: opendistro-index-management, index: .opendistro-ism-config\n[2025-08-19T20:38:02,334][INFO ][o.o.j.JobSchedulerPlugin ] [opensearch-0] Loaded scheduler extension: checkBatchJobTaskStatus, index: .ml_commons_task_polling_job\n[2025-08-19T20:38:02,335][INFO ][o.o.j.JobSchedulerPlugin ] [opensearch-0] Loaded scheduler extension: scheduler_geospatial_ip2geo_datasource, index: .scheduler-geospatial-ip2geo-datasource\n[2025-08-19T20:38:02,335][INFO ][o.o.j.JobSchedulerPlugin ] [opensearch-0] Loaded scheduler extension: opensearch_sap_job, index: .opensearch-sap--job\n[2025-08-19T20:38:02,336][INFO ][o.o.j.JobSchedulerPlugin ] [opensearch-0] Loaded scheduler extension: async-query-scheduler, index: .async-query-scheduler\n[2025-08-19T20:38:02,350][INFO ][o.o.p.PluginsService     ] [opensearch-0] loaded module [aggs-matrix-stats]\n[2025-08-19T20:38:02,350][INFO ][o.o.p.PluginsService     ] [opensearch-0] loaded module [analysis-common]\n[2025-08-19T20:38:02,350][INFO ][o.o.p.PluginsService     ] [opensearch-0] loaded module [cache-common]\n[2025-08-19T20:38:02,350][INFO ][o.o.p.PluginsService     ] [opensearch-0] loaded module [geo]\n[2025-08-19T20:38:02,350][INFO ][o.o.p.PluginsService     ] [opensearch-0] loaded module [ingest-common]\n[2025-08-19T20:38:02,350][INFO ][o.o.p.PluginsService     ] [opensearch-0] loaded module [ingest-geoip]\n[2025-08-19T20:38:02,350][INFO ][o.o.p.PluginsService     ] [opensearch-0] loaded module [ingest-user-agent]\n[2025-08-19T20:38:02,350][INFO ][o.o.p.PluginsService     ] [opensearch-0] loaded module [lang-expression]\n[2025-08-19T20:38:02,350][INFO ][o.o.p.PluginsService     ] [opensearch-0] loaded module [lang-mustache]\n[2025-08-19T20:38:02,350][INFO ][o.o.p.PluginsService     ] [opensearch-0] loaded module [lang-painless]\n[2025-08-19T20:38:02,350][INFO ][o.o.p.PluginsService     ] [opensearch-0] loaded module [mapper-extras]\n[2025-08-19T20:38:02,350][INFO ][o.o.p.PluginsService     ] [opensearch-0] loaded module [opensearch-dashboards]\n[2025-08-19T20:38:02,350][INFO ][o.o.p.PluginsService     ] [opensearch-0] loaded module [parent-join]\n[2025-08-19T20:38:02,350][INFO ][o.o.p.PluginsService     ] [opensearch-0] loaded module [percolator]\n[2025-08-19T20:38:02,350][INFO ][o.o.p.PluginsService     ] [opensearch-0] loaded module [rank-eval]\n[2025-08-19T20:38:02,350][INFO ][o.o.p.PluginsService     ] [opensearch-0] loaded module [reindex]\n[2025-08-19T20:38:02,350][INFO ][o.o.p.PluginsService     ] [opensearch-0] loaded module [repository-url]\n[2025-08-19T20:38:02,350][INFO ][o.o.p.PluginsService     ] [opensearch-0] loaded module [search-pipeline-common]\n[2025-08-19T20:38:02,350][INFO ][o.o.p.PluginsService     ] [opensearch-0] loaded module [systemd]\n[2025-08-19T20:38:02,350][INFO ][o.o.p.PluginsService     ] [opensearch-0] loaded module [transport-netty4]\n[2025-08-19T20:38:02,350][INFO ][o.o.p.PluginsService     ] [opensearch-0] loaded plugin [opensearch-alerting]\n[2025-08-19T20:38:02,350][INFO ][o.o.p.PluginsService     ] [opensearch-0] loaded plugin [opensearch-anomaly-detection]\n[2025-08-19T20:38:02,350][INFO ][o.o.p.PluginsService     ] [opensearch-0] loaded plugin [opensearch-asynchronous-search]\n[2025-08-19T20:38:02,350][INFO ][o.o.p.PluginsService     ] [opensearch-0] loaded plugin [opensearch-cross-cluster-replication]\n[2025-08-19T20:38:02,350][INFO ][o.o.p.PluginsService     ] [opensearch-0] loaded plugin [opensearch-custom-codecs]\n[2025-08-19T20:38:02,350][INFO ][o.o.p.PluginsService     ] [opensearch-0] loaded plugin [opensearch-flow-framework]\n[2025-08-19T20:38:02,351][INFO ][o.o.p.PluginsService     ] [opensearch-0] loaded plugin [opensearch-geospatial]\n[2025-08-19T20:38:02,351][INFO ][o.o.p.PluginsService     ] [opensearch-0] loaded plugin [opensearch-index-management]\n[2025-08-19T20:38:02,351][INFO ][o.o.p.PluginsService     ] [opensearch-0] loaded plugin [opensearch-job-scheduler]\n[2025-08-19T20:38:02,351][INFO ][o.o.p.PluginsService     ] [opensearch-0] loaded plugin [opensearch-knn]\n[2025-08-19T20:38:02,351][INFO ][o.o.p.PluginsService     ] [opensearch-0] loaded plugin [opensearch-ltr]\n[2025-08-19T20:38:02,351][INFO ][o.o.p.PluginsService     ] [opensearch-0] loaded plugin [opensearch-ml]\n[2025-08-19T20:38:02,351][INFO ][o.o.p.PluginsService     ] [opensearch-0] loaded plugin [opensearch-neural-search]\n[2025-08-19T20:38:02,351][INFO ][o.o.p.PluginsService     ] [opensearch-0] loaded plugin [opensearch-notifications]\n[2025-08-19T20:38:02,351][INFO ][o.o.p.PluginsService     ] [opensearch-0] loaded plugin [opensearch-notifications-core]\n[2025-08-19T20:38:02,351][INFO ][o.o.p.PluginsService     ] [opensearch-0] loaded plugin [opensearch-observability]\n[2025-08-19T20:38:02,351][INFO ][o.o.p.PluginsService     ] [opensearch-0] loaded plugin [opensearch-performance-analyzer]\n[2025-08-19T20:38:02,351][INFO ][o.o.p.PluginsService     ] [opensearch-0] loaded plugin [opensearch-reports-scheduler]\n[2025-08-19T20:38:02,351][INFO ][o.o.p.PluginsService     ] [opensearch-0] loaded plugin [opensearch-security]\n[2025-08-19T20:38:02,351][INFO ][o.o.p.PluginsService     ] [opensearch-0] loaded plugin [opensearch-security-analytics]\n[2025-08-19T20:38:02,351][INFO ][o.o.p.PluginsService     ] [opensearch-0] loaded plugin [opensearch-skills]\n[2025-08-19T20:38:02,351][INFO ][o.o.p.PluginsService     ] [opensearch-0] loaded plugin [opensearch-sql]\n[2025-08-19T20:38:02,351][INFO ][o.o.p.PluginsService     ] [opensearch-0] loaded plugin [opensearch-system-templates]\n[2025-08-19T20:38:02,351][INFO ][o.o.p.PluginsService     ] [opensearch-0] loaded plugin [query-insights]\n[2025-08-19T20:38:02,421][WARN ][stderr                   ] [opensearch-0] WARNING: A restricted method in java.lang.foreign.Linker has been called\n[2025-08-19T20:38:02,421][WARN ][stderr                   ] [opensearch-0] WARNING: java.lang.foreign.Linker::downcallHandle has been called by the unnamed module\n[2025-08-19T20:38:02,421][WARN ][stderr                   ] [opensearch-0] WARNING: Use --enable-native-access=ALL-UNNAMED to avoid a warning for this module\n[2025-08-19T20:38:02,467][INFO ][o.a.l.s.MemorySegmentIndexInputProvider] [opensearch-0] Using MemorySegmentIndexInput and native madvise support with Java 21 or later; to disable start with -Dorg.apache.lucene.store.MMapDirectory.enableMemorySegments=false\n[2025-08-19T20:38:02,471][INFO ][o.o.e.NodeEnvironment    ] [opensearch-0] using [1] data paths, mounts [[/ (overlay)]], net usable_space [295.5gb], net total_space [333.3gb], types [overlay]\n[2025-08-19T20:38:02,471][INFO ][o.o.e.NodeEnvironment    ] [opensearch-0] heap size [300mb], compressed ordinary object pointers [true]\n[2025-08-19T20:38:02,475][DEPRECATION][o.o.d.c.n.DiscoveryNodeRole] [opensearch-0] Assigning [master] role in setting [node.roles] is deprecated. To promote inclusive language, please use [cluster_manager] role instead.\n[2025-08-19T20:38:02,490][INFO ][o.o.n.Node               ] [opensearch-0] node name [opensearch-0], node ID [BuIcVgfGRjaB_j9Lle0TRQ], cluster name [demo-cluster], roles [ingest, remote_cluster_client, master, data]\n[2025-08-19T20:38:02,513][INFO ][o.o.e.ExtensionsManager  ] [opensearch-0] ExtensionsManager initialized\n[2025-08-19T20:38:03,440][DEPRECATION][o.o.d.c.s.Settings       ] [opensearch-0] [index.store.hybrid.mmap.extensions] setting was deprecated in OpenSearch and will be removed in a future release! See the breaking changes documentation for the next major version.\n[2025-08-19T20:38:03,913][INFO ][o.o.n.p.NeuralSearch     ] [opensearch-0] Registering hybrid query phase searcher with feature flag [plugins.neural_search.hybrid_search_disabled]\n[2025-08-19T20:38:04,196][INFO ][o.o.l.b.LTRCircuitBreakerService] [opensearch-0] Registered memory breaker.\n[2025-08-19T20:38:04,265][INFO ][o.o.t.b.CircuitBreakerService] [opensearch-0] Registered memory breaker.\n[2025-08-19T20:38:04,437][INFO ][o.o.r.m.c.i.SdkClientFactory] [opensearch-0] Using local opensearch cluster as metadata store.\n[2025-08-19T20:38:04,537][INFO ][o.o.r.m.c.i.SdkClientFactory] [opensearch-0] Using local opensearch cluster as metadata store.\n[2025-08-19T20:38:04,557][INFO ][o.o.m.b.MLCircuitBreakerService] [opensearch-0] Registered ML memory breaker.\n[2025-08-19T20:38:04,558][INFO ][o.o.m.b.MLCircuitBreakerService] [opensearch-0] Registered ML disk breaker.\n[2025-08-19T20:38:04,558][INFO ][o.o.m.b.MLCircuitBreakerService] [opensearch-0] Registered ML native memory breaker.\n[2025-08-19T20:38:04,705][INFO ][o.r.Reflections          ] [opensearch-0] Reflections took 94 ms to scan 1 urls, producing 27 keys and 67 values \n[2025-08-19T20:38:04,744][INFO ][o.r.Reflections          ] [opensearch-0] Reflections took 3 ms to scan 1 urls, producing 3 keys and 5 values \n[2025-08-19T20:38:04,911][WARN ][o.o.s.p.SQLPlugin        ] [opensearch-0] Master key is a required config for using create and update datasource APIs. Please set plugins.query.datasources.encryption.masterkey config in opensearch.yml in all the cluster nodes. More details can be found here: https://github.com/opensearch-project/sql/blob/main/docs/user/ppl/admin/datasources.rst#master-key-config-for-encrypting-credential-information\n[2025-08-19T20:38:05,167][INFO ][o.o.p.i.c.s.QueryInsightsService] [opensearch-0] Setting query insights index template priority to [1847]\n[2025-08-19T20:38:05,474][INFO ][o.o.t.NettyAllocator     ] [opensearch-0] creating NettyAllocator with the following configs: [name=unpooled, suggested_max_allocation_size=256kb, factors={opensearch.unsafe.use_unpooled_allocator=null, g1gc_enabled=true, g1gc_region_size=1mb, heap_size=300mb}]\n[2025-08-19T20:38:05,543][INFO ][o.o.d.DiscoveryModule    ] [opensearch-0] using discovery type [single-node] and seed hosts providers [settings]\n[2025-08-19T20:38:05,881][WARN ][o.o.g.DanglingIndicesState] [opensearch-0] gateway.auto_import_dangling_indices is disabled, dangling indices will not be automatically detected or imported and must be managed manually\n[2025-08-19T20:38:06,334][INFO ][o.o.p.h.c.PerformanceAnalyzerConfigAction] [opensearch-0] PerformanceAnalyzer Enabled: false\n[2025-08-19T20:38:06,393][INFO ][o.o.n.Node               ] [opensearch-0] initialized\n[2025-08-19T20:38:06,393][INFO ][o.o.n.Node               ] [opensearch-0] starting ...\n[2025-08-19T20:38:06,442][INFO ][o.o.s.l.BuiltinLogTypeLoader] [opensearch-0] Loaded [windows_logtype.json] log type\n[2025-08-19T20:38:06,442][INFO ][o.o.s.l.BuiltinLogTypeLoader] [opensearch-0] Loaded [waf_logtype.json] log type\n[2025-08-19T20:38:06,443][INFO ][o.o.s.l.BuiltinLogTypeLoader] [opensearch-0] Loaded [vpcflow_logtype.json] log type\n[2025-08-19T20:38:06,443][INFO ][o.o.s.l.BuiltinLogTypeLoader] [opensearch-0] Loaded [test_windows_logtype.json] log type\n[2025-08-19T20:38:06,443][INFO ][o.o.s.l.BuiltinLogTypeLoader] [opensearch-0] Loaded [s3_logtype.json] log type\n[2025-08-19T20:38:06,444][INFO ][o.o.s.l.BuiltinLogTypeLoader] [opensearch-0] Loaded [others_web_logtype.json] log type\n[2025-08-19T20:38:06,444][INFO ][o.o.s.l.BuiltinLogTypeLoader] [opensearch-0] Loaded [others_proxy_logtype.json] log type\n[2025-08-19T20:38:06,444][INFO ][o.o.s.l.BuiltinLogTypeLoader] [opensearch-0] Loaded [others_macos_logtype.json] log type\n[2025-08-19T20:38:06,444][INFO ][o.o.s.l.BuiltinLogTypeLoader] [opensearch-0] Loaded [others_compliance_logtype.json] log type\n[2025-08-19T20:38:06,444][INFO ][o.o.s.l.BuiltinLogTypeLoader] [opensearch-0] Loaded [others_cloud_logtype.json] log type\n[2025-08-19T20:38:06,445][INFO ][o.o.s.l.BuiltinLogTypeLoader] [opensearch-0] Loaded [others_apt_logtype.json] log type\n[2025-08-19T20:38:06,445][INFO ][o.o.s.l.BuiltinLogTypeLoader] [opensearch-0] Loaded [others_application_logtype.json] log type\n[2025-08-19T20:38:06,445][INFO ][o.o.s.l.BuiltinLogTypeLoader] [opensearch-0] Loaded [okta_logtype.json] log type\n[2025-08-19T20:38:06,448][INFO ][o.o.s.l.BuiltinLogTypeLoader] [opensearch-0] Loaded [network_logtype.json] log type\n[2025-08-19T20:38:06,448][INFO ][o.o.s.l.BuiltinLogTypeLoader] [opensearch-0] Loaded [netflow_logtype.json] log type\n[2025-08-19T20:38:06,448][INFO ][o.o.s.l.BuiltinLogTypeLoader] [opensearch-0] Loaded [m365_logtype.json] log type\n[2025-08-19T20:38:06,449][INFO ][o.o.s.l.BuiltinLogTypeLoader] [opensearch-0] Loaded [linux_logtype.json] log type\n[2025-08-19T20:38:06,449][INFO ][o.o.s.l.BuiltinLogTypeLoader] [opensearch-0] Loaded [gworkspace_logtype.json] log type\n[2025-08-19T20:38:06,449][INFO ][o.o.s.l.BuiltinLogTypeLoader] [opensearch-0] Loaded [github_logtype.json] log type\n[2025-08-19T20:38:06,449][INFO ][o.o.s.l.BuiltinLogTypeLoader] [opensearch-0] Loaded [dns_logtype.json] log type\n[2025-08-19T20:38:06,455][INFO ][o.o.s.l.BuiltinLogTypeLoader] [opensearch-0] Loaded [cloudtrail_logtype.json] log type\n[2025-08-19T20:38:06,456][INFO ][o.o.s.l.BuiltinLogTypeLoader] [opensearch-0] Loaded [azure_logtype.json] log type\n[2025-08-19T20:38:06,456][INFO ][o.o.s.l.BuiltinLogTypeLoader] [opensearch-0] Loaded [apache_access_logtype.json] log type\n[2025-08-19T20:38:06,459][INFO ][o.o.s.l.BuiltinLogTypeLoader] [opensearch-0] Loaded [ad_ldap_logtype.json] log type\n[2025-08-19T20:38:06,587][INFO ][o.o.t.TransportService   ] [opensearch-0] publish_address {10.244.1.104:9300}, bound_addresses {[::]:9300}\n[2025-08-19T20:38:06,589][INFO ][o.o.t.TransportService   ] [opensearch-0] Remote clusters initialized successfully.\n[2025-08-19T20:38:07,221][INFO ][o.o.c.c.Coordinator      ] [opensearch-0] setting initial configuration to VotingConfiguration{BuIcVgfGRjaB_j9Lle0TRQ}\n[2025-08-19T20:38:07,363][INFO ][o.o.c.s.MasterService    ] [opensearch-0] Tasks batched with key: org.opensearch.cluster.coordination.JoinHelper, count:3 and sample tasks: elected-as-cluster-manager ([1] nodes joined)[{opensearch-0}{BuIcVgfGRjaB_j9Lle0TRQ}{43ZY8DW4QTyrEIOfh8zY1A}{10.244.1.104}{10.244.1.104:9300}{dimr}{shard_indexing_pressure_enabled=true} elect leader, _BECOME_CLUSTER_MANAGER_TASK_, _FINISH_ELECTION_], term: 1, version: 1, delta: cluster-manager node changed {previous [], current [{opensearch-0}{BuIcVgfGRjaB_j9Lle0TRQ}{43ZY8DW4QTyrEIOfh8zY1A}{10.244.1.104}{10.244.1.104:9300}{dimr}{shard_indexing_pressure_enabled=true}]}\n[2025-08-19T20:38:07,392][INFO ][o.o.c.c.CoordinationState] [opensearch-0] cluster UUID set to [A2-S9vQ1QSayr0pBRmb-YQ]\n[2025-08-19T20:38:07,410][INFO ][o.o.c.s.ClusterApplierService] [opensearch-0] cluster-manager node changed {previous [], current [{opensearch-0}{BuIcVgfGRjaB_j9Lle0TRQ}{43ZY8DW4QTyrEIOfh8zY1A}{10.244.1.104}{10.244.1.104:9300}{dimr}{shard_indexing_pressure_enabled=true}]}, term: 1, version: 1, reason: Publication{term=1, version=1}\n[2025-08-19T20:38:07,414][INFO ][o.o.t.i.IndexManagement  ] [opensearch-0] Candidate custom result indices are empty.\n[2025-08-19T20:38:07,414][INFO ][o.o.t.i.IndexManagement  ] [opensearch-0] Candidate custom result indices are empty.\n[2025-08-19T20:38:07,414][INFO ][o.o.t.c.ClusterEventListener] [opensearch-0] Cluster is not recovered yet.\n[2025-08-19T20:38:07,427][INFO ][o.o.i.i.ManagedIndexCoordinator] [opensearch-0] Cache cluster manager node onClusterManager time: 1755635887427\n[2025-08-19T20:38:07,433][WARN ][o.o.p.c.s.h.ConfigOverridesClusterSettingHandler] [opensearch-0] Config override setting update called with empty string. Ignoring.\n[2025-08-19T20:38:07,440][INFO ][o.o.d.PeerFinder         ] [opensearch-0] setting findPeersInterval to [1s] as node commission status = [true] for local node [{opensearch-0}{BuIcVgfGRjaB_j9Lle0TRQ}{43ZY8DW4QTyrEIOfh8zY1A}{10.244.1.104}{10.244.1.104:9300}{dimr}{shard_indexing_pressure_enabled=true}]\n[2025-08-19T20:38:07,452][INFO ][o.o.h.AbstractHttpServerTransport] [opensearch-0] publish_address {10.244.1.104:9200}, bound_addresses {[::]:9200}\n[2025-08-19T20:38:07,452][INFO ][o.o.n.Node               ] [opensearch-0] started\n[2025-08-19T20:38:07,455][INFO ][o.o.s.OpenSearchSecurityPlugin] [opensearch-0] 0 OpenSearch Security modules loaded so far: []\n[2025-08-19T20:38:07,456][INFO ][o.o.t.c.HashRing         ] [opensearch-0] Node added: [BuIcVgfGRjaB_j9Lle0TRQ]\n[2025-08-19T20:38:07,461][INFO ][o.o.t.c.HashRing         ] [opensearch-0] Add data node to version hash ring: BuIcVgfGRjaB_j9Lle0TRQ\n[2025-08-19T20:38:07,464][INFO ][o.o.t.c.HashRing         ] [opensearch-0] All nodes with known version: {BuIcVgfGRjaB_j9Lle0TRQ=ADNodeInfo{version=2.19.3, isEligibleDataNode=true}}\n[2025-08-19T20:38:07,464][INFO ][o.o.t.c.HashRing         ] [opensearch-0] Rebuild hash ring for realtime with cooldown, nodeChangeEvents size 0\n[2025-08-19T20:38:07,464][INFO ][o.o.t.c.HashRing         ] [opensearch-0] Build version hash ring successfully\n[2025-08-19T20:38:07,465][INFO ][o.o.t.c.ADDataMigrator   ] [opensearch-0] Start migrating AD data\n[2025-08-19T20:38:07,465][INFO ][o.o.t.c.ADDataMigrator   ] [opensearch-0] AD job index doesn't exist, no need to migrate\n[2025-08-19T20:38:07,465][INFO ][o.o.t.c.ClusterEventListener] [opensearch-0] Init version hash ring successfully\n[2025-08-19T20:38:07,478][INFO ][o.o.g.GatewayService     ] [opensearch-0] recovered [0] indices into cluster_state\n[2025-08-19T20:38:07,494][INFO ][o.o.p.PluginsService     ] [opensearch-0] PluginService:onIndexModule index:[.opensearch-sap-log-types-config/CucLT24NTwq2nKBTNbnR9Q]\n[2025-08-19T20:38:07,566][INFO ][o.o.c.m.MetadataCreateIndexService] [opensearch-0] [.opensearch-sap-log-types-config] creating index, cause [auto(sap-logtype api)], templates [], shards [1]/[1]\n[2025-08-19T20:38:07,570][INFO ][o.o.c.r.a.AllocationService] [opensearch-0] updating number_of_replicas to [0] for indices [.opensearch-sap-log-types-config]\n[2025-08-19T20:38:07,614][INFO ][o.o.p.PluginsService     ] [opensearch-0] PluginService:onIndexModule index:[.opensearch-sap-log-types-config/CucLT24NTwq2nKBTNbnR9Q]\n[2025-08-19T20:38:07,682][INFO ][o.o.p.PluginsService     ] [opensearch-0] PluginService:onIndexModule index:[.opensearch-observability/7_cUaWmfSU2M6rR6Lat31w]\n[2025-08-19T20:38:07,695][INFO ][o.o.c.m.MetadataCreateIndexService] [opensearch-0] [.opensearch-observability] creating index, cause [api], templates [], shards [1]/[0]\n[2025-08-19T20:38:07,747][INFO ][o.o.p.PluginsService     ] [opensearch-0] PluginService:onIndexModule index:[.opensearch-observability/7_cUaWmfSU2M6rR6Lat31w]\n[2025-08-19T20:38:07,869][INFO ][o.o.s.l.LogTypeService   ] [opensearch-0] Loading builtin types!\n[2025-08-19T20:38:07,875][INFO ][o.o.s.l.LogTypeService   ] [opensearch-0] Indexing [432] fieldMappingDocs from logTypes: 24\n[2025-08-19T20:38:07,908][INFO ][o.o.c.r.a.AllocationService] [opensearch-0] Cluster health status changed from [YELLOW] to [GREEN] (reason: [shards started [[.opensearch-observability][0]]]).\n[2025-08-19T20:38:07,947][INFO ][o.o.s.l.LogTypeService   ] [opensearch-0] Indexing [432] fieldMappingDocs\n[2025-08-19T20:38:07,957][INFO ][o.o.o.i.ObservabilityIndex] [opensearch-0] observability:Index .opensearch-observability creation Acknowledged\n[2025-08-19T20:38:07,988][INFO ][o.o.p.PluginsService     ] [opensearch-0] PluginService:onIndexModule index:[.opensearch-sap-log-types-config/CucLT24NTwq2nKBTNbnR9Q]\n[2025-08-19T20:38:07,997][INFO ][o.o.c.m.MetadataMappingService] [opensearch-0] [.opensearch-sap-log-types-config/CucLT24NTwq2nKBTNbnR9Q] update_mapping [_doc]\n[2025-08-19T20:38:08,058][INFO ][o.o.p.PluginsService     ] [opensearch-0] PluginService:onIndexModule index:[.opensearch-sap-log-types-config/CucLT24NTwq2nKBTNbnR9Q]\n[2025-08-19T20:38:08,066][INFO ][o.o.c.m.MetadataMappingService] [opensearch-0] [.opensearch-sap-log-types-config/CucLT24NTwq2nKBTNbnR9Q] update_mapping [_doc]\n[2025-08-19T20:38:08,293][INFO ][o.o.s.l.LogTypeService   ] [opensearch-0] Loaded [432] field mapping docs successfully!\n[2025-08-19T20:38:08,305][INFO ][o.o.s.l.LogTypeService   ] [opensearch-0] Indexing [23] customLogTypes\n[2025-08-19T20:38:08,340][INFO ][o.o.s.l.LogTypeService   ] [opensearch-0] Loaded [23] customLogType docs successfully!\n[2025-08-19T20:38:08,341][INFO ][o.o.s.SecurityAnalyticsPlugin] [opensearch-0] LogType config index successfully created and builtin log types loaded\n[2025-08-19T20:38:08,442][INFO ][o.o.s.i.DetectorIndexManagementService] [opensearch-0] info deleteOldIndices\n[2025-08-19T20:38:08,443][INFO ][o.o.s.i.DetectorIndexManagementService] [opensearch-0] No Old Correlation Indices to delete\n[2025-08-19T20:38:08,444][INFO ][o.o.s.i.DetectorIndexManagementService] [opensearch-0] info deleteOldIndices\n[2025-08-19T20:38:08,444][INFO ][o.o.s.i.DetectorIndexManagementService] [opensearch-0] No Old IOC Findings Indices to delete\n[2025-08-19T20:38:08,445][INFO ][o.o.s.l.LogTypeService   ] [opensearch-0] Indexing [23] customLogTypes\n[2025-08-19T20:38:08,461][INFO ][o.o.s.l.LogTypeService   ] [opensearch-0] Indexing [23] customLogTypes\n[2025-08-19T20:38:08,499][INFO ][o.o.s.l.LogTypeService   ] [opensearch-0] Loaded [23] customLogType docs successfully!\n[2025-08-19T20:38:08,510][INFO ][o.o.s.l.LogTypeService   ] [opensearch-0] Loaded [23] customLogType docs successfully!\n[2025-08-19T20:38:08,553][INFO ][o.o.s.i.DetectorIndexManagementService] [opensearch-0] info deleteOldIndices\n[2025-08-19T20:38:08,553][INFO ][o.o.s.i.DetectorIndexManagementService] [opensearch-0] No Old Finding Indices to delete\n[2025-08-19T20:38:08,554][INFO ][o.o.s.i.DetectorIndexManagementService] [opensearch-0] info deleteOldIndices\n[2025-08-19T20:38:08,554][INFO ][o.o.s.i.DetectorIndexManagementService] [opensearch-0] No Old Alert Indices to delete\n[2025-08-19T20:38:17,431][INFO ][o.o.m.a.MLModelAutoReDeployer] [opensearch-0] Index not found, not performing auto reloading!\n[2025-08-19T20:38:17,431][INFO ][o.o.m.c.MLCommonsClusterManagerEventListener] [opensearch-0] Starting ML sync up job...\n[2025-08-19T20:38:17,652][INFO ][o.o.p.PluginsService     ] [opensearch-0] PluginService:onIndexModule index:[otel/K4Gg9EiGQjiXEN5Z_M7Vjg]\n[2025-08-19T20:38:17,653][INFO ][o.o.c.m.MetadataCreateIndexService] [opensearch-0] [otel] creating index, cause [auto(bulk api)], templates [], shards [1]/[1]\n[2025-08-19T20:38:17,697][INFO ][o.o.p.PluginsService     ] [opensearch-0] PluginService:onIndexModule index:[otel/K4Gg9EiGQjiXEN5Z_M7Vjg]\n[2025-08-19T20:38:17,787][INFO ][o.o.p.PluginsService     ] [opensearch-0] PluginService:onIndexModule index:[otel/K4Gg9EiGQjiXEN5Z_M7Vjg]\n[2025-08-19T20:38:17,791][INFO ][o.o.c.m.MetadataMappingService] [opensearch-0] [otel/K4Gg9EiGQjiXEN5Z_M7Vjg] create_mapping\n[2025-08-19T20:38:17,836][INFO ][o.o.p.PluginsService     ] [opensearch-0] PluginService:onIndexModule index:[otel/K4Gg9EiGQjiXEN5Z_M7Vjg]\n[2025-08-19T20:38:17,842][INFO ][o.o.c.m.MetadataMappingService] [opensearch-0] [otel/K4Gg9EiGQjiXEN5Z_M7Vjg] update_mapping [_doc]\n[2025-08-19T20:38:17,890][INFO ][o.o.p.PluginsService     ] [opensearch-0] PluginService:onIndexModule index:[otel/K4Gg9EiGQjiXEN5Z_M7Vjg]\n[2025-08-19T20:38:17,895][INFO ][o.o.c.m.MetadataMappingService] [opensearch-0] [otel/K4Gg9EiGQjiXEN5Z_M7Vjg] update_mapping [_doc]\n[2025-08-19T20:38:18,196][INFO ][o.o.p.PluginsService     ] [opensearch-0] PluginService:onIndexModule index:[otel/K4Gg9EiGQjiXEN5Z_M7Vjg]\n[2025-08-19T20:38:18,203][INFO ][o.o.c.m.MetadataMappingService] [opensearch-0] [otel/K4Gg9EiGQjiXEN5Z_M7Vjg] update_mapping [_doc]\n[2025-08-19T20:38:18,225][INFO ][o.o.p.PluginsService     ] [opensearch-0] PluginService:onIndexModule index:[otel/K4Gg9EiGQjiXEN5Z_M7Vjg]\n[2025-08-19T20:38:18,230][INFO ][o.o.c.m.MetadataMappingService] [opensearch-0] [otel/K4Gg9EiGQjiXEN5Z_M7Vjg] update_mapping [_doc]\n[2025-08-19T20:38:18,476][INFO ][o.o.p.PluginsService     ] [opensearch-0] PluginService:onIndexModule index:[otel/K4Gg9EiGQjiXEN5Z_M7Vjg]\n[2025-08-19T20:38:18,481][INFO ][o.o.c.m.MetadataMappingService] [opensearch-0] [otel/K4Gg9EiGQjiXEN5Z_M7Vjg] update_mapping [_doc]\n[2025-08-19T20:38:19,227][INFO ][o.o.p.PluginsService     ] [opensearch-0] PluginService:onIndexModule index:[otel/K4Gg9EiGQjiXEN5Z_M7Vjg]\n[2025-08-19T20:38:19,237][INFO ][o.o.c.m.MetadataMappingService] [opensearch-0] [otel/K4Gg9EiGQjiXEN5Z_M7Vjg] update_mapping [_doc]\n[2025-08-19T20:38:19,286][INFO ][o.o.p.PluginsService     ] [opensearch-0] PluginService:onIndexModule index:[otel/K4Gg9EiGQjiXEN5Z_M7Vjg]\n[2025-08-19T20:38:19,291][INFO ][o.o.c.m.MetadataMappingService] [opensearch-0] [otel/K4Gg9EiGQjiXEN5Z_M7Vjg] update_mapping [_doc]\n[2025-08-19T20:38:27,436][INFO ][o.o.p.PluginsService     ] [opensearch-0] PluginService:onIndexModule index:[.plugins-ml-config/hz4n_wD-SRKMiiezbSNTIg]\n[2025-08-19T20:38:27,451][INFO ][o.o.c.m.MetadataCreateIndexService] [opensearch-0] [.plugins-ml-config] creating index, cause [api], templates [], shards [1]/[1]\n[2025-08-19T20:38:27,454][INFO ][o.o.c.r.a.AllocationService] [opensearch-0] updating number_of_replicas to [0] for indices [.plugins-ml-config]\n[2025-08-19T20:38:27,525][INFO ][o.o.p.PluginsService     ] [opensearch-0] PluginService:onIndexModule index:[.plugins-ml-config/hz4n_wD-SRKMiiezbSNTIg]\n[2025-08-19T20:38:27,628][INFO ][o.o.m.e.i.MLIndicesHandler] [opensearch-0] create index:.plugins-ml-config\n[2025-08-19T20:38:27,646][INFO ][o.o.m.c.MLSyncUpCron     ] [opensearch-0] ML configuration initialized successfully\n[2025-08-19T20:39:07,429][INFO ][o.o.i.i.ManagedIndexCoordinator] [opensearch-0] Performing move cluster state metadata.\n[2025-08-19T20:39:07,432][INFO ][o.o.i.i.MetadataService  ] [opensearch-0] ISM config index not exist, so we cancel the metadata migration job.\n[2025-08-19T20:39:07,432][INFO ][o.o.i.i.ManagedIndexCoordinator] [opensearch-0] Performing ISM template migration.\n[2025-08-19T20:39:07,432][INFO ][o.o.i.i.m.ISMTemplateService] [opensearch-0] Doing ISM template migration 1 time.\n[2025-08-19T20:39:07,433][INFO ][o.o.i.i.m.ISMTemplateService] [opensearch-0] Use 2025-08-19T19:38:07.427Z as migrating ISM template last_updated_time\n[2025-08-19T20:39:07,433][INFO ][o.o.i.i.m.ISMTemplateService] [opensearch-0] ISM templates: {}\n[2025-08-19T20:39:07,433][INFO ][o.o.i.i.m.ISMTemplateService] [opensearch-0] Policies to update: []\n[2025-08-19T20:39:07,435][INFO ][o.o.i.i.m.ISMTemplateService] [opensearch-0] Failure experienced when migrating ISM Template and update ISM policies: {}\n[2025-08-19T20:39:07,523][INFO ][o.o.c.s.ClusterSettings  ] [opensearch-0] updating [plugins.index_state_management.template_migration.control] from [0] to [-1]\n[2025-08-19T20:39:07,528][INFO ][o.o.i.i.m.ISMTemplateService] [opensearch-0] Successfully update template migration setting\n"
        },
        {
            "role": "assistant",
            "content": "```\nexec_shell(\"kubectl get svc -n astronomy-shop -o wide\")\n```"
        },
        {
            "role": "env",
            "content": "NAME                  TYPE        CLUSTER-IP      EXTERNAL-IP   PORT(S)                                                            AGE   SELECTOR\nad                    ClusterIP   10.96.63.86     <none>        8080/TCP                                                           82s   opentelemetry.io/name=ad\ncart                  ClusterIP   10.96.192.113   <none>        8080/TCP                                                           82s   opentelemetry.io/name=cart\ncheckout              ClusterIP   10.96.68.228    <none>        8080/TCP                                                           82s   opentelemetry.io/name=checkout\ncurrency              ClusterIP   10.96.211.7     <none>        8080/TCP                                                           82s   opentelemetry.io/name=currency\nemail                 ClusterIP   10.96.6.166     <none>        8080/TCP                                                           82s   opentelemetry.io/name=email\nflagd                 ClusterIP   10.96.192.51    <none>        8013/TCP,8016/TCP,4000/TCP                                         82s   opentelemetry.io/name=flagd\nfrontend              ClusterIP   10.96.235.0     <none>        8080/TCP                                                           82s   opentelemetry.io/name=frontend\nfrontend-proxy        ClusterIP   10.96.139.77    <none>        8080/TCP                                                           82s   opentelemetry.io/name=frontend-proxy\ngrafana               ClusterIP   10.96.142.137   <none>        80/TCP                                                             82s   app.kubernetes.io/instance=astronomy-shop,app.kubernetes.io/name=grafana\nimage-provider        ClusterIP   10.96.52.194    <none>        8081/TCP                                                           82s   opentelemetry.io/name=image-provider\njaeger-agent          ClusterIP   None            <none>        5775/UDP,5778/TCP,6831/UDP,6832/UDP                                82s   app.kubernetes.io/component=all-in-one,app.kubernetes.io/instance=astronomy-shop,app.kubernetes.io/name=jaeger\njaeger-collector      ClusterIP   None            <none>        9411/TCP,14250/TCP,14267/TCP,14268/TCP,4317/TCP,4318/TCP           82s   app.kubernetes.io/component=all-in-one,app.kubernetes.io/instance=astronomy-shop,app.kubernetes.io/name=jaeger\njaeger-query          ClusterIP   None            <none>        16686/TCP,16685/TCP                                                82s   app.kubernetes.io/component=all-in-one,app.kubernetes.io/instance=astronomy-shop,app.kubernetes.io/name=jaeger\nkafka                 ClusterIP   10.96.135.143   <none>        9092/TCP,9093/TCP                                                  82s   opentelemetry.io/name=kafka\nload-generator        ClusterIP   10.96.112.156   <none>        8089/TCP                                                           82s   opentelemetry.io/name=load-generator\nopensearch            ClusterIP   10.96.144.4     <none>        9200/TCP,9300/TCP,9600/TCP                                         82s   app.kubernetes.io/instance=astronomy-shop,app.kubernetes.io/name=opensearch\nopensearch-headless   ClusterIP   None            <none>        9200/TCP,9300/TCP,9600/TCP                                         82s   app.kubernetes.io/instance=astronomy-shop,app.kubernetes.io/name=opensearch\notel-collector        ClusterIP   10.96.231.80    <none>        6831/UDP,14250/TCP,14268/TCP,8888/TCP,4317/TCP,4318/TCP,9411/TCP   82s   app.kubernetes.io/instance=astronomy-shop,app.kubernetes.io/name=opentelemetry-collector,component=standalone-collector\npayment               ClusterIP   10.96.210.55    <none>        8080/TCP                                                           82s   opentelemetry.io/name=payment\nproduct-catalog       ClusterIP   10.96.193.142   <none>        8080/TCP                                                           82s   opentelemetry.io/name=product-catalog\nprometheus            ClusterIP   10.96.158.72    <none>        9090/TCP                                                           82s   app.kubernetes.io/component=server,app.kubernetes.io/instance=astronomy-shop,app.kubernetes.io/name=prometheus\nquote                 ClusterIP   10.96.152.194   <none>        8080/TCP                                                           82s   opentelemetry.io/name=quote\nrecommendation        ClusterIP   10.96.100.122   <none>        8080/TCP                                                           82s   opentelemetry.io/name=recommendation\nshipping              ClusterIP   10.96.174.163   <none>        8080/TCP                                                           82s   opentelemetry.io/name=shipping\nvalkey-cart           ClusterIP   10.96.5.114     <none>        6379/TCP                                                           82s   opentelemetry.io/name=valkey-cart\n"
        },
        {
            "role": "assistant",
            "content": "```\nsubmit(\"Yes\")\n```"
        },
        {
            "role": "env",
            "content": "1"
        }
    ],
    "results": {
        "Detection Accuracy": "Correct",
        "TTD": 54.11619591712952,
        "steps": 8,
        "in_tokens": 30094,
        "out_tokens": 182
    }
}