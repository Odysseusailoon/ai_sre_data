{
    "agent": "gpt-w-shell",
    "session_id": "0af5bb4f-120e-42c7-8bc4-ab8bcdfb3f7e",
    "problem_id": "astronomy_shop_ad_service_high_cpu-detection-1",
    "start_time": 1755628128.8676531,
    "end_time": 1755628160.686599,
    "trace": [
        {
            "role": "assistant",
            "content": "```\nexec_shell(\"kubectl get pods -n astronomy-shop -o wide\")\n```"
        },
        {
            "role": "env",
            "content": "NAME                               READY   STATUS     RESTARTS     AGE   IP             NODE          NOMINATED NODE   READINESS GATES\naccounting-567f87bbcd-wmjdd        1/1     Running    0            34s   10.244.1.96    kind-worker   <none>           <none>\nad-856c77f595-sz4pz                1/1     Running    0            31s   10.244.1.109   kind-worker   <none>           <none>\ncart-6fd857c656-t47rk              1/1     Running    0            33s   10.244.1.102   kind-worker   <none>           <none>\ncheckout-5bc54f8cd8-q6nb4          1/1     Running    0            32s   10.244.1.108   kind-worker   <none>           <none>\ncurrency-5864f94d89-f84vj          1/1     Running    0            33s   10.244.1.104   kind-worker   <none>           <none>\nemail-59d9ffc758-9kdl7             1/1     Running    0            33s   10.244.1.103   kind-worker   <none>           <none>\nflagd-574bcddb7b-fgdwt             0/2     Init:0/1   0            1s    <none>         kind-worker   <none>           <none>\nflagd-86575d5796-qnhj8             2/2     Running    0            34s   10.244.1.88    kind-worker   <none>           <none>\nfraud-detection-65868bcdb5-q95pt   1/1     Running    0            32s   10.244.1.106   kind-worker   <none>           <none>\nfrontend-7d6b77c58-hxsc9           1/1     Running    0            33s   10.244.1.100   kind-worker   <none>           <none>\nfrontend-proxy-548b4649d7-sx6rv    1/1     Running    0            32s   10.244.1.105   kind-worker   <none>           <none>\ngrafana-6b9fb84fb7-wswrw           1/1     Running    0            34s   10.244.1.86    kind-worker   <none>           <none>\nimage-provider-7f5bc6bf-vvx65      1/1     Running    0            34s   10.244.1.97    kind-worker   <none>           <none>\njaeger-55f499595b-9nm8d            1/1     Running    0            34s   10.244.1.99    kind-worker   <none>           <none>\nkafka-595d889c56-vdgj9             1/1     Running    0            31s   10.244.1.110   kind-worker   <none>           <none>\nload-generator-68b6f899f5-548cw    1/1     Running    0            34s   10.244.1.83    kind-worker   <none>           <none>\nopensearch-0                       1/1     Running    0            34s   10.244.1.89    kind-worker   <none>           <none>\notel-collector-758b8784c-5c2j5     1/1     Running    0            34s   10.244.1.85    kind-worker   <none>           <none>\npayment-7597577cb5-n25z2           1/1     Running    0            34s   10.244.1.82    kind-worker   <none>           <none>\nproduct-catalog-6988cd5f49-jfpfg   1/1     Running    1 (6s ago)   31s   10.244.1.111   kind-worker   <none>           <none>\nprometheus-58b7bfb774-8flg4        1/1     Running    0            33s   10.244.1.101   kind-worker   <none>           <none>\nquote-78bf8788-2jjdn               1/1     Running    0            34s   10.244.1.84    kind-worker   <none>           <none>\nrecommendation-7b49c8cb75-lqmw4    1/1     Running    0            34s   10.244.1.87    kind-worker   <none>           <none>\nshipping-7f445d59dc-fsn7v          1/1     Running    0            32s   10.244.1.107   kind-worker   <none>           <none>\nvalkey-cart-6475dcc67f-brcbh       1/1     Running    0            34s   10.244.1.98    kind-worker   <none>           <none>\n"
        },
        {
            "role": "assistant",
            "content": "```\nexec_shell(\"kubectl describe pod flagd-574bcddb7b-fgdwt -n astronomy-shop\")\n```"
        },
        {
            "role": "env",
            "content": "Name:             flagd-574bcddb7b-fgdwt\nNamespace:        astronomy-shop\nPriority:         0\nService Account:  astronomy-shop\nNode:             kind-worker/192.168.97.2\nStart Time:       Tue, 19 Aug 2025 18:28:48 +0000\nLabels:           app.kubernetes.io/component=flagd\n                  app.kubernetes.io/name=flagd\n                  opentelemetry.io/name=flagd\n                  pod-template-hash=574bcddb7b\nAnnotations:      kubectl.kubernetes.io/restartedAt: 2025-08-19T11:28:48-07:00\nStatus:           Running\nIP:               10.244.1.112\nIPs:\n  IP:           10.244.1.112\nControlled By:  ReplicaSet/flagd-574bcddb7b\nInit Containers:\n  init-config:\n    Container ID:  containerd://3acf6056dc0f225d2f8199208b2357f1e9c2d25078a48de9d4b7393916ea4628\n    Image:         busybox\n    Image ID:      docker.io/library/busybox@sha256:ab33eacc8251e3807b85bb6dba570e4698c3998eca6f0fc2ccb60575a563ea74\n    Port:          <none>\n    Host Port:     <none>\n    Command:\n      sh\n      -c\n      cp /config-ro/demo.flagd.json /config-rw/demo.flagd.json && cat /config-rw/demo.flagd.json\n    State:          Terminated\n      Reason:       Completed\n      Exit Code:    0\n      Started:      Tue, 19 Aug 2025 18:28:50 +0000\n      Finished:     Tue, 19 Aug 2025 18:28:50 +0000\n    Ready:          True\n    Restart Count:  0\n    Environment:    <none>\n    Mounts:\n      /config-ro from config-ro (rw)\n      /config-rw from config-rw (rw)\n      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-zz2tx (ro)\nContainers:\n  flagd:\n    Container ID:  containerd://0130d1e12b317e909c4809d06c4b9c7e0efcc57b275b40bba8b2c05baefb14c7\n    Image:         ghcr.io/open-feature/flagd:v0.12.8\n    Image ID:      ghcr.io/open-feature/flagd@sha256:634bd05995f5fd0a1b8196822feb8c2d99d37be9e2a25cb17b5d6c4c8fd7e821\n    Ports:         8013/TCP, 8016/TCP\n    Host Ports:    0/TCP, 0/TCP\n    Command:\n      /flagd-build\n      start\n      --port\n      8013\n      --ofrep-port\n      8016\n      --uri\n      file:./etc/flagd/demo.flagd.json\n    State:          Running\n      Started:      Tue, 19 Aug 2025 18:28:51 +0000\n    Ready:          True\n    Restart Count:  0\n    Limits:\n      memory:  75Mi\n    Requests:\n      memory:  75Mi\n    Environment:\n      OTEL_SERVICE_NAME:                                   (v1:metadata.labels['app.kubernetes.io/component'])\n      OTEL_COLLECTOR_NAME:                                otel-collector\n      OTEL_EXPORTER_OTLP_METRICS_TEMPORALITY_PREFERENCE:  cumulative\n      FLAGD_METRICS_EXPORTER:                             otel\n      FLAGD_OTEL_COLLECTOR_URI:                           $(OTEL_COLLECTOR_NAME):4317\n      OTEL_RESOURCE_ATTRIBUTES:                           service.name=$(OTEL_SERVICE_NAME),service.namespace=opentelemetry-demo,service.version=2.0.2\n    Mounts:\n      /etc/flagd from config-rw (rw)\n      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-zz2tx (ro)\n  flagd-ui:\n    Container ID:   containerd://ce9cfbfaa5878edfc9606db030f8afaceb6068ee5f60f5814f1d6340afaed713\n    Image:          ghcr.io/open-telemetry/demo:2.0.2-flagd-ui\n    Image ID:       ghcr.io/open-telemetry/demo@sha256:655da6a8735b7bb7da2c36c8992387d03ae06420f2a06fbc499f2591889c1d03\n    Port:           4000/TCP\n    Host Port:      0/TCP\n    State:          Running\n      Started:      Tue, 19 Aug 2025 18:28:51 +0000\n    Ready:          True\n    Restart Count:  0\n    Limits:\n      memory:  100Mi\n    Requests:\n      memory:  100Mi\n    Environment:\n      OTEL_SERVICE_NAME:                                   (v1:metadata.labels['app.kubernetes.io/component'])\n      OTEL_COLLECTOR_NAME:                                otel-collector\n      OTEL_EXPORTER_OTLP_METRICS_TEMPORALITY_PREFERENCE:  cumulative\n      FLAGD_METRICS_EXPORTER:                             otel\n      OTEL_EXPORTER_OTLP_ENDPOINT:                        http://$(OTEL_COLLECTOR_NAME):4318\n      OTEL_RESOURCE_ATTRIBUTES:                           service.name=$(OTEL_SERVICE_NAME),service.namespace=opentelemetry-demo,service.version=2.0.2\n    Mounts:\n      /app/data from config-rw (rw)\n      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-zz2tx (ro)\nConditions:\n  Type                        Status\n  PodReadyToStartContainers   True \n  Initialized                 True \n  Ready                       True \n  ContainersReady             True \n  PodScheduled                True \nVolumes:\n  config-rw:\n    Type:       EmptyDir (a temporary directory that shares a pod's lifetime)\n    Medium:     \n    SizeLimit:  <unset>\n  config-ro:\n    Type:      ConfigMap (a volume populated by a ConfigMap)\n    Name:      flagd-config\n    Optional:  false\n  kube-api-access-zz2tx:\n    Type:                    Projected (a volume that contains injected data from multiple sources)\n    TokenExpirationSeconds:  3607\n    ConfigMapName:           kube-root-ca.crt\n    ConfigMapOptional:       <nil>\n    DownwardAPI:             true\nQoS Class:                   Burstable\nNode-Selectors:              <none>\nTolerations:                 node.kubernetes.io/not-ready:NoExecute op=Exists for 300s\n                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s\nEvents:\n  Type    Reason     Age   From               Message\n  ----    ------     ----  ----               -------\n  Normal  Scheduled  6s    default-scheduler  Successfully assigned astronomy-shop/flagd-574bcddb7b-fgdwt to kind-worker\n  Normal  Pulling    5s    kubelet            Pulling image \"busybox\"\n  Normal  Pulled     4s    kubelet            Successfully pulled image \"busybox\" in 966ms (966ms including waiting). Image size: 1913387 bytes.\n  Normal  Created    4s    kubelet            Created container: init-config\n  Normal  Started    4s    kubelet            Started container init-config\n  Normal  Pulled     3s    kubelet            Container image \"ghcr.io/open-feature/flagd:v0.12.8\" already present on machine\n  Normal  Created    3s    kubelet            Created container: flagd\n  Normal  Started    3s    kubelet            Started container flagd\n  Normal  Pulled     3s    kubelet            Container image \"ghcr.io/open-telemetry/demo:2.0.2-flagd-ui\" already present on machine\n  Normal  Created    3s    kubelet            Created container: flagd-ui\n  Normal  Started    3s    kubelet            Started container flagd-ui\n"
        },
        {
            "role": "assistant",
            "content": "```\nexec_shell(\"kubectl logs product-catalog-6988cd5f49-jfpfg -n astronomy-shop --previous\")\n```"
        },
        {
            "role": "env",
            "content": "time=\"2025-08-19T18:28:19Z\" level=info msg=\"Loading Product Catalog...\"\ntime=\"2025-08-19T18:28:19Z\" level=info msg=\"Loaded 10 products\"\ntime=\"2025-08-19T18:28:19Z\" level=info msg=\"Product Catalog reload interval: 10\"\ntime=\"2025-08-19T18:28:19Z\" level=info msg=\"Product Catalog gRPC server started on port: 8080\"\ntime=\"2025-08-19T18:28:29Z\" level=info msg=\"Reloading Product Catalog...\"\n2025/08/19 18:28:36 traces export: context deadline exceeded: rpc error: code = DeadlineExceeded desc = context deadline exceeded\n"
        },
        {
            "role": "assistant",
            "content": "```\nexec_shell(\"kubectl logs otel-collector-758b8784c-5c2j5 -n astronomy-shop\")\n```"
        },
        {
            "role": "env",
            "content": "2025-08-19T18:28:16.697Z\tinfo\tservice@v0.131.0/service.go:214\tSetting up own telemetry...\t{\"resource\": {\"service.instance.id\": \"2e7d82f3-fb74-414f-a45c-c6b01d1a215a\", \"service.name\": \"otelcol-contrib\", \"service.version\": \"0.131.0\"}}\n2025-08-19T18:28:16.702Z\tinfo\tmemorylimiter@v0.131.0/memorylimiter.go:149\tUsing percentage memory limiter\t{\"resource\": {\"service.instance.id\": \"2e7d82f3-fb74-414f-a45c-c6b01d1a215a\", \"service.name\": \"otelcol-contrib\", \"service.version\": \"0.131.0\"}, \"otelcol.component.kind\": \"processor\", \"total_memory_mib\": 200, \"limit_percentage\": 80, \"spike_limit_percentage\": 25}\n2025-08-19T18:28:16.703Z\tinfo\tmemorylimiter@v0.131.0/memorylimiter.go:74\tMemory limiter configured\t{\"resource\": {\"service.instance.id\": \"2e7d82f3-fb74-414f-a45c-c6b01d1a215a\", \"service.name\": \"otelcol-contrib\", \"service.version\": \"0.131.0\"}, \"otelcol.component.kind\": \"processor\", \"limit_mib\": 160, \"spike_limit_mib\": 50, \"check_interval\": 5}\n2025-08-19T18:28:16.705Z\tinfo\tspanmetricsconnector@v0.131.0/connector.go:114\tBuilding spanmetrics connector\t{\"resource\": {\"service.instance.id\": \"2e7d82f3-fb74-414f-a45c-c6b01d1a215a\", \"service.name\": \"otelcol-contrib\", \"service.version\": \"0.131.0\"}, \"otelcol.component.id\": \"spanmetrics\", \"otelcol.component.kind\": \"connector\", \"otelcol.signal\": \"traces\", \"otelcol.signal.output\": \"metrics\"}\n2025-08-19T18:28:16.708Z\tinfo\tottl@v0.131.0/parser_collection.go:646\tone or more paths were modified to include their context prefix, please rewrite them accordingly\t{\"resource\": {\"service.instance.id\": \"2e7d82f3-fb74-414f-a45c-c6b01d1a215a\", \"service.name\": \"otelcol-contrib\", \"service.version\": \"0.131.0\"}, \"otelcol.component.id\": \"transform\", \"otelcol.component.kind\": \"processor\", \"otelcol.pipeline.id\": \"traces\", \"otelcol.signal\": \"traces\", \"values\": {\"[0]\": {\"original\": \"replace_pattern(name, \\\"\\\\\\\\?.*\\\", \\\"\\\")\", \"modified\": \"replace_pattern(span.name, \\\"\\\\\\\\?.*\\\", \\\"\\\")\"}, \"[1]\": {\"original\": \"replace_match(name, \\\"GET /api/products/*\\\", \\\"GET /api/products/{productId}\\\")\", \"modified\": \"replace_match(span.name, \\\"GET /api/products/*\\\", \\\"GET /api/products/{productId}\\\")\"}}}\n2025-08-19T18:28:16.711Z\tinfo\tbuilders/extension.go:50\tUnmaintained component. Actively looking for contributors. Component will become deprecated after 3 months of remaining unmaintained.\t{\"resource\": {\"service.instance.id\": \"2e7d82f3-fb74-414f-a45c-c6b01d1a215a\", \"service.name\": \"otelcol-contrib\", \"service.version\": \"0.131.0\"}, \"otelcol.component.id\": \"health_check\", \"otelcol.component.kind\": \"extension\"}\n2025-08-19T18:28:16.712Z\tinfo\tservice@v0.131.0/service.go:276\tStarting otelcol-contrib...\t{\"resource\": {\"service.instance.id\": \"2e7d82f3-fb74-414f-a45c-c6b01d1a215a\", \"service.name\": \"otelcol-contrib\", \"service.version\": \"0.131.0\"}, \"Version\": \"0.131.0\", \"NumCPU\": 16}\n2025-08-19T18:28:16.712Z\tinfo\textensions/extensions.go:41\tStarting extensions...\t{\"resource\": {\"service.instance.id\": \"2e7d82f3-fb74-414f-a45c-c6b01d1a215a\", \"service.name\": \"otelcol-contrib\", \"service.version\": \"0.131.0\"}}\n2025-08-19T18:28:16.712Z\tinfo\textensions/extensions.go:45\tExtension is starting...\t{\"resource\": {\"service.instance.id\": \"2e7d82f3-fb74-414f-a45c-c6b01d1a215a\", \"service.name\": \"otelcol-contrib\", \"service.version\": \"0.131.0\"}, \"otelcol.component.id\": \"health_check\", \"otelcol.component.kind\": \"extension\"}\n2025-08-19T18:28:16.712Z\tinfo\thealthcheckextension@v0.131.0/healthcheckextension.go:32\tStarting health_check extension\t{\"resource\": {\"service.instance.id\": \"2e7d82f3-fb74-414f-a45c-c6b01d1a215a\", \"service.name\": \"otelcol-contrib\", \"service.version\": \"0.131.0\"}, \"otelcol.component.id\": \"health_check\", \"otelcol.component.kind\": \"extension\", \"config\": {\"Endpoint\":\"10.244.1.85:13133\",\"TLS\":{},\"CORS\":{},\"Auth\":{},\"MaxRequestBodySize\":0,\"IncludeMetadata\":false,\"ResponseHeaders\":null,\"CompressionAlgorithms\":null,\"ReadTimeout\":0,\"ReadHeaderTimeout\":0,\"WriteTimeout\":0,\"IdleTimeout\":0,\"Middlewares\":null,\"Path\":\"/\",\"ResponseBody\":null,\"CheckCollectorPipeline\":{\"Enabled\":false,\"Interval\":\"5m\",\"ExporterFailureThreshold\":5}}}\n2025-08-19T18:28:16.713Z\tinfo\textensions/extensions.go:62\tExtension started.\t{\"resource\": {\"service.instance.id\": \"2e7d82f3-fb74-414f-a45c-c6b01d1a215a\", \"service.name\": \"otelcol-contrib\", \"service.version\": \"0.131.0\"}, \"otelcol.component.id\": \"health_check\", \"otelcol.component.kind\": \"extension\"}\n2025-08-19T18:28:16.719Z\tinfo\tkube/client.go:172\tk8s filtering\t{\"resource\": {\"service.instance.id\": \"2e7d82f3-fb74-414f-a45c-c6b01d1a215a\", \"service.name\": \"otelcol-contrib\", \"service.version\": \"0.131.0\"}, \"otelcol.component.id\": \"k8sattributes\", \"otelcol.component.kind\": \"processor\", \"otelcol.pipeline.id\": \"metrics\", \"otelcol.signal\": \"metrics\", \"labelSelector\": \"\", \"fieldSelector\": \"\"}\n2025-08-19T18:28:16.722Z\tinfo\totlpreceiver@v0.131.0/otlp.go:117\tStarting GRPC server\t{\"resource\": {\"service.instance.id\": \"2e7d82f3-fb74-414f-a45c-c6b01d1a215a\", \"service.name\": \"otelcol-contrib\", \"service.version\": \"0.131.0\"}, \"otelcol.component.id\": \"otlp\", \"otelcol.component.kind\": \"receiver\", \"endpoint\": \"10.244.1.85:4317\"}\n2025-08-19T18:28:16.727Z\tinfo\totlpreceiver@v0.131.0/otlp.go:175\tStarting HTTP server\t{\"resource\": {\"service.instance.id\": \"2e7d82f3-fb74-414f-a45c-c6b01d1a215a\", \"service.name\": \"otelcol-contrib\", \"service.version\": \"0.131.0\"}, \"otelcol.component.id\": \"otlp\", \"otelcol.component.kind\": \"receiver\", \"endpoint\": \"10.244.1.85:4318\"}\n2025-08-19T18:28:16.737Z\tinfo\tspanmetricsconnector@v0.131.0/connector.go:207\tStarting spanmetrics connector\t{\"resource\": {\"service.instance.id\": \"2e7d82f3-fb74-414f-a45c-c6b01d1a215a\", \"service.name\": \"otelcol-contrib\", \"service.version\": \"0.131.0\"}, \"otelcol.component.id\": \"spanmetrics\", \"otelcol.component.kind\": \"connector\", \"otelcol.signal\": \"traces\", \"otelcol.signal.output\": \"metrics\"}\n2025-08-19T18:28:16.745Z\tinfo\tkube/client.go:172\tk8s filtering\t{\"resource\": {\"service.instance.id\": \"2e7d82f3-fb74-414f-a45c-c6b01d1a215a\", \"service.name\": \"otelcol-contrib\", \"service.version\": \"0.131.0\"}, \"otelcol.component.id\": \"k8sattributes\", \"otelcol.component.kind\": \"processor\", \"otelcol.pipeline.id\": \"traces\", \"otelcol.signal\": \"traces\", \"labelSelector\": \"\", \"fieldSelector\": \"\"}\n2025-08-19T18:28:16.745Z\twarn\tgrpc@v1.74.2/clientconn.go:1414\t[core] [Channel #3 SubChannel #5]grpc: addrConn.createTransport failed to connect to {Addr: \"jaeger-collector:4317\", ServerName: \"jaeger-collector:4317\", }. Err: connection error: desc = \"transport: Error while dialing: dial tcp: lookup jaeger-collector on 10.96.0.10:53: no such host\"\t{\"resource\": {\"service.instance.id\": \"2e7d82f3-fb74-414f-a45c-c6b01d1a215a\", \"service.name\": \"otelcol-contrib\", \"service.version\": \"0.131.0\"}, \"grpc_log\": true}\n2025-08-19T18:28:16.751Z\tinfo\tjaegerreceiver@v0.131.0/trace_receiver.go:233\tStarting UDP server for Compact Thrift\t{\"resource\": {\"service.instance.id\": \"2e7d82f3-fb74-414f-a45c-c6b01d1a215a\", \"service.name\": \"otelcol-contrib\", \"service.version\": \"0.131.0\"}, \"otelcol.component.id\": \"jaeger\", \"otelcol.component.kind\": \"receiver\", \"otelcol.signal\": \"traces\", \"endpoint\": \"10.244.1.85:6831\"}\n2025-08-19T18:28:16.759Z\tinfo\tjaegerreceiver@v0.131.0/trace_receiver.go:337\tStarting HTTP server for Jaeger Thrift\t{\"resource\": {\"service.instance.id\": \"2e7d82f3-fb74-414f-a45c-c6b01d1a215a\", \"service.name\": \"otelcol-contrib\", \"service.version\": \"0.131.0\"}, \"otelcol.component.id\": \"jaeger\", \"otelcol.component.kind\": \"receiver\", \"otelcol.signal\": \"traces\", \"endpoint\": \"10.244.1.85:14268\"}\n2025-08-19T18:28:16.764Z\tinfo\tjaegerreceiver@v0.131.0/trace_receiver.go:362\tStarting gRPC server for Jaeger Protobuf\t{\"resource\": {\"service.instance.id\": \"2e7d82f3-fb74-414f-a45c-c6b01d1a215a\", \"service.name\": \"otelcol-contrib\", \"service.version\": \"0.131.0\"}, \"otelcol.component.id\": \"jaeger\", \"otelcol.component.kind\": \"receiver\", \"otelcol.signal\": \"traces\", \"endpoint\": \"10.244.1.85:14250\"}\n2025-08-19T18:28:16.770Z\tinfo\tkube/client.go:172\tk8s filtering\t{\"resource\": {\"service.instance.id\": \"2e7d82f3-fb74-414f-a45c-c6b01d1a215a\", \"service.name\": \"otelcol-contrib\", \"service.version\": \"0.131.0\"}, \"otelcol.component.id\": \"k8sattributes\", \"otelcol.component.kind\": \"processor\", \"otelcol.pipeline.id\": \"logs\", \"otelcol.signal\": \"logs\", \"labelSelector\": \"\", \"fieldSelector\": \"\"}\n2025-08-19T18:28:16.773Z\tinfo\thealthcheck/handler.go:131\tHealth Check state change\t{\"resource\": {\"service.instance.id\": \"2e7d82f3-fb74-414f-a45c-c6b01d1a215a\", \"service.name\": \"otelcol-contrib\", \"service.version\": \"0.131.0\"}, \"otelcol.component.id\": \"health_check\", \"otelcol.component.kind\": \"extension\", \"status\": \"ready\"}\n2025-08-19T18:28:16.774Z\tinfo\tservice@v0.131.0/service.go:299\tEverything is ready. Begin running and processing data.\t{\"resource\": {\"service.instance.id\": \"2e7d82f3-fb74-414f-a45c-c6b01d1a215a\", \"service.name\": \"otelcol-contrib\", \"service.version\": \"0.131.0\"}}\n2025-08-19T18:28:17.758Z\twarn\tgrpc@v1.74.2/clientconn.go:1414\t[core] [Channel #3 SubChannel #5]grpc: addrConn.createTransport failed to connect to {Addr: \"jaeger-collector:4317\", ServerName: \"jaeger-collector:4317\", }. Err: connection error: desc = \"transport: Error while dialing: dial tcp: lookup jaeger-collector on 10.96.0.10:53: no such host\"\t{\"resource\": {\"service.instance.id\": \"2e7d82f3-fb74-414f-a45c-c6b01d1a215a\", \"service.name\": \"otelcol-contrib\", \"service.version\": \"0.131.0\"}, \"grpc_log\": true}\n2025-08-19T18:28:17.917Z\tinfo\tMetrics\t{\"resource\": {\"service.instance.id\": \"2e7d82f3-fb74-414f-a45c-c6b01d1a215a\", \"service.name\": \"otelcol-contrib\", \"service.version\": \"0.131.0\"}, \"otelcol.component.id\": \"debug\", \"otelcol.component.kind\": \"exporter\", \"otelcol.signal\": \"metrics\", \"resource metrics\": 2, \"metrics\": 29, \"data points\": 38}\n2025-08-19T18:28:17.924Z\tinfo\tinternal/retry_sender.go:133\tExporting failed. Will retry the request after interval.\t{\"resource\": {\"service.instance.id\": \"2e7d82f3-fb74-414f-a45c-c6b01d1a215a\", \"service.name\": \"otelcol-contrib\", \"service.version\": \"0.131.0\"}, \"otelcol.component.id\": \"otlphttp/prometheus\", \"otelcol.component.kind\": \"exporter\", \"otelcol.signal\": \"metrics\", \"error\": \"failed to make an HTTP request: Post \\\"http://prometheus:9090/api/v1/otlp/v1/metrics\\\": dial tcp 10.96.227.216:9090: connect: connection refused\", \"interval\": \"3.994329278s\"}\n2025-08-19T18:28:19.560Z\tinfo\tTraces\t{\"resource\": {\"service.instance.id\": \"2e7d82f3-fb74-414f-a45c-c6b01d1a215a\", \"service.name\": \"otelcol-contrib\", \"service.version\": \"0.131.0\"}, \"otelcol.component.id\": \"debug\", \"otelcol.component.kind\": \"exporter\", \"otelcol.signal\": \"traces\", \"resource spans\": 4, \"spans\": 20}\n2025-08-19T18:28:19.762Z\tinfo\tTraces\t{\"resource\": {\"service.instance.id\": \"2e7d82f3-fb74-414f-a45c-c6b01d1a215a\", \"service.name\": \"otelcol-contrib\", \"service.version\": \"0.131.0\"}, \"otelcol.component.id\": \"debug\", \"otelcol.component.kind\": \"exporter\", \"otelcol.signal\": \"traces\", \"resource spans\": 10, \"spans\": 42}\n2025-08-19T18:28:19.764Z\terror\topensearchexporter@v0.131.0/logger.go:36\tRequest failed.\t{\"resource\": {\"service.instance.id\": \"2e7d82f3-fb74-414f-a45c-c6b01d1a215a\", \"service.name\": \"otelcol-contrib\", \"service.version\": \"0.131.0\"}, \"otelcol.component.id\": \"opensearch\", \"otelcol.component.kind\": \"exporter\", \"otelcol.signal\": \"logs\", \"path\": \"/_bulk\", \"method\": \"POST\", \"duration\": 0.00114887, \"reason\": \"dial tcp 10.96.44.10:9200: connect: connection refused\"}\ngithub.com/open-telemetry/opentelemetry-collector-contrib/exporter/opensearchexporter.(*clientLogger).LogRoundTrip\n\tgithub.com/open-telemetry/opentelemetry-collector-contrib/exporter/opensearchexporter@v0.131.0/logger.go:36\ngithub.com/opensearch-project/opensearch-go/v2/opensearchtransport.(*Client).logRoundTrip\n\tgithub.com/opensearch-project/opensearch-go/v2@v2.3.0/opensearchtransport/opensearchtransport.go:498\ngithub.com/opensearch-project/opensearch-go/v2/opensearchtransport.(*Client).Perform\n\tgithub.com/opensearch-project/opensearch-go/v2@v2.3.0/opensearchtransport/opensearchtransport.go:337\ngithub.com/opensearch-project/opensearch-go/v2.(*Client).Perform\n\tgithub.com/opensearch-project/opensearch-go/v2@v2.3.0/opensearch.go:267\ngithub.com/opensearch-project/opensearch-go/v2/opensearchapi.BulkRequest.Do\n\tgithub.com/opensearch-project/opensearch-go/v2@v2.3.0/opensearchapi/api.bulk.go:192\ngithub.com/opensearch-project/opensearch-go/v2/opensearchutil.(*worker).flush\n\tgithub.com/opensearch-project/opensearch-go/v2@v2.3.0/opensearchutil/bulk_indexer.go:552\ngithub.com/opensearch-project/opensearch-go/v2/opensearchutil.(*bulkIndexer).Close\n\tgithub.com/opensearch-project/opensearch-go/v2@v2.3.0/opensearchutil/bulk_indexer.go:299\ngithub.com/open-telemetry/opentelemetry-collector-contrib/exporter/opensearchexporter.(*logBulkIndexer).close\n\tgithub.com/open-telemetry/opentelemetry-collector-contrib/exporter/opensearchexporter@v0.131.0/log_bulk_indexer.go:41\ngithub.com/open-telemetry/opentelemetry-collector-contrib/exporter/opensearchexporter.(*logExporter).pushLogData\n\tgithub.com/open-telemetry/opentelemetry-collector-contrib/exporter/opensearchexporter@v0.131.0/sso_log_exporter.go:88\ngo.opentelemetry.io/collector/consumer.ConsumeLogsFunc.ConsumeLogs\n\tgo.opentelemetry.io/collector/consumer@v1.37.0/logs.go:27\ngo.opentelemetry.io/collector/exporter/exporterhelper.NewLogs.requestConsumeFromLogs.func2\n\tgo.opentelemetry.io/collector/exporter@v0.131.0/exporterhelper/logs.go:136\ngo.opentelemetry.io/collector/exporter/exporterhelper/internal/sender.(*sender[...]).Send\n\tgo.opentelemetry.io/collector/exporter@v0.131.0/exporterhelper/internal/sender/sender.go:31\ngo.opentelemetry.io/collector/exporter/exporterhelper/internal.(*retrySender).Send\n\tgo.opentelemetry.io/collector/exporter@v0.131.0/exporterhelper/internal/retry_sender.go:91\ngo.opentelemetry.io/collector/exporter/exporterhelper/internal.(*obsReportSender[...]).Send\n\tgo.opentelemetry.io/collector/exporter@v0.131.0/exporterhelper/internal/obs_report_sender.go:92\ngo.opentelemetry.io/collector/exporter/exporterhelper/internal.(*BaseExporter).Send\n\tgo.opentelemetry.io/collector/exporter@v0.131.0/exporterhelper/internal/base_exporter.go:115\ngo.opentelemetry.io/collector/exporter/exporterhelper.NewLogsRequest.newConsumeLogs.func1\n\tgo.opentelemetry.io/collector/exporter@v0.131.0/exporterhelper/logs.go:191\ngo.opentelemetry.io/collector/consumer.ConsumeLogsFunc.ConsumeLogs\n\tgo.opentelemetry.io/collector/consumer@v1.37.0/logs.go:27\ngo.opentelemetry.io/collector/internal/fanoutconsumer.(*logsConsumer).ConsumeLogs\n\tgo.opentelemetry.io/collector/internal/fanoutconsumer@v0.131.0/logs.go:64\ngo.opentelemetry.io/collector/processor/batchprocessor.(*batchLogs).export\n\tgo.opentelemetry.io/collector/processor/batchprocessor@v0.131.0/batch_processor.go:543\ngo.opentelemetry.io/collector/processor/batchprocessor.(*shard[...]).sendItems\n\tgo.opentelemetry.io/collector/processor/batchprocessor@v0.131.0/batch_processor.go:262\ngo.opentelemetry.io/collector/processor/batchprocessor.(*shard[...]).startLoop\n\tgo.opentelemetry.io/collector/processor/batchprocessor@v0.131.0/batch_processor.go:222\n2025-08-19T18:28:19.768Z\terror\tinternal/base_exporter.go:117\tExporting failed. Rejecting data. Try enabling sending_queue to survive temporary failures.\t{\"resource\": {\"service.instance.id\": \"2e7d82f3-fb74-414f-a45c-c6b01d1a215a\", \"service.name\": \"otelcol-contrib\", \"service.version\": \"0.131.0\"}, \"otelcol.component.id\": \"opensearch\", \"otelcol.component.kind\": \"exporter\", \"otelcol.signal\": \"logs\", \"error\": \"not retryable error: Permanent error: Permanent error: flush: dial tcp 10.96.44.10:9200: connect: connection refused\\nPermanent error: Permanent error: flush: dial tcp 10.96.44.10:9200: connect: connection refused\", \"rejected_items\": 37}\ngo.opentelemetry.io/collector/exporter/exporterhelper/internal.(*BaseExporter).Send\n\tgo.opentelemetry.io/collector/exporter@v0.131.0/exporterhelper/internal/base_exporter.go:117\ngo.opentelemetry.io/collector/exporter/exporterhelper.NewLogsRequest.newConsumeLogs.func1\n\tgo.opentelemetry.io/collector/exporter@v0.131.0/exporterhelper/logs.go:191\ngo.opentelemetry.io/collector/consumer.ConsumeLogsFunc.ConsumeLogs\n\tgo.opentelemetry.io/collector/consumer@v1.37.0/logs.go:27\ngo.opentelemetry.io/collector/internal/fanoutconsumer.(*logsConsumer).ConsumeLogs\n\tgo.opentelemetry.io/collector/internal/fanoutconsumer@v0.131.0/logs.go:64\ngo.opentelemetry.io/collector/processor/batchprocessor.(*batchLogs).export\n\tgo.opentelemetry.io/collector/processor/batchprocessor@v0.131.0/batch_processor.go:543\ngo.opentelemetry.io/collector/processor/batchprocessor.(*shard[...]).sendItems\n\tgo.opentelemetry.io/collector/processor/batchprocessor@v0.131.0/batch_processor.go:262\ngo.opentelemetry.io/collector/processor/batchprocessor.(*shard[...]).startLoop\n\tgo.opentelemetry.io/collector/processor/batchprocessor@v0.131.0/batch_processor.go:222\n2025-08-19T18:28:19.771Z\tinfo\tLogs\t{\"resource\": {\"service.instance.id\": \"2e7d82f3-fb74-414f-a45c-c6b01d1a215a\", \"service.name\": \"otelcol-contrib\", \"service.version\": \"0.131.0\"}, \"otelcol.component.id\": \"debug\", \"otelcol.component.kind\": \"exporter\", \"otelcol.signal\": \"logs\", \"resource logs\": 11, \"log records\": 37}\n2025-08-19T18:28:19.771Z\twarn\tbatchprocessor@v0.131.0/batch_processor.go:264\tSender failed\t{\"resource\": {\"service.instance.id\": \"2e7d82f3-fb74-414f-a45c-c6b01d1a215a\", \"service.name\": \"otelcol-contrib\", \"service.version\": \"0.131.0\"}, \"otelcol.component.id\": \"batch\", \"otelcol.component.kind\": \"processor\", \"otelcol.pipeline.id\": \"logs\", \"otelcol.signal\": \"logs\", \"error\": \"not retryable error: Permanent error: Permanent error: flush: dial tcp 10.96.44.10:9200: connect: connection refused\\nPermanent error: Permanent error: flush: dial tcp 10.96.44.10:9200: connect: connection refused\"}\n2025-08-19T18:28:19.965Z\tinfo\tTraces\t{\"resource\": {\"service.instance.id\": \"2e7d82f3-fb74-414f-a45c-c6b01d1a215a\", \"service.name\": \"otelcol-contrib\", \"service.version\": \"0.131.0\"}, \"otelcol.component.id\": \"debug\", \"otelcol.component.kind\": \"exporter\", \"otelcol.signal\": \"traces\", \"resource spans\": 4, \"spans\": 20}\n2025-08-19T18:28:19.983Z\terror\topensearchexporter@v0.131.0/logger.go:36\tRequest failed.\t{\"resource\": {\"service.instance.id\": \"2e7d82f3-fb74-414f-a45c-c6b01d1a215a\", \"service.name\": \"otelcol-contrib\", \"service.version\": \"0.131.0\"}, \"otelcol.component.id\": \"opensearch\", \"otelcol.component.kind\": \"exporter\", \"otelcol.signal\": \"logs\", \"path\": \"/_bulk\", \"method\": \"POST\", \"duration\": 0.005285808, \"reason\": \"dial tcp 10.96.44.10:9200: connect: connection refused\"}\ngithub.com/open-telemetry/opentelemetry-collector-contrib/exporter/opensearchexporter.(*clientLogger).LogRoundTrip\n\tgithub.com/open-telemetry/opentelemetry-collector-contrib/exporter/opensearchexporter@v0.131.0/logger.go:36\ngithub.com/opensearch-project/opensearch-go/v2/opensearchtransport.(*Client).logRoundTrip\n\tgithub.com/opensearch-project/opensearch-go/v2@v2.3.0/opensearchtransport/opensearchtransport.go:498\ngithub.com/opensearch-project/opensearch-go/v2/opensearchtransport.(*Client).Perform\n\tgithub.com/opensearch-project/opensearch-go/v2@v2.3.0/opensearchtransport/opensearchtransport.go:337\ngithub.com/opensearch-project/opensearch-go/v2.(*Client).Perform\n\tgithub.com/opensearch-project/opensearch-go/v2@v2.3.0/opensearch.go:267\ngithub.com/opensearch-project/opensearch-go/v2/opensearchapi.BulkRequest.Do\n\tgithub.com/opensearch-project/opensearch-go/v2@v2.3.0/opensearchapi/api.bulk.go:192\ngithub.com/opensearch-project/opensearch-go/v2/opensearchutil.(*worker).flush\n\tgithub.com/opensearch-project/opensearch-go/v2@v2.3.0/opensearchutil/bulk_indexer.go:552\ngithub.com/opensearch-project/opensearch-go/v2/opensearchutil.(*bulkIndexer).Close\n\tgithub.com/opensearch-project/opensearch-go/v2@v2.3.0/opensearchutil/bulk_indexer.go:299\ngithub.com/open-telemetry/opentelemetry-collector-contrib/exporter/opensearchexporter.(*logBulkIndexer).close\n\tgithub.com/open-telemetry/opentelemetry-collector-contrib/exporter/opensearchexporter@v0.131.0/log_bulk_indexer.go:41\ngithub.com/open-telemetry/opentelemetry-collector-contrib/exporter/opensearchexporter.(*logExporter).pushLogData\n\tgithub.com/open-telemetry/opentelemetry-collector-contrib/exporter/opensearchexporter@v0.131.0/sso_log_exporter.go:88\ngo.opentelemetry.io/collector/consumer.ConsumeLogsFunc.ConsumeLogs\n\tgo.opentelemetry.io/collector/consumer@v1.37.0/logs.go:27\ngo.opentelemetry.io/collector/exporter/exporterhelper.NewLogs.requestConsumeFromLogs.func2\n\tgo.opentelemetry.io/collector/exporter@v0.131.0/exporterhelper/logs.go:136\ngo.opentelemetry.io/collector/exporter/exporterhelper/internal/sender.(*sender[...]).Send\n\tgo.opentelemetry.io/collector/exporter@v0.131.0/exporterhelper/internal/sender/sender.go:31\ngo.opentelemetry.io/collector/exporter/exporterhelper/internal.(*retrySender).Send\n\tgo.opentelemetry.io/collector/exporter@v0.131.0/exporterhelper/internal/retry_sender.go:91\ngo.opentelemetry.io/collector/exporter/exporterhelper/internal.(*obsReportSender[...]).Send\n\tgo.opentelemetry.io/collector/exporter@v0.131.0/exporterhelper/internal/obs_report_sender.go:92\ngo.opentelemetry.io/collector/exporter/exporterhelper/internal.(*BaseExporter).Send\n\tgo.opentelemetry.io/collector/exporter@v0.131.0/exporterhelper/internal/base_exporter.go:115\ngo.opentelemetry.io/collector/exporter/exporterhelper.NewLogsRequest.newConsumeLogs.func1\n\tgo.opentelemetry.io/collector/exporter@v0.131.0/exporterhelper/logs.go:191\ngo.opentelemetry.io/collector/consumer.ConsumeLogsFunc.ConsumeLogs\n\tgo.opentelemetry.io/collector/consumer@v1.37.0/logs.go:27\ngo.opentelemetry.io/collector/internal/fanoutconsumer.(*logsConsumer).ConsumeLogs\n\tgo.opentelemetry.io/collector/internal/fanoutconsumer@v0.131.0/logs.go:64\ngo.opentelemetry.io/collector/processor/batchprocessor.(*batchLogs).export\n\tgo.opentelemetry.io/collector/processor/batchprocessor@v0.131.0/batch_processor.go:543\ngo.opentelemetry.io/collector/processor/batchprocessor.(*shard[...]).sendItems\n\tgo.opentelemetry.io/collector/processor/batchprocessor@v0.131.0/batch_processor.go:262\ngo.opentelemetry.io/collector/processor/batchprocessor.(*shard[...]).startLoop\n\tgo.opentelemetry.io/collector/processor/batchprocessor@v0.131.0/batch_processor.go:222\n2025-08-19T18:28:19.984Z\terror\tinternal/base_exporter.go:117\tExporting failed. Rejecting data. Try enabling sending_queue to survive temporary failures.\t{\"resource\": {\"service.instance.id\": \"2e7d82f3-fb74-414f-a45c-c6b01d1a215a\", \"service.name\": \"otelcol-contrib\", \"service.version\": \"0.131.0\"}, \"otelcol.component.id\": \"opensearch\", \"otelcol.component.kind\": \"exporter\", \"otelcol.signal\": \"logs\", \"error\": \"not retryable error: Permanent error: Permanent error: flush: dial tcp 10.96.44.10:9200: connect: connection refused\\nPermanent error: Permanent error: flush: dial tcp 10.96.44.10:9200: connect: connection refused\", \"rejected_items\": 1}\ngo.opentelemetry.io/collector/exporter/exporterhelper/internal.(*BaseExporter).Send\n\tgo.opentelemetry.io/collector/exporter@v0.131.0/exporterhelper/internal/base_exporter.go:117\ngo.opentelemetry.io/collector/exporter/exporterhelper.NewLogsRequest.newConsumeLogs.func1\n\tgo.opentelemetry.io/collector/exporter@v0.131.0/exporterhelper/logs.go:191\ngo.opentelemetry.io/collector/consumer.ConsumeLogsFunc.ConsumeLogs\n\tgo.opentelemetry.io/collector/consumer@v1.37.0/logs.go:27\ngo.opentelemetry.io/collector/internal/fanoutconsumer.(*logsConsumer).ConsumeLogs\n\tgo.opentelemetry.io/collector/internal/fanoutconsumer@v0.131.0/logs.go:64\ngo.opentelemetry.io/collector/processor/batchprocessor.(*batchLogs).export\n\tgo.opentelemetry.io/collector/processor/batchprocessor@v0.131.0/batch_processor.go:543\ngo.opentelemetry.io/collector/processor/batchprocessor.(*shard[...]).sendItems\n\tgo.opentelemetry.io/collector/processor/batchprocessor@v0.131.0/batch_processor.go:262\ngo.opentelemetry.io/collector/processor/batchprocessor.(*shard[...]).startLoop\n\tgo.opentelemetry.io/collector/processor/batchprocessor@v0.131.0/batch_processor.go:222\n2025-08-19T18:28:19.984Z\tinfo\tLogs\t{\"resource\": {\"service.instance.id\": \"2e7d82f3-fb74-414f-a45c-c6b01d1a215a\", \"service.name\": \"otelcol-contrib\", \"service.version\": \"0.131.0\"}, \"otelcol.component.id\": \"debug\", \"otelcol.component.kind\": \"exporter\", \"otelcol.signal\": \"logs\", \"resource logs\": 1, \"log records\": 1}\n2025-08-19T18:28:19.984Z\twarn\tbatchprocessor@v0.131.0/batch_processor.go:264\tSender failed\t{\"resource\": {\"service.instance.id\": \"2e7d82f3-fb74-414f-a45c-c6b01d1a215a\", \"service.name\": \"otelcol-contrib\", \"service.version\": \"0.131.0\"}, \"otelcol.component.id\": \"batch\", \"otelcol.component.kind\": \"processor\", \"otelcol.pipeline.id\": \"logs\", \"otelcol.signal\": \"logs\", \"error\": \"not retryable error: Permanent error: Permanent error: flush: dial tcp 10.96.44.10:9200: connect: connection refused\\nPermanent error: Permanent error: flush: dial tcp 10.96.44.10:9200: connect: connection refused\"}\n2025-08-19T18:28:20.166Z\tinfo\tTraces\t{\"resource\": {\"service.instance.id\": \"2e7d82f3-fb74-414f-a45c-c6b01d1a215a\", \"service.name\": \"otelcol-contrib\", \"service.version\": \"0.131.0\"}, \"otelcol.component.id\": \"debug\", \"otelcol.component.kind\": \"exporter\", \"otelcol.signal\": \"traces\", \"resource spans\": 1, \"spans\": 20}\n2025-08-19T18:28:20.368Z\tinfo\tTraces\t{\"resource\": {\"service.instance.id\": \"2e7d82f3-fb74-414f-a45c-c6b01d1a215a\", \"service.name\": \"otelcol-contrib\", \"service.version\": \"0.131.0\"}, \"otelcol.component.id\": \"debug\", \"otelcol.component.kind\": \"exporter\", \"otelcol.signal\": \"traces\", \"resource spans\": 1, \"spans\": 20}\n2025-08-19T18:28:20.772Z\tinfo\tTraces\t{\"resource\": {\"service.instance.id\": \"2e7d82f3-fb74-414f-a45c-c6b01d1a215a\", \"service.name\": \"otelcol-contrib\", \"service.version\": \"0.131.0\"}, \"otelcol.component.id\": \"debug\", \"otelcol.component.kind\": \"exporter\", \"otelcol.signal\": \"traces\", \"resource spans\": 4, \"spans\": 20}\n2025-08-19T18:28:20.788Z\terror\topensearchexporter@v0.131.0/logger.go:36\tRequest failed.\t{\"resource\": {\"service.instance.id\": \"2e7d82f3-fb74-414f-a45c-c6b01d1a215a\", \"service.name\": \"otelcol-contrib\", \"service.version\": \"0.131.0\"}, \"otelcol.component.id\": \"opensearch\", \"otelcol.component.kind\": \"exporter\", \"otelcol.signal\": \"logs\", \"path\": \"/_bulk\", \"method\": \"POST\", \"duration\": 0.000735913, \"reason\": \"dial tcp 10.96.44.10:9200: connect: connection refused\"}\ngithub.com/open-telemetry/opentelemetry-collector-contrib/exporter/opensearchexporter.(*clientLogger).LogRoundTrip\n\tgithub.com/open-telemetry/opentelemetry-collector-contrib/exporter/opensearchexporter@v0.131.0/logger.go:36\ngithub.com/opensearch-project/opensearch-go/v2/opensearchtransport.(*Client).logRoundTrip\n\tgithub.com/opensearch-project/opensearch-go/v2@v2.3.0/opensearchtransport/opensearchtransport.go:498\ngithub.com/opensearch-project/opensearch-go/v2/opensearchtransport.(*Client).Perform\n\tgithub.com/opensearch-project/opensearch-go/v2@v2.3.0/opensearchtransport/opensearchtransport.go:337\ngithub.com/opensearch-project/opensearch-go/v2.(*Client).Perform\n\tgithub.com/opensearch-project/opensearch-go/v2@v2.3.0/opensearch.go:267\ngithub.com/opensearch-project/opensearch-go/v2/opensearchapi.BulkRequest.Do\n\tgithub.com/opensearch-project/opensearch-go/v2@v2.3.0/opensearchapi/api.bulk.go:192\ngithub.com/opensearch-project/opensearch-go/v2/opensearchutil.(*worker).flush\n\tgithub.com/opensearch-project/opensearch-go/v2@v2.3.0/opensearchutil/bulk_indexer.go:552\ngithub.com/opensearch-project/opensearch-go/v2/opensearchutil.(*bulkIndexer).Close\n\tgithub.com/opensearch-project/opensearch-go/v2@v2.3.0/opensearchutil/bulk_indexer.go:299\ngithub.com/open-telemetry/opentelemetry-collector-contrib/exporter/opensearchexporter.(*logBulkIndexer).close\n\tgithub.com/open-telemetry/opentelemetry-collector-contrib/exporter/opensearchexporter@v0.131.0/log_bulk_indexer.go:41\ngithub.com/open-telemetry/opentelemetry-collector-contrib/exporter/opensearchexporter.(*logExporter).pushLogData\n\tgithub.com/open-telemetry/opentelemetry-collector-contrib/exporter/opensearchexporter@v0.131.0/sso_log_exporter.go:88\ngo.opentelemetry.io/collector/consumer.ConsumeLogsFunc.ConsumeLogs\n\tgo.opentelemetry.io/collector/consumer@v1.37.0/logs.go:27\ngo.opentelemetry.io/collector/exporter/exporterhelper.NewLogs.requestConsumeFromLogs.func2\n\tgo.opentelemetry.io/collector/exporter@v0.131.0/exporterhelper/logs.go:136\ngo.opentelemetry.io/collector/exporter/exporterhelper/internal/sender.(*sender[...]).Send\n\tgo.opentelemetry.io/collector/exporter@v0.131.0/exporterhelper/internal/sender/sender.go:31\ngo.opentelemetry.io/collector/exporter/exporterhelper/internal.(*retrySender).Send\n\tgo.opentelemetry.io/collector/exporter@v0.131.0/exporterhelper/internal/retry_sender.go:91\ngo.opentelemetry.io/collector/exporter/exporterhelper/internal.(*obsReportSender[...]).Send\n\tgo.opentelemetry.io/collector/exporter@v0.131.0/exporterhelper/internal/obs_report_sender.go:92\ngo.opentelemetry.io/collector/exporter/exporterhelper/internal.(*BaseExporter).Send\n\tgo.opentelemetry.io/collector/exporter@v0.131.0/exporterhelper/internal/base_exporter.go:115\ngo.opentelemetry.io/collector/exporter/exporterhelper.NewLogsRequest.newConsumeLogs.func1\n\tgo.opentelemetry.io/collector/exporter@v0.131.0/exporterhelper/logs.go:191\ngo.opentelemetry.io/collector/consumer.ConsumeLogsFunc.ConsumeLogs\n\tgo.opentelemetry.io/collector/consumer@v1.37.0/logs.go:27\ngo.opentelemetry.io/collector/internal/fanoutconsumer.(*logsConsumer).ConsumeLogs\n\tgo.opentelemetry.io/collector/internal/fanoutconsumer@v0.131.0/logs.go:64\ngo.opentelemetry.io/collector/processor/batchprocessor.(*batchLogs).export\n\tgo.opentelemetry.io/collector/processor/batchprocessor@v0.131.0/batch_processor.go:543\ngo.opentelemetry.io/collector/processor/batchprocessor.(*shard[...]).sendItems\n\tgo.opentelemetry.io/collector/processor/batchprocessor@v0.131.0/batch_processor.go:262\ngo.opentelemetry.io/collector/processor/batchprocessor.(*shard[...]).startLoop\n\tgo.opentelemetry.io/collector/processor/batchprocessor@v0.131.0/batch_processor.go:222\n2025-08-19T18:28:20.788Z\terror\tinternal/base_exporter.go:117\tExporting failed. Rejecting data. Try enabling sending_queue to survive temporary failures.\t{\"resource\": {\"service.instance.id\": \"2e7d82f3-fb74-414f-a45c-c6b01d1a215a\", \"service.name\": \"otelcol-contrib\", \"service.version\": \"0.131.0\"}, \"otelcol.component.id\": \"opensearch\", \"otelcol.component.kind\": \"exporter\", \"otelcol.signal\": \"logs\", \"error\": \"not retryable error: Permanent error: Permanent error: flush: dial tcp 10.96.44.10:9200: connect: connection refused\\nPermanent error: Permanent error: flush: dial tcp 10.96.44.10:9200: connect: connection refused\", \"rejected_items\": 13}\ngo.opentelemetry.io/collector/exporter/exporterhelper/internal.(*BaseExporter).Send\n\tgo.opentelemetry.io/collector/exporter@v0.131.0/exporterhelper/internal/base_exporter.go:117\ngo.opentelemetry.io/collector/exporter/exporterhelper.NewLogsRequest.newConsumeLogs.func1\n\tgo.opentelemetry.io/collector/exporter@v0.131.0/exporterhelper/logs.go:191\ngo.opentelemetry.io/collector/consumer.ConsumeLogsFunc.ConsumeLogs\n\tgo.opentelemetry.io/collector/consumer@v1.37.0/logs.go:27\ngo.opentelemetry.io/collector/internal/fanoutconsumer.(*logsConsumer).ConsumeLogs\n\tgo.opentelemetry.io/collector/internal/fanoutconsumer@v0.131.0/logs.go:64\ngo.opentelemetry.io/collector/processor/batchprocessor.(*batchLogs).export\n\tgo.opentelemetry.io/collector/processor/batchprocessor@v0.131.0/batch_processor.go:543\ngo.opentelemetry.io/collector/processor/batchprocessor.(*shard[...]).sendItems\n\tgo.opentelemetry.io/collector/processor/batchprocessor@v0.131.0/batch_processor.go:262\ngo.opentelemetry.io/collector/processor/batchprocessor.(*shard[...]).startLoop\n\tgo.opentelemetry.io/collector/processor/batchprocessor@v0.131.0/batch_processor.go:222\n2025-08-19T18:28:20.788Z\tinfo\tLogs\t{\"resource\": {\"service.instance.id\": \"2e7d82f3-fb74-414f-a45c-c6b01d1a215a\", \"service.name\": \"otelcol-contrib\", \"service.version\": \"0.131.0\"}, \"otelcol.component.id\": \"debug\", \"otelcol.component.kind\": \"exporter\", \"otelcol.signal\": \"logs\", \"resource logs\": 6, \"log records\": 13}\n2025-08-19T18:28:20.788Z\twarn\tbatchprocessor@v0.131.0/batch_processor.go:264\tSender failed\t{\"resource\": {\"service.instance.id\": \"2e7d82f3-fb74-414f-a45c-c6b01d1a215a\", \"service.name\": \"otelcol-contrib\", \"service.version\": \"0.131.0\"}, \"otelcol.component.id\": \"batch\", \"otelcol.component.kind\": \"processor\", \"otelcol.pipeline.id\": \"logs\", \"otelcol.signal\": \"logs\", \"error\": \"not retryable error: Permanent error: Permanent error: flush: dial tcp 10.96.44.10:9200: connect: connection refused\\nPermanent error: Permanent error: flush: dial tcp 10.96.44.10:9200: connect: connection refused\"}\n2025-08-19T18:28:21.388Z\tinfo\tTraces\t{\"resource\": {\"service.instance.id\": \"2e7d82f3-fb74-414f-a45c-c6b01d1a215a\", \"service.name\": \"otelcol-contrib\", \"service.version\": \"0.131.0\"}, \"otelcol.component.id\": \"debug\", \"otelcol.component.kind\": \"exporter\", \"otelcol.signal\": \"traces\", \"resource spans\": 1, \"spans\": 2}\n2025-08-19T18:28:21.589Z\tinfo\tTraces\t{\"resource\": {\"service.instance.id\": \"2e7d82f3-fb74-414f-a45c-c6b01d1a215a\", \"service.name\": \"otelcol-contrib\", \"service.version\": \"0.131.0\"}, \"otelcol.component.id\": \"debug\", \"otelcol.component.kind\": \"exporter\", \"otelcol.signal\": \"traces\", \"resource spans\": 8, \"spans\": 54}\n2025-08-19T18:28:21.789Z\tinfo\tTraces\t{\"resource\": {\"service.instance.id\": \"2e7d82f3-fb74-414f-a45c-c6b01d1a215a\", \"service.name\": \"otelcol-contrib\", \"service.version\": \"0.131.0\"}, \"otelcol.component.id\": \"debug\", \"otelcol.component.kind\": \"exporter\", \"otelcol.signal\": \"traces\", \"resource spans\": 5, \"spans\": 21}\n2025-08-19T18:28:21.809Z\terror\topensearchexporter@v0.131.0/logger.go:36\tRequest failed.\t{\"resource\": {\"service.instance.id\": \"2e7d82f3-fb74-414f-a45c-c6b01d1a215a\", \"service.name\": \"otelcol-contrib\", \"service.version\": \"0.131.0\"}, \"otelcol.component.id\": \"opensearch\", \"otelcol.component.kind\": \"exporter\", \"otelcol.signal\": \"logs\", \"path\": \"/_bulk\", \"method\": \"POST\", \"duration\": 0.000783371, \"reason\": \"dial tcp 10.96.44.10:9200: connect: connection refused\"}\ngithub.com/open-telemetry/opentelemetry-collector-contrib/exporter/opensearchexporter.(*clientLogger).LogRoundTrip\n\tgithub.com/open-telemetry/opentelemetry-collector-contrib/exporter/opensearchexporter@v0.131.0/logger.go:36\ngithub.com/opensearch-project/opensearch-go/v2/opensearchtransport.(*Client).logRoundTrip\n\tgithub.com/opensearch-project/opensearch-go/v2@v2.3.0/opensearchtransport/opensearchtransport.go:498\ngithub.com/opensearch-project/opensearch-go/v2/opensearchtransport.(*Client).Perform\n\tgithub.com/opensearch-project/opensearch-go/v2@v2.3.0/opensearchtransport/opensearchtransport.go:337\ngithub.com/opensearch-project/opensearch-go/v2.(*Client).Perform\n\tgithub.com/opensearch-project/opensearch-go/v2@v2.3.0/opensearch.go:267\ngithub.com/opensearch-project/opensearch-go/v2/opensearchapi.BulkRequest.Do\n\tgithub.com/opensearch-project/opensearch-go/v2@v2.3.0/opensearchapi/api.bulk.go:192\ngithub.com/opensearch-project/opensearch-go/v2/opensearchutil.(*worker).flush\n\tgithub.com/opensearch-project/opensearch-go/v2@v2.3.0/opensearchutil/bulk_indexer.go:552\ngithub.com/opensearch-project/opensearch-go/v2/opensearchutil.(*bulkIndexer).Close\n\tgithub.com/opensearch-project/opensearch-go/v2@v2.3.0/opensearchutil/bulk_indexer.go:299\ngithub.com/open-telemetry/opentelemetry-collector-contrib/exporter/opensearchexporter.(*logBulkIndexer).close\n\tgithub.com/open-telemetry/opentelemetry-collector-contrib/exporter/opensearchexporter@v0.131.0/log_bulk_indexer.go:41\ngithub.com/open-telemetry/opentelemetry-collector-contrib/exporter/opensearchexporter.(*logExporter).pushLogData\n\tgithub.com/open-telemetry/opentelemetry-collector-contrib/exporter/opensearchexporter@v0.131.0/sso_log_exporter.go:88\ngo.opentelemetry.io/collector/consumer.ConsumeLogsFunc.ConsumeLogs\n\tgo.opentelemetry.io/collector/consumer@v1.37.0/logs.go:27\ngo.opentelemetry.io/collector/exporter/exporterhelper.NewLogs.requestConsumeFromLogs.func2\n\tgo.opentelemetry.io/collector/exporter@v0.131.0/exporterhelper/logs.go:136\ngo.opentelemetry.io/collector/exporter/exporterhelper/internal/sender.(*sender[...]).Send\n\tgo.opentelemetry.io/collector/exporter@v0.131.0/exporterhelper/internal/sender/sender.go:31\ngo.opentelemetry.io/collector/exporter/exporterhelper/internal.(*retrySender).Send\n\tgo.opentelemetry.io/collector/exporter@v0.131.0/exporterhelper/internal/retry_sender.go:91\ngo.opentelemetry.io/collector/exporter/exporterhelper/internal.(*obsReportSender[...]).Send\n\tgo.opentelemetry.io/collector/exporter@v0.131.0/exporterhelper/internal/obs_report_sender.go:92\ngo.opentelemetry.io/collector/exporter/exporterhelper/internal.(*BaseExporter).Send\n\tgo.opentelemetry.io/collector/exporter@v0.131.0/exporterhelper/internal/base_exporter.go:115\ngo.opentelemetry.io/collector/exporter/exporterhelper.NewLogsRequest.newConsumeLogs.func1\n\tgo.opentelemetry.io/collector/exporter@v0.131.0/exporterhelper/logs.go:191\ngo.opentelemetry.io/collector/consumer.ConsumeLogsFunc.ConsumeLogs\n\tgo.opentelemetry.io/collector/consumer@v1.37.0/logs.go:27\ngo.opentelemetry.io/collector/internal/fanoutconsumer.(*logsConsumer).ConsumeLogs\n\tgo.opentelemetry.io/collector/internal/fanoutconsumer@v0.131.0/logs.go:64\ngo.opentelemetry.io/collector/processor/batchprocessor.(*batchLogs).export\n\tgo.opentelemetry.io/collector/processor/batchprocessor@v0.131.0/batch_processor.go:543\ngo.opentelemetry.io/collector/processor/batchprocessor.(*shard[...]).sendItems\n\tgo.opentelemetry.io/collector/processor/batchprocessor@v0.131.0/batch_processor.go:262\ngo.opentelemetry.io/collector/processor/batchprocessor.(*shard[...]).startLoop\n\tgo.opentelemetry.io/collector/processor/batchprocessor@v0.131.0/batch_processor.go:222\n2025-08-19T18:28:21.809Z\terror\tinternal/base_exporter.go:117\tExporting failed. Rejecting data. Try enabling sending_queue to survive temporary failures.\t{\"resource\": {\"service.instance.id\": \"2e7d82f3-fb74-414f-a45c-c6b01d1a215a\", \"service.name\": \"otelcol-contrib\", \"service.version\": \"0.131.0\"}, \"otelcol.component.id\": \"opensearch\", \"otelcol.component.kind\": \"exporter\", \"otelcol.signal\": \"logs\", \"error\": \"not retryable error: Permanent error: Permanent error: flush: dial tcp 10.96.44.10:9200: connect: connection refused\\nPermanent error: Permanent error: flush: dial tcp 10.96.44.10:9200: connect: connection refused\", \"rejected_items\": 26}\ngo.opentelemetry.io/collector/exporter/exporterhelper/internal.(*BaseExporter).Send\n\tgo.opentelemetry.io/collector/exporter@v0.131.0/exporterhelper/internal/base_exporter.go:117\ngo.opentelemetry.io/collector/exporter/exporterhelper.NewLogsRequest.newConsumeLogs.func1\n\tgo.opentelemetry.io/collector/exporter@v0.131.0/exporterhelper/logs.go:191\ngo.opentelemetry.io/collector/consumer.ConsumeLogsFunc.ConsumeLogs\n\tgo.opentelemetry.io/collector/consumer@v1.37.0/logs.go:27\ngo.opentelemetry.io/collector/internal/fanoutconsumer.(*logsConsumer).ConsumeLogs\n\tgo.opentelemetry.io/collector/internal/fanoutconsumer@v0.131.0/logs.go:64\ngo.opentelemetry.io/collector/processor/batchprocessor.(*batchLogs).export\n\tgo.opentelemetry.io/collector/processor/batchprocessor@v0.131.0/batch_processor.go:543\ngo.opentelemetry.io/collector/processor/batchprocessor.(*shard[...]).sendItems\n\tgo.opentelemetry.io/collector/processor/batchprocessor@v0.131.0/batch_processor.go:262\ngo.opentelemetry.io/collector/processor/batchprocessor.(*shard[...]).startLoop\n\tgo.opentelemetry.io/collector/processor/batchprocessor@v0.131.0/batch_processor.go:222\n2025-08-19T18:28:21.809Z\tinfo\tLogs\t{\"resource\": {\"service.instance.id\": \"2e7d82f3-fb74-414f-a45c-c6b01d1a215a\", \"service.name\": \"otelcol-contrib\", \"service.version\": \"0.131.0\"}, \"otelcol.component.id\": \"debug\", \"otelcol.component.kind\": \"exporter\", \"otelcol.signal\": \"logs\", \"resource logs\": 11, \"log records\": 26}\n2025-08-19T18:28:21.809Z\twarn\tbatchprocessor@v0.131.0/batch_processor.go:264\tSender failed\t{\"resource\": {\"service.instance.id\": \"2e7d82f3-fb74-414f-a45c-c6b01d1a215a\", \"service.name\": \"otelcol-contrib\", \"service.version\": \"0.131.0\"}, \"otelcol.component.id\": \"batch\", \"otelcol.component.kind\": \"processor\", \"otelcol.pipeline.id\": \"logs\", \"otelcol.signal\": \"logs\", \"error\": \"not retryable error: Permanent error: Permanent error: flush: dial tcp 10.96.44.10:9200: connect: connection refused\\nPermanent error: Permanent error: flush: dial tcp 10.96.44.10:9200: connect: connection refused\"}\n2025-08-19T18:28:21.933Z\tinfo\tMetrics\t{\"resource\": {\"service.instance.id\": \"2e7d82f3-fb74-414f-a45c-c6b01d1a215a\", \"service.name\": \"otelcol-contrib\", \"service.version\": \"0.131.0\"}, \"otelcol.component.id\": \"debug\", \"otelcol.component.kind\": \"exporter\", \"otelcol.signal\": \"metrics\", \"resource metrics\": 1, \"metrics\": 3, \"data points\": 3}\n2025-08-19T18:28:21.936Z\tinfo\tinternal/retry_sender.go:133\tExporting failed. Will retry the request after interval.\t{\"resource\": {\"service.instance.id\": \"2e7d82f3-fb74-414f-a45c-c6b01d1a215a\", \"service.name\": \"otelcol-contrib\", \"service.version\": \"0.131.0\"}, \"otelcol.component.id\": \"otlphttp/prometheus\", \"otelcol.component.kind\": \"exporter\", \"otelcol.signal\": \"metrics\", \"error\": \"failed to make an HTTP request: Post \\\"http://prometheus:9090/api/v1/otlp/v1/metrics\\\": dial tcp 10.96.227.216:9090: connect: connection refused\", \"interval\": \"6.069506498s\"}\n2025-08-19T18:28:21.936Z\tinfo\tinternal/retry_sender.go:133\tExporting failed. Will retry the request after interval.\t{\"resource\": {\"service.instance.id\": \"2e7d82f3-fb74-414f-a45c-c6b01d1a215a\", \"service.name\": \"otelcol-contrib\", \"service.version\": \"0.131.0\"}, \"otelcol.component.id\": \"otlphttp/prometheus\", \"otelcol.component.kind\": \"exporter\", \"otelcol.signal\": \"metrics\", \"error\": \"failed to make an HTTP request: Post \\\"http://prometheus:9090/api/v1/otlp/v1/metrics\\\": dial tcp 10.96.227.216:9090: connect: connection refused\", \"interval\": \"5.309842415s\"}\n2025-08-19T18:28:21.991Z\tinfo\tTraces\t{\"resource\": {\"service.instance.id\": \"2e7d82f3-fb74-414f-a45c-c6b01d1a215a\", \"service.name\": \"otelcol-contrib\", \"service.version\": \"0.131.0\"}, \"otelcol.component.id\": \"debug\", \"otelcol.component.kind\": \"exporter\", \"otelcol.signal\": \"traces\", \"resource spans\": 1, \"spans\": 4}\n2025-08-19T18:28:22.017Z\terror\topensearchexporter@v0.131.0/logger.go:36\tRequest failed.\t{\"resource\": {\"service.instance.id\": \"2e7d82f3-fb74-414f-a45c-c6b01d1a215a\", \"service.name\": \"otelcol-contrib\", \"service.version\": \"0.131.0\"}, \"otelcol.component.id\": \"opensearch\", \"otelcol.component.kind\": \"exporter\", \"otelcol.signal\": \"logs\", \"path\": \"/_bulk\", \"method\": \"POST\", \"duration\": 0.005005684, \"reason\": \"dial tcp 10.96.44.10:9200: connect: connection refused\"}\ngithub.com/open-telemetry/opentelemetry-collector-contrib/exporter/opensearchexporter.(*clientLogger).LogRoundTrip\n\tgithub.com/open-telemetry/opentelemetry-collector-contrib/exporter/opensearchexporter@v0.131.0/logger.go:36\ngithub.com/opensearch-project/opensearch-go/v2/opensearchtransport.(*Client).logRoundTrip\n\tgithub.com/opensearch-project/opensearch-go/v2@v2.3.0/opensearchtransport/opensearchtransport.go:498\ngithub.com/opensearch-project/opensearch-go/v2/opensearchtransport.(*Client).Perform\n\tgithub.com/opensearch-project/opensearch-go/v2@v2.3.0/opensearchtransport/opensearchtransport.go:337\ngithub.com/opensearch-project/opensearch-go/v2.(*Client).Perform\n\tgithub.com/opensearch-project/opensearch-go/v2@v2.3.0/opensearch.go:267\ngithub.com/opensearch-project/opensearch-go/v2/opensearchapi.BulkRequest.Do\n\tgithub.com/opensearch-project/opensearch-go/v2@v2.3.0/opensearchapi/api.bulk.go:192\ngithub.com/opensearch-project/opensearch-go/v2/opensearchutil.(*worker).flush\n\tgithub.com/opensearch-project/opensearch-go/v2@v2.3.0/opensearchutil/bulk_indexer.go:552\ngithub.com/opensearch-project/opensearch-go/v2/opensearchutil.(*bulkIndexer).Close\n\tgithub.com/opensearch-project/opensearch-go/v2@v2.3.0/opensearchutil/bulk_indexer.go:299\ngithub.com/open-telemetry/opentelemetry-collector-contrib/exporter/opensearchexporter.(*logBulkIndexer).close\n\tgithub.com/open-telemetry/opentelemetry-collector-contrib/exporter/opensearchexporter@v0.131.0/log_bulk_indexer.go:41\ngithub.com/open-telemetry/opentelemetry-collector-contrib/exporter/opensearchexporter.(*logExporter).pushLogData\n\tgithub.com/open-telemetry/opentelemetry-collector-contrib/exporter/opensearchexporter@v0.131.0/sso_log_exporter.go:88\ngo.opentelemetry.io/collector/consumer.ConsumeLogsFunc.ConsumeLogs\n\tgo.opentelemetry.io/collector/consumer@v1.37.0/logs.go:27\ngo.opentelemetry.io/collector/exporter/exporterhelper.NewLogs.requestConsumeFromLogs.func2\n\tgo.opentelemetry.io/collector/exporter@v0.131.0/exporterhelper/logs.go:136\ngo.opentelemetry.io/collector/exporter/exporterhelper/internal/sender.(*sender[...]).Send\n\tgo.opentelemetry.io/collector/exporter@v0.131.0/exporterhelper/internal/sender/sender.go:31\ngo.opentelemetry.io/collector/exporter/exporterhelper/internal.(*retrySender).Send\n\tgo.opentelemetry.io/collector/exporter@v0.131.0/exporterhelper/internal/retry_sender.go:91\ngo.opentelemetry.io/collector/exporter/exporterhelper/internal.(*obsReportSender[...]).Send\n\tgo.opentelemetry.io/collector/exporter@v0.131.0/exporterhelper/internal/obs_report_sender.go:92\ngo.opentelemetry.io/collector/exporter/exporterhelper/internal.(*BaseExporter).Send\n\tgo.opentelemetry.io/collector/exporter@v0.131.0/exporterhelper/internal/base_exporter.go:115\ngo.opentelemetry.io/collector/exporter/exporterhelper.NewLogsRequest.newConsumeLogs.func1\n\tgo.opentelemetry.io/collector/exporter@v0.131.0/exporterhelper/logs.go:191\ngo.opentelemetry.io/collector/consumer.ConsumeLogsFunc.ConsumeLogs\n\tgo.opentelemetry.io/collector/consumer@v1.37.0/logs.go:27\ngo.opentelemetry.io/collector/internal/fanoutconsumer.(*logsConsumer).ConsumeLogs\n\tgo.opentelemetry.io/collector/internal/fanoutconsumer@v0.131.0/logs.go:64\ngo.opentelemetry.io/collector/processor/batchprocessor.(*batchLogs).export\n\tgo.opentelemetry.io/collector/processor/batchprocessor@v0.131.0/batch_processor.go:543\ngo.opentelemetry.io/collector/processor/batchprocessor.(*shard[...]).sendItems\n\tgo.opentelemetry.io/collector/processor/batchprocessor@v0.131.0/batch_processor.go:262\ngo.opentelemetry.io/collector/processor/batchprocessor.(*shard[...]).startLoop\n\tgo.opentelemetry.io/collector/processor/batchprocessor@v0.131.0/batch_processor.go:222\n2025-08-19T18:28:22.019Z\terror\tinternal/base_exporter.go:117\tExporting failed. Rejecting data. Try enabling sending_queue to survive temporary failures.\t{\"resource\": {\"service.instance.id\": \"2e7d82f3-fb74-414f-a45c-c6b01d1a215a\", \"service.name\": \"otelcol-contrib\", \"service.version\": \"0.131.0\"}, \"otelcol.component.id\": \"opensearch\", \"otelcol.component.kind\": \"exporter\", \"otelcol.signal\": \"logs\", \"error\": \"not retryable error: Permanent error: Permanent error: flush: dial tcp 10.96.44.10:9200: connect: connection refused\\nPermanent error: Permanent error: flush: dial tcp 10.96.44.10:9200: connect: connection refused\", \"rejected_items\": 5}\ngo.opentelemetry.io/collector/exporter/exporterhelper/internal.(*BaseExporter).Send\n\tgo.opentelemetry.io/collector/exporter@v0.131.0/exporterhelper/internal/base_exporter.go:117\ngo.opentelemetry.io/collector/exporter/exporterhelper.NewLogsRequest.newConsumeLogs.func1\n\tgo.opentelemetry.io/collector/exporter@v0.131.0/exporterhelper/logs.go:191\ngo.opentelemetry.io/collector/consumer.ConsumeLogsFunc.ConsumeLogs\n\tgo.opentelemetry.io/collector/consumer@v1.37.0/logs.go:27\ngo.opentelemetry.io/collector/internal/fanoutconsumer.(*logsConsumer).ConsumeLogs\n\tgo.opentelemetry.io/collector/internal/fanoutconsumer@v0.131.0/logs.go:64\ngo.opentelemetry.io/collector/processor/batchprocessor.(*batchLogs).export\n\tgo.opentelemetry.io/collector/processor/batchprocessor@v0.131.0/batch_processor.go:543\ngo.opentelemetry.io/collector/processor/batchprocessor.(*shard[...]).sendItems\n\tgo.opentelemetry.io/collector/processor/batchprocessor@v0.131.0/batch_processor.go:262\ngo.opentelemetry.io/collector/processor/batchprocessor.(*shard[...]).startLoop\n\tgo.opentelemetry.io/collector/processor/batchprocessor@v0.131.0/batch_processor.go:222\n2025-08-19T18:28:22.021Z\tinfo\tLogs\t{\"resource\": {\"service.instance.id\": \"2e7d82f3-fb74-414f-a45c-c6b01d1a215a\", \"service.name\": \"otelcol-contrib\", \"service.version\": \"0.131.0\"}, \"otelcol.component.id\": \"debug\", \"otelcol.component.kind\": \"exporter\", \"otelcol.signal\": \"logs\", \"resource logs\": 1, \"log records\": 5}\n2025-08-19T18:28:22.021Z\twarn\tbatchprocessor@v0.131.0/batch_processor.go:264\tSender failed\t{\"resource\": {\"service.instance.id\": \"2e7d82f3-fb74-414f-a45c-c6b01d1a215a\", \"service.name\": \"otelcol-contrib\", \"service.version\": \"0.131.0\"}, \"otelcol.component.id\": \"batch\", \"otelcol.component.kind\": \"processor\", \"otelcol.pipeline.id\": \"logs\", \"otelcol.signal\": \"logs\", \"error\": \"not retryable error: Permanent error: Permanent error: flush: dial tcp 10.96.44.10:9200: connect: connection refused\\nPermanent error: Permanent error: flush: dial tcp 10.96.44.10:9200: connect: connection refused\"}\n2025-08-19T18:28:22.231Z\terror\topensearchexporter@v0.131.0/logger.go:36\tRequest failed.\t{\"resource\": {\"service.instance.id\": \"2e7d82f3-fb74-414f-a45c-c6b01d1a215a\", \"service.name\": \"otelcol-contrib\", \"service.version\": \"0.131.0\"}, \"otelcol.component.id\": \"opensearch\", \"otelcol.component.kind\": \"exporter\", \"otelcol.signal\": \"logs\", \"path\": \"/_bulk\", \"method\": \"POST\", \"duration\": 0.000951871, \"reason\": \"dial tcp 10.96.44.10:9200: connect: connection refused\"}\ngithub.com/open-telemetry/opentelemetry-collector-contrib/exporter/opensearchexporter.(*clientLogger).LogRoundTrip\n\tgithub.com/open-telemetry/opentelemetry-collector-contrib/exporter/opensearchexporter@v0.131.0/logger.go:36\ngithub.com/opensearch-project/opensearch-go/v2/opensearchtransport.(*Client).logRoundTrip\n\tgithub.com/opensearch-project/opensearch-go/v2@v2.3.0/opensearchtransport/opensearchtransport.go:498\ngithub.com/opensearch-project/opensearch-go/v2/opensearchtransport.(*Client).Perform\n\tgithub.com/opensearch-project/opensearch-go/v2@v2.3.0/opensearchtransport/opensearchtransport.go:337\ngithub.com/opensearch-project/opensearch-go/v2.(*Client).Perform\n\tgithub.com/opensearch-project/opensearch-go/v2@v2.3.0/opensearch.go:267\ngithub.com/opensearch-project/opensearch-go/v2/opensearchapi.BulkRequest.Do\n\tgithub.com/opensearch-project/opensearch-go/v2@v2.3.0/opensearchapi/api.bulk.go:192\ngithub.com/opensearch-project/opensearch-go/v2/opensearchutil.(*worker).flush\n\tgithub.com/opensearch-project/opensearch-go/v2@v2.3.0/opensearchutil/bulk_indexer.go:552\ngithub.com/opensearch-project/opensearch-go/v2/opensearchutil.(*bulkIndexer).Close\n\tgithub.com/opensearch-project/opensearch-go/v2@v2.3.0/opensearchutil/bulk_indexer.go:299\ngithub.com/open-telemetry/opentelemetry-collector-contrib/exporter/opensearchexporter.(*logBulkIndexer).close\n\tgithub.com/open-telemetry/opentelemetry-collector-contrib/exporter/opensearchexporter@v0.131.0/log_bulk_indexer.go:41\ngithub.com/open-telemetry/opentelemetry-collector-contrib/exporter/opensearchexporter.(*logExporter).pushLogData\n\tgithub.com/open-telemetry/opentelemetry-collector-contrib/exporter/opensearchexporter@v0.131.0/sso_log_exporter.go:88\ngo.opentelemetry.io/collector/consumer.ConsumeLogsFunc.ConsumeLogs\n\tgo.opentelemetry.io/collector/consumer@v1.37.0/logs.go:27\ngo.opentelemetry.io/collector/exporter/exporterhelper.NewLogs.requestConsumeFromLogs.func2\n\tgo.opentelemetry.io/collector/exporter@v0.131.0/exporterhelper/logs.go:136\ngo.opentelemetry.io/collector/exporter/exporterhelper/internal/sender.(*sender[...]).Send\n\tgo.opentelemetry.io/collector/exporter@v0.131.0/exporterhelper/internal/sender/sender.go:31\ngo.opentelemetry.io/collector/exporter/exporterhelper/internal.(*retrySender).Send\n\tgo.opentelemetry.io/collector/exporter@v0.131.0/exporterhelper/internal/retry_sender.go:91\ngo.opentelemetry.io/collector/exporter/exporterhelper/internal.(*obsReportSender[...]).Send\n\tgo.opentelemetry.io/collector/exporter@v0.131.0/exporterhelper/internal/obs_report_sender.go:92\ngo.opentelemetry.io/collector/exporter/exporterhelper/internal.(*BaseExporter).Send\n\tgo.opentelemetry.io/collector/exporter@v0.131.0/exporterhelper/internal/base_exporter.go:115\ngo.opentelemetry.io/collector/exporter/exporterhelper.NewLogsRequest.newConsumeLogs.func1\n\tgo.opentelemetry.io/collector/exporter@v0.131.0/exporterhelper/logs.go:191\ngo.opentelemetry.io/collector/consumer.ConsumeLogsFunc.ConsumeLogs\n\tgo.opentelemetry.io/collector/consumer@v1.37.0/logs.go:27\ngo.opentelemetry.io/collector/internal/fanoutconsumer.(*logsConsumer).ConsumeLogs\n\tgo.opentelemetry.io/collector/internal/fanoutconsumer@v0.131.0/logs.go:64\ngo.opentelemetry.io/collector/processor/batchprocessor.(*batchLogs).export\n\tgo.opentelemetry.io/collector/processor/batchprocessor@v0.131.0/batch_processor.go:543\ngo.opentelemetry.io/collector/processor/batchprocessor.(*shard[...]).sendItems\n\tgo.opentelemetry.io/collector/processor/batchprocessor@v0.131.0/batch_processor.go:262\ngo.opentelemetry.io/collector/processor/batchprocessor.(*shard[...]).startLoop\n\tgo.opentelemetry.io/collector/processor/batchprocessor@v0.131.0/batch_processor.go:222\n2025-08-19T18:28:22.231Z\terror\tinternal/base_exporter.go:117\tExporting failed. Rejecting data. Try enabling sending_queue to survive temporary failures.\t{\"resource\": {\"service.instance.id\": \"2e7d82f3-fb74-414f-a45c-c6b01d1a215a\", \"service.name\": \"otelcol-contrib\", \"service.version\": \"0.131.0\"}, \"otelcol.component.id\": \"opensearch\", \"otelcol.component.kind\": \"exporter\", \"otelcol.signal\": \"logs\", \"error\": \"not retryable error: Permanent error: Permanent error: flush: dial tcp 10.96.44.10:9200: connect: connection refused\\nPermanent error: Permanent error: flush: dial tcp 10.96.44.10:9200: connect: connection refused\", \"rejected_items\": 12}\ngo.opentelemetry.io/collector/exporter/exporterhelper/internal.(*BaseExporter).Send\n\tgo.opentelemetry.io/collector/exporter@v0.131.0/exporterhelper/internal/base_exporter.go:117\ngo.opentelemetry.io/collector/exporter/exporterhelper.NewLogsRequest.newConsumeLogs.func1\n\tgo.opentelemetry.io/collector/exporter@v0.131.0/exporterhelper/logs.go:191\ngo.opentelemetry.io/collector/consumer.ConsumeLogsFunc.ConsumeLogs\n\tgo.opentelemetry.io/collector/consumer@v1.37.0/logs.go:27\ngo.opentelemetry.io/collector/internal/fanoutconsumer.(*logsConsumer).ConsumeLogs\n\tgo.opentelemetry.io/collector/internal/fanoutconsumer@v0.131.0/logs.go:64\ngo.opentelemetry.io/collector/processor/batchprocessor.(*batchLogs).export\n\tgo.opentelemetry.io/collector/processor/batchprocessor@v0.131.0/batch_processor.go:543\ngo.opentelemetry.io/collector/processor/batchprocessor.(*shard[...]).sendItems\n\tgo.opentelemetry.io/collector/processor/batchprocessor@v0.131.0/batch_processor.go:262\ngo.opentelemetry.io/collector/processor/batchprocessor.(*shard[...]).startLoop\n\tgo.opentelemetry.io/collector/processor/batchprocessor@v0.131.0/batch_processor.go:222\n2025-08-19T18:28:22.231Z\tinfo\tLogs\t{\"resource\": {\"service.instance.id\": \"2e7d82f3-fb74-414f-a45c-c6b01d1a215a\", \"service.name\": \"otelcol-contrib\", \"service.version\": \"0.131.0\"}, \"otelcol.component.id\": \"debug\", \"otelcol.component.kind\": \"exporter\", \"otelcol.signal\": \"logs\", \"resource logs\": 2, \"log records\": 12}\n2025-08-19T18:28:22.231Z\twarn\tbatchprocessor@v0.131.0/batch_processor.go:264\tSender failed\t{\"resource\": {\"service.instance.id\": \"2e7d82f3-fb74-414f-a45c-c6b01d1a215a\", \"service.name\": \"otelcol-contrib\", \"service.version\": \"0.131.0\"}, \"otelcol.component.id\": \"batch\", \"otelcol.component.kind\": \"processor\", \"otelcol.pipeline.id\": \"logs\", \"otelcol.signal\": \"logs\", \"error\": \"not retryable error: Permanent error: Permanent error: flush: dial tcp 10.96.44.10:9200: connect: connection refused\\nPermanent error: Permanent error: flush: dial tcp 10.96.44.10:9200: connect: connection refused\"}\n2025-08-19T18:28:22.835Z\terror\topensearchexporter@v0.131.0/logger.go:36\tRequest failed.\t{\"resource\": {\"service.instance.id\": \"2e7d82f3-fb74-414f-a45c-c6b01d1a215a\", \"service.name\": \"otelcol-contrib\", \"service.version\": \"0.131.0\"}, \"otelcol.component.id\": \"opensearch\", \"otelcol.component.kind\": \"exporter\", \"otelcol.signal\": \"logs\", \"path\": \"/_bulk\", \"method\": \"POST\", \"duration\": 0.000652955, \"reason\": \"dial tcp 10.96.44.10:9200: connect: connection refused\"}\ngithub.com/open-telemetry/opentelemetry-collector-contrib/exporter/opensearchexporter.(*clientLogger).LogRoundTrip\n\tgithub.com/open-telemetry/opentelemetry-collector-contrib/exporter/opensearchexporter@v0.131.0/logger.go:36\ngithub.com/opensearch-project/opensearch-go/v2/opensearchtransport.(*Client).logRoundTrip\n\tgithub.com/opensearch-project/opensearch-go/v2@v2.3.0/opensearchtransport/opensearchtransport.go:498\ngithub.com/opensearch-project/opensearch-go/v2/opensearchtransport.(*Client).Perform\n\tgithub.com/opensearch-project/opensearch-go/v2@v2.3.0/opensearchtransport/opensearchtransport.go:337\ngithub.com/opensearch-project/opensearch-go/v2.(*Client).Perform\n\tgithub.com/opensearch-project/opensearch-go/v2@v2.3.0/opensearch.go:267\ngithub.com/opensearch-project/opensearch-go/v2/opensearchapi.BulkRequest.Do\n\tgithub.com/opensearch-project/opensearch-go/v2@v2.3.0/opensearchapi/api.bulk.go:192\ngithub.com/opensearch-project/opensearch-go/v2/opensearchutil.(*worker).flush\n\tgithub.com/opensearch-project/opensearch-go/v2@v2.3.0/opensearchutil/bulk_indexer.go:552\ngithub.com/opensearch-project/opensearch-go/v2/opensearchutil.(*bulkIndexer).Close\n\tgithub.com/opensearch-project/opensearch-go/v2@v2.3.0/opensearchutil/bulk_indexer.go:299\ngithub.com/open-telemetry/opentelemetry-collector-contrib/exporter/opensearchexporter.(*logBulkIndexer).close\n\tgithub.com/open-telemetry/opentelemetry-collector-contrib/exporter/opensearchexporter@v0.131.0/log_bulk_indexer.go:41\ngithub.com/open-telemetry/opentelemetry-collector-contrib/exporter/opensearchexporter.(*logExporter).pushLogData\n\tgithub.com/open-telemetry/opentelemetry-collector-contrib/exporter/opensearchexporter@v0.131.0/sso_log_exporter.go:88\ngo.opentelemetry.io/collector/consumer.ConsumeLogsFunc.ConsumeLogs\n\tgo.opentelemetry.io/collector/consumer@v1.37.0/logs.go:27\ngo.opentelemetry.io/collector/exporter/exporterhelper.NewLogs.requestConsumeFromLogs.func2\n\tgo.opentelemetry.io/collector/exporter@v0.131.0/exporterhelper/logs.go:136\ngo.opentelemetry.io/collector/exporter/exporterhelper/internal/sender.(*sender[...]).Send\n\tgo.opentelemetry.io/collector/exporter@v0.131.0/exporterhelper/internal/sender/sender.go:31\ngo.opentelemetry.io/collector/exporter/exporterhelper/internal.(*retrySender).Send\n\tgo.opentelemetry.io/collector/exporter@v0.131.0/exporterhelper/internal/retry_sender.go:91\ngo.opentelemetry.io/collector/exporter/exporterhelper/internal.(*obsReportSender[...]).Send\n\tgo.opentelemetry.io/collector/exporter@v0.131.0/exporterhelper/internal/obs_report_sender.go:92\ngo.opentelemetry.io/collector/exporter/exporterhelper/internal.(*BaseExporter).Send\n\tgo.opentelemetry.io/collector/exporter@v0.131.0/exporterhelper/internal/base_exporter.go:115\ngo.opentelemetry.io/collector/exporter/exporterhelper.NewLogsRequest.newConsumeLogs.func1\n\tgo.opentelemetry.io/collector/exporter@v0.131.0/exporterhelper/logs.go:191\ngo.opentelemetry.io/collector/consumer.ConsumeLogsFunc.ConsumeLogs\n\tgo.opentelemetry.io/collector/consumer@v1.37.0/logs.go:27\ngo.opentelemetry.io/collector/internal/fanoutconsumer.(*logsConsumer).ConsumeLogs\n\tgo.opentelemetry.io/collector/internal/fanoutconsumer@v0.131.0/logs.go:64\ngo.opentelemetry.io/collector/processor/batchprocessor.(*batchLogs).export\n\tgo.opentelemetry.io/collector/processor/batchprocessor@v0.131.0/batch_processor.go:543\ngo.opentelemetry.io/collector/processor/batchprocessor.(*shard[...]).sendItems\n\tgo.opentelemetry.io/collector/processor/batchprocessor@v0.131.0/batch_processor.go:262\ngo.opentelemetry.io/collector/processor/batchprocessor.(*shard[...]).startLoop\n\tgo.opentelemetry.io/collector/processor/batchprocessor@v0.131.0/batch_processor.go:222\n2025-08-19T18:28:22.837Z\terror\tinternal/base_exporter.go:117\tExporting failed. Rejecting data. Try enabling sending_queue to survive temporary failures.\t{\"resource\": {\"service.instance.id\": \"2e7d82f3-fb74-414f-a45c-c6b01d1a215a\", \"service.name\": \"otelcol-contrib\", \"service.version\": \"0.131.0\"}, \"otelcol.component.id\": \"opensearch\", \"otelcol.component.kind\": \"exporter\", \"otelcol.signal\": \"logs\", \"error\": \"not retryable error: Permanent error: Permanent error: flush: dial tcp 10.96.44.10:9200: connect: connection refused\\nPermanent error: Permanent error: flush: dial tcp 10.96.44.10:9200: connect: connection refused\", \"rejected_items\": 9}\ngo.opentelemetry.io/collector/exporter/exporterhelper/internal.(*BaseExporter).Send\n\tgo.opentelemetry.io/collector/exporter@v0.131.0/exporterhelper/internal/base_exporter.go:117\ngo.opentelemetry.io/collector/exporter/exporterhelper.NewLogsRequest.newConsumeLogs.func1\n\tgo.opentelemetry.io/collector/exporter@v0.131.0/exporterhelper/logs.go:191\ngo.opentelemetry.io/collector/consumer.ConsumeLogsFunc.ConsumeLogs\n\tgo.opentelemetry.io/collector/consumer@v1.37.0/logs.go:27\ngo.opentelemetry.io/collector/internal/fanoutconsumer.(*logsConsumer).ConsumeLogs\n\tgo.opentelemetry.io/collector/internal/fanoutconsumer@v0.131.0/logs.go:64\ngo.opentelemetry.io/collector/processor/batchprocessor.(*batchLogs).export\n\tgo.opentelemetry.io/collector/processor/batchprocessor@v0.131.0/batch_processor.go:543\ngo.opentelemetry.io/collector/processor/batchprocessor.(*shard[...]).sendItems\n\tgo.opentelemetry.io/collector/processor/batchprocessor@v0.131.0/batch_processor.go:262\ngo.opentelemetry.io/collector/processor/batchprocessor.(*shard[...]).startLoop\n\tgo.opentelemetry.io/collector/processor/batchprocessor@v0.131.0/batch_processor.go:222\n2025-08-19T18:28:22.837Z\tinfo\tLogs\t{\"resource\": {\"service.instance.id\": \"2e7d82f3-fb74-414f-a45c-c6b01d1a215a\", \"service.name\": \"otelcol-contrib\", \"service.version\": \"0.131.0\"}, \"otelcol.component.id\": \"debug\", \"otelcol.component.kind\": \"exporter\", \"otelcol.signal\": \"logs\", \"resource logs\": 5, \"log records\": 9}\n2025-08-19T18:28:22.837Z\twarn\tbatchprocessor@v0.131.0/batch_processor.go:264\tSender failed\t{\"resource\": {\"service.instance.id\": \"2e7d82f3-fb74-414f-a45c-c6b01d1a215a\", \"service.name\": \"otelcol-contrib\", \"service.version\": \"0.131.0\"}, \"otelcol.component.id\": \"batch\", \"otelcol.component.kind\": \"processor\", \"otelcol.pipeline.id\": \"logs\", \"otelcol.signal\": \"logs\", \"error\": \"not retryable error: Permanent error: Permanent error: flush: dial tcp 10.96.44.10:9200: connect: connection refused\\nPermanent error: Permanent error: flush: dial tcp 10.96.44.10:9200: connect: connection refused\"}\n2025-08-19T18:28:24.049Z\terror\topensearchexporter@v0.131.0/logger.go:36\tRequest failed.\t{\"resource\": {\"service.instance.id\": \"2e7d82f3-fb74-414f-a45c-c6b01d1a215a\", \"service.name\": \"otelcol-contrib\", \"service.version\": \"0.131.0\"}, \"otelcol.component.id\": \"opensearch\", \"otelcol.component.kind\": \"exporter\", \"otelcol.signal\": \"logs\", \"path\": \"/_bulk\", \"method\": \"POST\", \"duration\": 1.009031276, \"reason\": \"dial tcp 10.96.44.10:9200: connect: connection refused\"}\ngithub.com/open-telemetry/opentelemetry-collector-contrib/exporter/opensearchexporter.(*clientLogger).LogRoundTrip\n\tgithub.com/open-telemetry/opentelemetry-collector-contrib/exporter/opensearchexporter@v0.131.0/logger.go:36\ngithub.com/opensearch-project/opensearch-go/v2/opensearchtransport.(*Client).logRoundTrip\n\tgithub.com/opensearch-project/opensearch-go/v2@v2.3.0/opensearchtransport/opensearchtransport.go:498\ngithub.com/opensearch-project/opensearch-go/v2/opensearchtransport.(*Client).Perform\n\tgithub.com/opensearch-project/opensearch-go/v2@v2.3.0/opensearchtransport/opensearchtransport.go:337\ngithub.com/opensearch-project/opensearch-go/v2.(*Client).Perform\n\tgithub.com/opensearch-project/opensearch-go/v2@v2.3.0/opensearch.go:267\ngithub.com/opensearch-project/opensearch-go/v2/opensearchapi.BulkRequest.Do\n\tgithub.com/opensearch-project/opensearch-go/v2@v2.3.0/opensearchapi/api.bulk.go:192\ngithub.com/opensearch-project/opensearch-go/v2/opensearchutil.(*worker).flush\n\tgithub.com/opensearch-project/opensearch-go/v2@v2.3.0/opensearchutil/bulk_indexer.go:552\ngithub.com/opensearch-project/opensearch-go/v2/opensearchutil.(*bulkIndexer).Close\n\tgithub.com/opensearch-project/opensearch-go/v2@v2.3.0/opensearchutil/bulk_indexer.go:299\ngithub.com/open-telemetry/opentelemetry-collector-contrib/exporter/opensearchexporter.(*logBulkIndexer).close\n\tgithub.com/open-telemetry/opentelemetry-collector-contrib/exporter/opensearchexporter@v0.131.0/log_bulk_indexer.go:41\ngithub.com/open-telemetry/opentelemetry-collector-contrib/exporter/opensearchexporter.(*logExporter).pushLogData\n\tgithub.com/open-telemetry/opentelemetry-collector-contrib/exporter/opensearchexporter@v0.131.0/sso_log_exporter.go:88\ngo.opentelemetry.io/collector/consumer.ConsumeLogsFunc.ConsumeLogs\n\tgo.opentelemetry.io/collector/consumer@v1.37.0/logs.go:27\ngo.opentelemetry.io/collector/exporter/exporterhelper.NewLogs.requestConsumeFromLogs.func2\n\tgo.opentelemetry.io/collector/exporter@v0.131.0/exporterhelper/logs.go:136\ngo.opentelemetry.io/collector/exporter/exporterhelper/internal/sender.(*sender[...]).Send\n\tgo.opentelemetry.io/collector/exporter@v0.131.0/exporterhelper/internal/sender/sender.go:31\ngo.opentelemetry.io/collector/exporter/exporterhelper/internal.(*retrySender).Send\n\tgo.opentelemetry.io/collector/exporter@v0.131.0/exporterhelper/internal/retry_sender.go:91\ngo.opentelemetry.io/collector/exporter/exporterhelper/internal.(*obsReportSender[...]).Send\n\tgo.opentelemetry.io/collector/exporter@v0.131.0/exporterhelper/internal/obs_report_sender.go:92\ngo.opentelemetry.io/collector/exporter/exporterhelper/internal.(*BaseExporter).Send\n\tgo.opentelemetry.io/collector/exporter@v0.131.0/exporterhelper/internal/base_exporter.go:115\ngo.opentelemetry.io/collector/exporter/exporterhelper.NewLogsRequest.newConsumeLogs.func1\n\tgo.opentelemetry.io/collector/exporter@v0.131.0/exporterhelper/logs.go:191\ngo.opentelemetry.io/collector/consumer.ConsumeLogsFunc.ConsumeLogs\n\tgo.opentelemetry.io/collector/consumer@v1.37.0/logs.go:27\ngo.opentelemetry.io/collector/internal/fanoutconsumer.(*logsConsumer).ConsumeLogs\n\tgo.opentelemetry.io/collector/internal/fanoutconsumer@v0.131.0/logs.go:64\ngo.opentelemetry.io/collector/processor/batchprocessor.(*batchLogs).export\n\tgo.opentelemetry.io/collector/processor/batchprocessor@v0.131.0/batch_processor.go:543\ngo.opentelemetry.io/collector/processor/batchprocessor.(*shard[...]).sendItems\n\tgo.opentelemetry.io/collector/processor/batchprocessor@v0.131.0/batch_processor.go:262\ngo.opentelemetry.io/collector/processor/batchprocessor.(*shard[...]).startLoop\n\tgo.opentelemetry.io/collector/processor/batchprocessor@v0.131.0/batch_processor.go:222\n2025-08-19T18:28:24.053Z\terror\tinternal/base_exporter.go:117\tExporting failed. Rejecting data. Try enabling sending_queue to survive temporary failures.\t{\"resource\": {\"service.instance.id\": \"2e7d82f3-fb74-414f-a45c-c6b01d1a215a\", \"service.name\": \"otelcol-contrib\", \"service.version\": \"0.131.0\"}, \"otelcol.component.id\": \"opensearch\", \"otelcol.component.kind\": \"exporter\", \"otelcol.signal\": \"logs\", \"error\": \"not retryable error: Permanent error: Permanent error: flush: dial tcp 10.96.44.10:9200: connect: connection refused\\nPermanent error: Permanent error: flush: dial tcp 10.96.44.10:9200: connect: connection refused\", \"rejected_items\": 1}\ngo.opentelemetry.io/collector/exporter/exporterhelper/internal.(*BaseExporter).Send\n\tgo.opentelemetry.io/collector/exporter@v0.131.0/exporterhelper/internal/base_exporter.go:117\ngo.opentelemetry.io/collector/exporter/exporterhelper.NewLogsRequest.newConsumeLogs.func1\n\tgo.opentelemetry.io/collector/exporter@v0.131.0/exporterhelper/logs.go:191\ngo.opentelemetry.io/collector/consumer.ConsumeLogsFunc.ConsumeLogs\n\tgo.opentelemetry.io/collector/consumer@v1.37.0/logs.go:27\ngo.opentelemetry.io/collector/internal/fanoutconsumer.(*logsConsumer).ConsumeLogs\n\tgo.opentelemetry.io/collector/internal/fanoutconsumer@v0.131.0/logs.go:64\ngo.opentelemetry.io/collector/processor/batchprocessor.(*batchLogs).export\n\tgo.opentelemetry.io/collector/processor/batchprocessor@v0.131.0/batch_processor.go:543\ngo.opentelemetry.io/collector/processor/batchprocessor.(*shard[...]).sendItems\n\tgo.opentelemetry.io/collector/processor/batchprocessor@v0.131.0/batch_processor.go:262\ngo.opentelemetry.io/collector/processor/batchprocessor.(*shard[...]).startLoop\n\tgo.opentelemetry.io/collector/processor/batchprocessor@v0.131.0/batch_processor.go:222\n2025-08-19T18:28:24.055Z\tinfo\tLogs\t{\"resource\": {\"service.instance.id\": \"2e7d82f3-fb74-414f-a45c-c6b01d1a215a\", \"service.name\": \"otelcol-contrib\", \"service.version\": \"0.131.0\"}, \"otelcol.component.id\": \"debug\", \"otelcol.component.kind\": \"exporter\", \"otelcol.signal\": \"logs\", \"resource logs\": 1, \"log records\": 1}\n2025-08-19T18:28:24.055Z\twarn\tbatchprocessor@v0.131.0/batch_processor.go:264\tSender failed\t{\"resource\": {\"service.instance.id\": \"2e7d82f3-fb74-414f-a45c-c6b01d1a215a\", \"service.name\": \"otelcol-contrib\", \"service.version\": \"0.131.0\"}, \"otelcol.component.id\": \"batch\", \"otelcol.component.kind\": \"processor\", \"otelcol.pipeline.id\": \"logs\", \"otelcol.signal\": \"logs\", \"error\": \"not retryable error: Permanent error: Permanent error: flush: dial tcp 10.96.44.10:9200: connect: connection refused\\nPermanent error: Permanent error: flush: dial tcp 10.96.44.10:9200: connect: connection refused\"}\n2025-08-19T18:28:25.327Z\terror\topensearchexporter@v0.131.0/logger.go:36\tRequest failed.\t{\"resource\": {\"service.instance.id\": \"2e7d82f3-fb74-414f-a45c-c6b01d1a215a\", \"service.name\": \"otelcol-contrib\", \"service.version\": \"0.131.0\"}, \"otelcol.component.id\": \"opensearch\", \"otelcol.component.kind\": \"exporter\", \"otelcol.signal\": \"logs\", \"path\": \"/_bulk\", \"method\": \"POST\", \"duration\": 1.06305393, \"reason\": \"dial tcp 10.96.44.10:9200: connect: connection refused\"}\ngithub.com/open-telemetry/opentelemetry-collector-contrib/exporter/opensearchexporter.(*clientLogger).LogRoundTrip\n\tgithub.com/open-telemetry/opentelemetry-collector-contrib/exporter/opensearchexporter@v0.131.0/logger.go:36\ngithub.com/opensearch-project/opensearch-go/v2/opensearchtransport.(*Client).logRoundTrip\n\tgithub.com/opensearch-project/opensearch-go/v2@v2.3.0/opensearchtransport/opensearchtransport.go:498\ngithub.com/opensearch-project/opensearch-go/v2/opensearchtransport.(*Client).Perform\n\tgithub.com/opensearch-project/opensearch-go/v2@v2.3.0/opensearchtransport/opensearchtransport.go:337\ngithub.com/opensearch-project/opensearch-go/v2.(*Client).Perform\n\tgithub.com/opensearch-project/opensearch-go/v2@v2.3.0/opensearch.go:267\ngithub.com/opensearch-project/opensearch-go/v2/opensearchapi.BulkRequest.Do\n\tgithub.com/opensearch-project/opensearch-go/v2@v2.3.0/opensearchapi/api.bulk.go:192\ngithub.com/opensearch-project/opensearch-go/v2/opensearchutil.(*worker).flush\n\tgithub.com/opensearch-project/opensearch-go/v2@v2.3.0/opensearchutil/bulk_indexer.go:552\ngithub.com/opensearch-project/opensearch-go/v2/opensearchutil.(*bulkIndexer).Close\n\tgithub.com/opensearch-project/opensearch-go/v2@v2.3.0/opensearchutil/bulk_indexer.go:299\ngithub.com/open-telemetry/opentelemetry-collector-contrib/exporter/opensearchexporter.(*logBulkIndexer).close\n\tgithub.com/open-telemetry/opentelemetry-collector-contrib/exporter/opensearchexporter@v0.131.0/log_bulk_indexer.go:41\ngithub.com/open-telemetry/opentelemetry-collector-contrib/exporter/opensearchexporter.(*logExporter).pushLogData\n\tgithub.com/open-telemetry/opentelemetry-collector-contrib/exporter/opensearchexporter@v0.131.0/sso_log_exporter.go:88\ngo.opentelemetry.io/collector/consumer.ConsumeLogsFunc.ConsumeLogs\n\tgo.opentelemetry.io/collector/consumer@v1.37.0/logs.go:27\ngo.opentelemetry.io/collector/exporter/exporterhelper.NewLogs.requestConsumeFromLogs.func2\n\tgo.opentelemetry.io/collector/exporter@v0.131.0/exporterhelper/logs.go:136\ngo.opentelemetry.io/collector/exporter/exporterhelper/internal/sender.(*sender[...]).Send\n\tgo.opentelemetry.io/collector/exporter@v0.131.0/exporterhelper/internal/sender/sender.go:31\ngo.opentelemetry.io/collector/exporter/exporterhelper/internal.(*retrySender).Send\n\tgo.opentelemetry.io/collector/exporter@v0.131.0/exporterhelper/internal/retry_sender.go:91\ngo.opentelemetry.io/collector/exporter/exporterhelper/internal.(*obsReportSender[...]).Send\n\tgo.opentelemetry.io/collector/exporter@v0.131.0/exporterhelper/internal/obs_report_sender.go:92\ngo.opentelemetry.io/collector/exporter/exporterhelper/internal.(*BaseExporter).Send\n\tgo.opentelemetry.io/collector/exporter@v0.131.0/exporterhelper/internal/base_exporter.go:115\ngo.opentelemetry.io/collector/exporter/exporterhelper.NewLogsRequest.newConsumeLogs.func1\n\tgo.opentelemetry.io/collector/exporter@v0.131.0/exporterhelper/logs.go:191\ngo.opentelemetry.io/collector/consumer.ConsumeLogsFunc.ConsumeLogs\n\tgo.opentelemetry.io/collector/consumer@v1.37.0/logs.go:27\ngo.opentelemetry.io/collector/internal/fanoutconsumer.(*logsConsumer).ConsumeLogs\n\tgo.opentelemetry.io/collector/internal/fanoutconsumer@v0.131.0/logs.go:64\ngo.opentelemetry.io/collector/processor/batchprocessor.(*batchLogs).export\n\tgo.opentelemetry.io/collector/processor/batchprocessor@v0.131.0/batch_processor.go:543\ngo.opentelemetry.io/collector/processor/batchprocessor.(*shard[...]).sendItems\n\tgo.opentelemetry.io/collector/processor/batchprocessor@v0.131.0/batch_processor.go:262\ngo.opentelemetry.io/collector/processor/batchprocessor.(*shard[...]).startLoop\n\tgo.opentelemetry.io/collector/processor/batchprocessor@v0.131.0/batch_processor.go:222\n2025-08-19T18:28:25.331Z\terror\tinternal/base_exporter.go:117\tExporting failed. Rejecting data. Try enabling sending_queue to survive temporary failures.\t{\"resource\": {\"service.instance.id\": \"2e7d82f3-fb74-414f-a45c-c6b01d1a215a\", \"service.name\": \"otelcol-contrib\", \"service.version\": \"0.131.0\"}, \"otelcol.component.id\": \"opensearch\", \"otelcol.component.kind\": \"exporter\", \"otelcol.signal\": \"logs\", \"error\": \"not retryable error: Permanent error: Permanent error: flush: dial tcp 10.96.44.10:9200: connect: connection refused\\nPermanent error: Permanent error: flush: dial tcp 10.96.44.10:9200: connect: connection refused\", \"rejected_items\": 66}\ngo.opentelemetry.io/collector/exporter/exporterhelper/internal.(*BaseExporter).Send\n\tgo.opentelemetry.io/collector/exporter@v0.131.0/exporterhelper/internal/base_exporter.go:117\ngo.opentelemetry.io/collector/exporter/exporterhelper.NewLogsRequest.newConsumeLogs.func1\n\tgo.opentelemetry.io/collector/exporter@v0.131.0/exporterhelper/logs.go:191\ngo.opentelemetry.io/collector/consumer.ConsumeLogsFunc.ConsumeLogs\n\tgo.opentelemetry.io/collector/consumer@v1.37.0/logs.go:27\ngo.opentelemetry.io/collector/internal/fanoutconsumer.(*logsConsumer).ConsumeLogs\n\tgo.opentelemetry.io/collector/internal/fanoutconsumer@v0.131.0/logs.go:64\ngo.opentelemetry.io/collector/processor/batchprocessor.(*batchLogs).export\n\tgo.opentelemetry.io/collector/processor/batchprocessor@v0.131.0/batch_processor.go:543\ngo.opentelemetry.io/collector/processor/batchprocessor.(*shard[...]).sendItems\n\tgo.opentelemetry.io/collector/processor/batchprocessor@v0.131.0/batch_processor.go:262\ngo.opentelemetry.io/collector/processor/batchprocessor.(*shard[...]).startLoop\n\tgo.opentelemetry.io/collector/processor/batchprocessor@v0.131.0/batch_processor.go:222\n2025-08-19T18:28:25.331Z\tinfo\tLogs\t{\"resource\": {\"service.instance.id\": \"2e7d82f3-fb74-414f-a45c-c6b01d1a215a\", \"service.name\": \"otelcol-contrib\", \"service.version\": \"0.131.0\"}, \"otelcol.component.id\": \"debug\", \"otelcol.component.kind\": \"exporter\", \"otelcol.signal\": \"logs\", \"resource logs\": 16, \"log records\": 66}\n2025-08-19T18:28:25.331Z\twarn\tbatchprocessor@v0.131.0/batch_processor.go:264\tSender failed\t{\"resource\": {\"service.instance.id\": \"2e7d82f3-fb74-414f-a45c-c6b01d1a215a\", \"service.name\": \"otelcol-contrib\", \"service.version\": \"0.131.0\"}, \"otelcol.component.id\": \"batch\", \"otelcol.component.kind\": \"processor\", \"otelcol.pipeline.id\": \"logs\", \"otelcol.signal\": \"logs\", \"error\": \"not retryable error: Permanent error: Permanent error: flush: dial tcp 10.96.44.10:9200: connect: connection refused\\nPermanent error: Permanent error: flush: dial tcp 10.96.44.10:9200: connect: connection refused\"}\n2025-08-19T18:28:26.729Z\twarn\tgrpc@v1.74.2/clientconn.go:1414\t[core] [Channel #1 SubChannel #37]grpc: addrConn.createTransport failed to connect to {Addr: \"10.96.63.99:4318\", ServerName: \"otel-collector:4318\", }. Err: connection error: desc = \"error reading server preface: http2: failed reading the frame payload: http2: frame too large, note that the frame header looked like an HTTP/1.1 header\"\t{\"resource\": {\"service.instance.id\": \"2e7d82f3-fb74-414f-a45c-c6b01d1a215a\", \"service.name\": \"otelcol-contrib\", \"service.version\": \"0.131.0\"}, \"grpc_log\": true}\n2025-08-19T18:28:27.250Z\tinfo\tinternal/retry_sender.go:133\tExporting failed. Will retry the request after interval.\t{\"resource\": {\"service.instance.id\": \"2e7d82f3-fb74-414f-a45c-c6b01d1a215a\", \"service.name\": \"otelcol-contrib\", \"service.version\": \"0.131.0\"}, \"otelcol.component.id\": \"otlphttp/prometheus\", \"otelcol.component.kind\": \"exporter\", \"otelcol.signal\": \"metrics\", \"error\": \"failed to make an HTTP request: Post \\\"http://prometheus:9090/api/v1/otlp/v1/metrics\\\": dial tcp 10.96.227.216:9090: connect: connection refused\", \"interval\": \"10.851476151s\"}\n2025-08-19T18:28:27.283Z\tinfo\tLogs\t{\"resource\": {\"service.instance.id\": \"2e7d82f3-fb74-414f-a45c-c6b01d1a215a\", \"service.name\": \"otelcol-contrib\", \"service.version\": \"0.131.0\"}, \"otelcol.component.id\": \"debug\", \"otelcol.component.kind\": \"exporter\", \"otelcol.signal\": \"logs\", \"resource logs\": 14, \"log records\": 36}\n2025-08-19T18:28:27.743Z\twarn\tgrpc@v1.74.2/clientconn.go:1414\t[core] [Channel #1 SubChannel #37]grpc: addrConn.createTransport failed to connect to {Addr: \"10.96.63.99:4318\", ServerName: \"otel-collector:4318\", }. Err: connection error: desc = \"error reading server preface: http2: failed reading the frame payload: http2: frame too large, note that the frame header looked like an HTTP/1.1 header\"\t{\"resource\": {\"service.instance.id\": \"2e7d82f3-fb74-414f-a45c-c6b01d1a215a\", \"service.name\": \"otelcol-contrib\", \"service.version\": \"0.131.0\"}, \"grpc_log\": true}\n2025-08-19T18:28:27.959Z\tinfo\tMetrics\t{\"resource\": {\"service.instance.id\": \"2e7d82f3-fb74-414f-a45c-c6b01d1a215a\", \"service.name\": \"otelcol-contrib\", \"service.version\": \"0.131.0\"}, \"otelcol.component.id\": \"debug\", \"otelcol.component.kind\": \"exporter\", \"otelcol.signal\": \"metrics\", \"resource metrics\": 1, \"metrics\": 29, \"data points\": 34}\n2025-08-19T18:28:28.990Z\tinfo\tinternal/retry_sender.go:133\tExporting failed. Will retry the request after interval.\t{\"resource\": {\"service.instance.id\": \"2e7d82f3-fb74-414f-a45c-c6b01d1a215a\", \"service.name\": \"otelcol-contrib\", \"service.version\": \"0.131.0\"}, \"otelcol.component.id\": \"otlphttp/prometheus\", \"otelcol.component.kind\": \"exporter\", \"otelcol.signal\": \"metrics\", \"error\": \"failed to make an HTTP request: Post \\\"http://prometheus:9090/api/v1/otlp/v1/metrics\\\": dial tcp 10.96.227.216:9090: connect: connection refused\", \"interval\": \"2.838121068s\"}\n2025-08-19T18:28:29.505Z\twarn\tgrpc@v1.74.2/clientconn.go:1414\t[core] [Channel #1 SubChannel #37]grpc: addrConn.createTransport failed to connect to {Addr: \"10.96.63.99:4318\", ServerName: \"otel-collector:4318\", }. Err: connection error: desc = \"error reading server preface: http2: failed reading the frame payload: http2: frame too large, note that the frame header looked like an HTTP/1.1 header\"\t{\"resource\": {\"service.instance.id\": \"2e7d82f3-fb74-414f-a45c-c6b01d1a215a\", \"service.name\": \"otelcol-contrib\", \"service.version\": \"0.131.0\"}, \"grpc_log\": true}\n2025-08-19T18:28:29.670Z\tinfo\tTraces\t{\"resource\": {\"service.instance.id\": \"2e7d82f3-fb74-414f-a45c-c6b01d1a215a\", \"service.name\": \"otelcol-contrib\", \"service.version\": \"0.131.0\"}, \"otelcol.component.id\": \"debug\", \"otelcol.component.kind\": \"exporter\", \"otelcol.signal\": \"traces\", \"resource spans\": 12, \"spans\": 95}\n2025-08-19T18:28:29.873Z\tinfo\tTraces\t{\"resource\": {\"service.instance.id\": \"2e7d82f3-fb74-414f-a45c-c6b01d1a215a\", \"service.name\": \"otelcol-contrib\", \"service.version\": \"0.131.0\"}, \"otelcol.component.id\": \"debug\", \"otelcol.component.kind\": \"exporter\", \"otelcol.signal\": \"traces\", \"resource spans\": 9, \"spans\": 37}\n2025-08-19T18:28:29.913Z\tinfo\tLogs\t{\"resource\": {\"service.instance.id\": \"2e7d82f3-fb74-414f-a45c-c6b01d1a215a\", \"service.name\": \"otelcol-contrib\", \"service.version\": \"0.131.0\"}, \"otelcol.component.id\": \"debug\", \"otelcol.component.kind\": \"exporter\", \"otelcol.signal\": \"logs\", \"resource logs\": 16, \"log records\": 85}\n2025-08-19T18:28:30.061Z\tinfo\tinternal/retry_sender.go:133\tExporting failed. Will retry the request after interval.\t{\"resource\": {\"service.instance.id\": \"2e7d82f3-fb74-414f-a45c-c6b01d1a215a\", \"service.name\": \"otelcol-contrib\", \"service.version\": \"0.131.0\"}, \"otelcol.component.id\": \"otlphttp/prometheus\", \"otelcol.component.kind\": \"exporter\", \"otelcol.signal\": \"metrics\", \"error\": \"failed to make an HTTP request: Post \\\"http://prometheus:9090/api/v1/otlp/v1/metrics\\\": dial tcp 10.96.227.216:9090: connect: connection refused\", \"interval\": \"12.927372266s\"}\n2025-08-19T18:28:30.073Z\tinfo\tTraces\t{\"resource\": {\"service.instance.id\": \"2e7d82f3-fb74-414f-a45c-c6b01d1a215a\", \"service.name\": \"otelcol-contrib\", \"service.version\": \"0.131.0\"}, \"otelcol.component.id\": \"debug\", \"otelcol.component.kind\": \"exporter\", \"otelcol.signal\": \"traces\", \"resource spans\": 3, \"spans\": 29}\n2025-08-19T18:28:30.273Z\tinfo\tTraces\t{\"resource\": {\"service.instance.id\": \"2e7d82f3-fb74-414f-a45c-c6b01d1a215a\", \"service.name\": \"otelcol-contrib\", \"service.version\": \"0.131.0\"}, \"otelcol.component.id\": \"debug\", \"otelcol.component.kind\": \"exporter\", \"otelcol.signal\": \"traces\", \"resource spans\": 1, \"spans\": 20}\n2025-08-19T18:28:30.475Z\tinfo\tTraces\t{\"resource\": {\"service.instance.id\": \"2e7d82f3-fb74-414f-a45c-c6b01d1a215a\", \"service.name\": \"otelcol-contrib\", \"service.version\": \"0.131.0\"}, \"otelcol.component.id\": \"debug\", \"otelcol.component.kind\": \"exporter\", \"otelcol.signal\": \"traces\", \"resource spans\": 4, \"spans\": 55}\n2025-08-19T18:28:30.563Z\tinfo\tLogs\t{\"resource\": {\"service.instance.id\": \"2e7d82f3-fb74-414f-a45c-c6b01d1a215a\", \"service.name\": \"otelcol-contrib\", \"service.version\": \"0.131.0\"}, \"otelcol.component.id\": \"debug\", \"otelcol.component.kind\": \"exporter\", \"otelcol.signal\": \"logs\", \"resource logs\": 1, \"log records\": 31}\n2025-08-19T18:28:30.678Z\tinfo\tTraces\t{\"resource\": {\"service.instance.id\": \"2e7d82f3-fb74-414f-a45c-c6b01d1a215a\", \"service.name\": \"otelcol-contrib\", \"service.version\": \"0.131.0\"}, \"otelcol.component.id\": \"debug\", \"otelcol.component.kind\": \"exporter\", \"otelcol.signal\": \"traces\", \"resource spans\": 3, \"spans\": 3}\n2025-08-19T18:28:30.783Z\tinfo\tLogs\t{\"resource\": {\"service.instance.id\": \"2e7d82f3-fb74-414f-a45c-c6b01d1a215a\", \"service.name\": \"otelcol-contrib\", \"service.version\": \"0.131.0\"}, \"otelcol.component.id\": \"debug\", \"otelcol.component.kind\": \"exporter\", \"otelcol.signal\": \"logs\", \"resource logs\": 12, \"log records\": 27}\n2025-08-19T18:28:30.881Z\tinfo\tTraces\t{\"resource\": {\"service.instance.id\": \"2e7d82f3-fb74-414f-a45c-c6b01d1a215a\", \"service.name\": \"otelcol-contrib\", \"service.version\": \"0.131.0\"}, \"otelcol.component.id\": \"debug\", \"otelcol.component.kind\": \"exporter\", \"otelcol.signal\": \"traces\", \"resource spans\": 1, \"spans\": 5}\n2025-08-19T18:28:31.084Z\tinfo\tTraces\t{\"resource\": {\"service.instance.id\": \"2e7d82f3-fb74-414f-a45c-c6b01d1a215a\", \"service.name\": \"otelcol-contrib\", \"service.version\": \"0.131.0\"}, \"otelcol.component.id\": \"debug\", \"otelcol.component.kind\": \"exporter\", \"otelcol.signal\": \"traces\", \"resource spans\": 5, \"spans\": 25}\n2025-08-19T18:28:31.204Z\tinfo\tLogs\t{\"resource\": {\"service.instance.id\": \"2e7d82f3-fb74-414f-a45c-c6b01d1a215a\", \"service.name\": \"otelcol-contrib\", \"service.version\": \"0.131.0\"}, \"otelcol.component.id\": \"debug\", \"otelcol.component.kind\": \"exporter\", \"otelcol.signal\": \"logs\", \"resource logs\": 1, \"log records\": 1}\n2025-08-19T18:28:31.284Z\tinfo\tTraces\t{\"resource\": {\"service.instance.id\": \"2e7d82f3-fb74-414f-a45c-c6b01d1a215a\", \"service.name\": \"otelcol-contrib\", \"service.version\": \"0.131.0\"}, \"otelcol.component.id\": \"debug\", \"otelcol.component.kind\": \"exporter\", \"otelcol.signal\": \"traces\", \"resource spans\": 11, \"spans\": 59}\n2025-08-19T18:28:31.420Z\tinfo\tLogs\t{\"resource\": {\"service.instance.id\": \"2e7d82f3-fb74-414f-a45c-c6b01d1a215a\", \"service.name\": \"otelcol-contrib\", \"service.version\": \"0.131.0\"}, \"otelcol.component.id\": \"debug\", \"otelcol.component.kind\": \"exporter\", \"otelcol.signal\": \"logs\", \"resource logs\": 1, \"log records\": 1}\n2025-08-19T18:28:31.486Z\tinfo\tTraces\t{\"resource\": {\"service.instance.id\": \"2e7d82f3-fb74-414f-a45c-c6b01d1a215a\", \"service.name\": \"otelcol-contrib\", \"service.version\": \"0.131.0\"}, \"otelcol.component.id\": \"debug\", \"otelcol.component.kind\": \"exporter\", \"otelcol.signal\": \"traces\", \"resource spans\": 4, \"spans\": 12}\n2025-08-19T18:28:31.658Z\tinfo\tLogs\t{\"resource\": {\"service.instance.id\": \"2e7d82f3-fb74-414f-a45c-c6b01d1a215a\", \"service.name\": \"otelcol-contrib\", \"service.version\": \"0.131.0\"}, \"otelcol.component.id\": \"debug\", \"otelcol.component.kind\": \"exporter\", \"otelcol.signal\": \"logs\", \"resource logs\": 1, \"log records\": 1}\n2025-08-19T18:28:31.674Z\twarn\tgrpc@v1.74.2/clientconn.go:1414\t[core] [Channel #1 SubChannel #37]grpc: addrConn.createTransport failed to connect to {Addr: \"10.96.63.99:4318\", ServerName: \"otel-collector:4318\", }. Err: connection error: desc = \"error reading server preface: http2: failed reading the frame payload: http2: frame too large, note that the frame header looked like an HTTP/1.1 header\"\t{\"resource\": {\"service.instance.id\": \"2e7d82f3-fb74-414f-a45c-c6b01d1a215a\", \"service.name\": \"otelcol-contrib\", \"service.version\": \"0.131.0\"}, \"grpc_log\": true}\n2025/08/19 18:28:31 failed to upload metrics: reader collect and export timeout: rpc error: code = Unavailable desc = connection error: desc = \"error reading server preface: http2: failed reading the frame payload: http2: frame too large, note that the frame header looked like an HTTP/1.1 header\"\n2025-08-19T18:28:31.888Z\tinfo\tLogs\t{\"resource\": {\"service.instance.id\": \"2e7d82f3-fb74-414f-a45c-c6b01d1a215a\", \"service.name\": \"otelcol-contrib\", \"service.version\": \"0.131.0\"}, \"otelcol.component.id\": \"debug\", \"otelcol.component.kind\": \"exporter\", \"otelcol.signal\": \"logs\", \"resource logs\": 12, \"log records\": 46}\n2025-08-19T18:28:32.332Z\tinfo\tLogs\t{\"resource\": {\"service.instance.id\": \"2e7d82f3-fb74-414f-a45c-c6b01d1a215a\", \"service.name\": \"otelcol-contrib\", \"service.version\": \"0.131.0\"}, \"otelcol.component.id\": \"debug\", \"otelcol.component.kind\": \"exporter\", \"otelcol.signal\": \"logs\", \"resource logs\": 1, \"log records\": 1}\n2025-08-19T18:28:32.758Z\tinfo\tLogs\t{\"resource\": {\"service.instance.id\": \"2e7d82f3-fb74-414f-a45c-c6b01d1a215a\", \"service.name\": \"otelcol-contrib\", \"service.version\": \"0.131.0\"}, \"otelcol.component.id\": \"debug\", \"otelcol.component.kind\": \"exporter\", \"otelcol.signal\": \"logs\", \"resource logs\": 5, \"log records\": 6}\n2025-08-19T18:28:32.906Z\tinfo\tinternal/retry_sender.go:133\tExporting failed. Will retry the request after interval.\t{\"resource\": {\"service.instance.id\": \"2e7d82f3-fb74-414f-a45c-c6b01d1a215a\", \"service.name\": \"otelcol-contrib\", \"service.version\": \"0.131.0\"}, \"otelcol.component.id\": \"otlphttp/prometheus\", \"otelcol.component.kind\": \"exporter\", \"otelcol.signal\": \"metrics\", \"error\": \"failed to make an HTTP request: Post \\\"http://prometheus:9090/api/v1/otlp/v1/metrics\\\": dial tcp 10.96.227.216:9090: connect: connection refused\", \"interval\": \"5.529897887s\"}\n2025-08-19T18:28:33.415Z\tinfo\tLogs\t{\"resource\": {\"service.instance.id\": \"2e7d82f3-fb74-414f-a45c-c6b01d1a215a\", \"service.name\": \"otelcol-contrib\", \"service.version\": \"0.131.0\"}, \"otelcol.component.id\": \"debug\", \"otelcol.component.kind\": \"exporter\", \"otelcol.signal\": \"logs\", \"resource logs\": 1, \"log records\": 1}\n2025-08-19T18:28:35.320Z\twarn\tgrpc@v1.74.2/clientconn.go:1414\t[core] [Channel #1 SubChannel #37]grpc: addrConn.createTransport failed to connect to {Addr: \"10.96.63.99:4318\", ServerName: \"otel-collector:4318\", }. Err: connection error: desc = \"error reading server preface: http2: failed reading the frame payload: http2: frame too large, note that the frame header looked like an HTTP/1.1 header\"\t{\"resource\": {\"service.instance.id\": \"2e7d82f3-fb74-414f-a45c-c6b01d1a215a\", \"service.name\": \"otelcol-contrib\", \"service.version\": \"0.131.0\"}, \"grpc_log\": true}\n2025-08-19T18:28:37.833Z\tinfo\tMetrics\t{\"resource\": {\"service.instance.id\": \"2e7d82f3-fb74-414f-a45c-c6b01d1a215a\", \"service.name\": \"otelcol-contrib\", \"service.version\": \"0.131.0\"}, \"otelcol.component.id\": \"debug\", \"otelcol.component.kind\": \"exporter\", \"otelcol.signal\": \"metrics\", \"resource metrics\": 1, \"metrics\": 29, \"data points\": 34}\n2025-08-19T18:28:37.881Z\tinfo\tinternal/retry_sender.go:133\tExporting failed. Will retry the request after interval.\t{\"resource\": {\"service.instance.id\": \"2e7d82f3-fb74-414f-a45c-c6b01d1a215a\", \"service.name\": \"otelcol-contrib\", \"service.version\": \"0.131.0\"}, \"otelcol.component.id\": \"otlphttp/prometheus\", \"otelcol.component.kind\": \"exporter\", \"otelcol.signal\": \"metrics\", \"error\": \"failed to make an HTTP request: Post \\\"http://prometheus:9090/api/v1/otlp/v1/metrics\\\": dial tcp 10.96.227.216:9090: connect: connection refused\", \"interval\": \"4.645653507s\"}\n2025-08-19T18:28:38.162Z\tinfo\tinternal/retry_sender.go:133\tExporting failed. Will retry the request after interval.\t{\"resource\": {\"service.instance.id\": \"2e7d82f3-fb74-414f-a45c-c6b01d1a215a\", \"service.name\": \"otelcol-contrib\", \"service.version\": \"0.131.0\"}, \"otelcol.component.id\": \"otlphttp/prometheus\", \"otelcol.component.kind\": \"exporter\", \"otelcol.signal\": \"metrics\", \"error\": \"failed to make an HTTP request: Post \\\"http://prometheus:9090/api/v1/otlp/v1/metrics\\\": dial tcp 10.96.227.216:9090: connect: connection refused\", \"interval\": \"10.777203492s\"}\n2025-08-19T18:28:38.494Z\tinfo\tinternal/retry_sender.go:133\tExporting failed. Will retry the request after interval.\t{\"resource\": {\"service.instance.id\": \"2e7d82f3-fb74-414f-a45c-c6b01d1a215a\", \"service.name\": \"otelcol-contrib\", \"service.version\": \"0.131.0\"}, \"otelcol.component.id\": \"otlphttp/prometheus\", \"otelcol.component.kind\": \"exporter\", \"otelcol.signal\": \"metrics\", \"error\": \"failed to make an HTTP request: Post \\\"http://prometheus:9090/api/v1/otlp/v1/metrics\\\": dial tcp 10.96.227.216:9090: connect: connection refused\", \"interval\": \"6.92847575s\"}\n2025-08-19T18:28:39.811Z\tinfo\tTraces\t{\"resource\": {\"service.instance.id\": \"2e7d82f3-fb74-414f-a45c-c6b01d1a215a\", \"service.name\": \"otelcol-contrib\", \"service.version\": \"0.131.0\"}, \"otelcol.component.id\": \"debug\", \"otelcol.component.kind\": \"exporter\", \"otelcol.signal\": \"traces\", \"resource spans\": 8, \"spans\": 51}\n2025-08-19T18:28:40.028Z\tinfo\tLogs\t{\"resource\": {\"service.instance.id\": \"2e7d82f3-fb74-414f-a45c-c6b01d1a215a\", \"service.name\": \"otelcol-contrib\", \"service.version\": \"0.131.0\"}, \"otelcol.component.id\": \"debug\", \"otelcol.component.kind\": \"exporter\", \"otelcol.signal\": \"logs\", \"resource logs\": 1, \"log records\": 9}\n2025-08-19T18:28:40.039Z\tinfo\tTraces\t{\"resource\": {\"service.instance.id\": \"2e7d82f3-fb74-414f-a45c-c6b01d1a215a\", \"service.name\": \"otelcol-contrib\", \"service.version\": \"0.131.0\"}, \"otelcol.component.id\": \"debug\", \"otelcol.component.kind\": \"exporter\", \"otelcol.signal\": \"traces\", \"resource spans\": 1, \"spans\": 22}\n2025-08-19T18:28:40.241Z\tinfo\tTraces\t{\"resource\": {\"service.instance.id\": \"2e7d82f3-fb74-414f-a45c-c6b01d1a215a\", \"service.name\": \"otelcol-contrib\", \"service.version\": \"0.131.0\"}, \"otelcol.component.id\": \"debug\", \"otelcol.component.kind\": \"exporter\", \"otelcol.signal\": \"traces\", \"resource spans\": 2, \"spans\": 43}\n2025-08-19T18:28:40.977Z\tinfo\tLogs\t{\"resource\": {\"service.instance.id\": \"2e7d82f3-fb74-414f-a45c-c6b01d1a215a\", \"service.name\": \"otelcol-contrib\", \"service.version\": \"0.131.0\"}, \"otelcol.component.id\": \"debug\", \"otelcol.component.kind\": \"exporter\", \"otelcol.signal\": \"logs\", \"resource logs\": 4, \"log records\": 7}\n2025-08-19T18:28:41.060Z\tinfo\tTraces\t{\"resource\": {\"service.instance.id\": \"2e7d82f3-fb74-414f-a45c-c6b01d1a215a\", \"service.name\": \"otelcol-contrib\", \"service.version\": \"0.131.0\"}, \"otelcol.component.id\": \"debug\", \"otelcol.component.kind\": \"exporter\", \"otelcol.signal\": \"traces\", \"resource spans\": 4, \"spans\": 20}\n2025-08-19T18:28:41.265Z\tinfo\tTraces\t{\"resource\": {\"service.instance.id\": \"2e7d82f3-fb74-414f-a45c-c6b01d1a215a\", \"service.name\": \"otelcol-contrib\", \"service.version\": \"0.131.0\"}, \"otelcol.component.id\": \"debug\", \"otelcol.component.kind\": \"exporter\", \"otelcol.signal\": \"traces\", \"resource spans\": 7, \"spans\": 31}\n2025-08-19T18:28:41.442Z\tinfo\tLogs\t{\"resource\": {\"service.instance.id\": \"2e7d82f3-fb74-414f-a45c-c6b01d1a215a\", \"service.name\": \"otelcol-contrib\", \"service.version\": \"0.131.0\"}, \"otelcol.component.id\": \"debug\", \"otelcol.component.kind\": \"exporter\", \"otelcol.signal\": \"logs\", \"resource logs\": 1, \"log records\": 1}\n2025-08-19T18:28:41.480Z\tinfo\tTraces\t{\"resource\": {\"service.instance.id\": \"2e7d82f3-fb74-414f-a45c-c6b01d1a215a\", \"service.name\": \"otelcol-contrib\", \"service.version\": \"0.131.0\"}, \"otelcol.component.id\": \"debug\", \"otelcol.component.kind\": \"exporter\", \"otelcol.signal\": \"traces\", \"resource spans\": 3, \"spans\": 24}\n2025-08-19T18:28:41.685Z\tinfo\tTraces\t{\"resource\": {\"service.instance.id\": \"2e7d82f3-fb74-414f-a45c-c6b01d1a215a\", \"service.name\": \"otelcol-contrib\", \"service.version\": \"0.131.0\"}, \"otelcol.component.id\": \"debug\", \"otelcol.component.kind\": \"exporter\", \"otelcol.signal\": \"traces\", \"resource spans\": 1, \"spans\": 12}\n2025/08/19 18:28:41 failed to upload metrics: reader collect and export timeout: rpc error: code = Unavailable desc = connection error: desc = \"error reading server preface: http2: failed reading the frame payload: http2: frame too large, note that the frame header looked like an HTTP/1.1 header\"\n2025-08-19T18:28:41.895Z\tinfo\tLogs\t{\"resource\": {\"service.instance.id\": \"2e7d82f3-fb74-414f-a45c-c6b01d1a215a\", \"service.name\": \"otelcol-contrib\", \"service.version\": \"0.131.0\"}, \"otelcol.component.id\": \"debug\", \"otelcol.component.kind\": \"exporter\", \"otelcol.signal\": \"logs\", \"resource logs\": 6, \"log records\": 28}\n2025-08-19T18:28:41.899Z\tinfo\tTraces\t{\"resource\": {\"service.instance.id\": \"2e7d82f3-fb74-414f-a45c-c6b01d1a215a\", \"service.name\": \"otelcol-contrib\", \"service.version\": \"0.131.0\"}, \"otelcol.component.id\": \"debug\", \"otelcol.component.kind\": \"exporter\", \"otelcol.signal\": \"traces\", \"resource spans\": 1, \"spans\": 23}\n2025-08-19T18:28:41.987Z\twarn\tgrpc@v1.74.2/clientconn.go:1414\t[core] [Channel #1 SubChannel #37]grpc: addrConn.createTransport failed to connect to {Addr: \"10.96.63.99:4318\", ServerName: \"otel-collector:4318\", }. Err: connection error: desc = \"error reading server preface: http2: failed reading the frame payload: http2: frame too large, note that the frame header looked like an HTTP/1.1 header\"\t{\"resource\": {\"service.instance.id\": \"2e7d82f3-fb74-414f-a45c-c6b01d1a215a\", \"service.name\": \"otelcol-contrib\", \"service.version\": \"0.131.0\"}, \"grpc_log\": true}\n2025-08-19T18:28:42.310Z\tinfo\tTraces\t{\"resource\": {\"service.instance.id\": \"2e7d82f3-fb74-414f-a45c-c6b01d1a215a\", \"service.name\": \"otelcol-contrib\", \"service.version\": \"0.131.0\"}, \"otelcol.component.id\": \"debug\", \"otelcol.component.kind\": \"exporter\", \"otelcol.signal\": \"traces\", \"resource spans\": 1, \"spans\": 5}\n2025-08-19T18:28:42.364Z\tinfo\tLogs\t{\"resource\": {\"service.instance.id\": \"2e7d82f3-fb74-414f-a45c-c6b01d1a215a\", \"service.name\": \"otelcol-contrib\", \"service.version\": \"0.131.0\"}, \"otelcol.component.id\": \"debug\", \"otelcol.component.kind\": \"exporter\", \"otelcol.signal\": \"logs\", \"resource logs\": 1, \"log records\": 3}\n2025-08-19T18:28:42.536Z\tinfo\tTraces\t{\"resource\": {\"service.instance.id\": \"2e7d82f3-fb74-414f-a45c-c6b01d1a215a\", \"service.name\": \"otelcol-contrib\", \"service.version\": \"0.131.0\"}, \"otelcol.component.id\": \"debug\", \"otelcol.component.kind\": \"exporter\", \"otelcol.signal\": \"traces\", \"resource spans\": 2, \"spans\": 8}\n2025-08-19T18:28:42.585Z\tinfo\tinternal/retry_sender.go:133\tExporting failed. Will retry the request after interval.\t{\"resource\": {\"service.instance.id\": \"2e7d82f3-fb74-414f-a45c-c6b01d1a215a\", \"service.name\": \"otelcol-contrib\", \"service.version\": \"0.131.0\"}, \"otelcol.component.id\": \"otlphttp/prometheus\", \"otelcol.component.kind\": \"exporter\", \"otelcol.signal\": \"metrics\", \"error\": \"failed to make an HTTP request: Post \\\"http://prometheus:9090/api/v1/otlp/v1/metrics\\\": dial tcp 10.96.227.216:9090: connect: connection refused\", \"interval\": \"5.672932344s\"}\n2025-08-19T18:28:42.870Z\tinfo\tLogs\t{\"resource\": {\"service.instance.id\": \"2e7d82f3-fb74-414f-a45c-c6b01d1a215a\", \"service.name\": \"otelcol-contrib\", \"service.version\": \"0.131.0\"}, \"otelcol.component.id\": \"debug\", \"otelcol.component.kind\": \"exporter\", \"otelcol.signal\": \"logs\", \"resource logs\": 9, \"log records\": 23}\n2025-08-19T18:28:43.119Z\tinfo\tLogs\t{\"resource\": {\"service.instance.id\": \"2e7d82f3-fb74-414f-a45c-c6b01d1a215a\", \"service.name\": \"otelcol-contrib\", \"service.version\": \"0.131.0\"}, \"otelcol.component.id\": \"debug\", \"otelcol.component.kind\": \"exporter\", \"otelcol.signal\": \"logs\", \"resource logs\": 2, \"log records\": 2}\n2025-08-19T18:28:43.383Z\tinfo\tLogs\t{\"resource\": {\"service.instance.id\": \"2e7d82f3-fb74-414f-a45c-c6b01d1a215a\", \"service.name\": \"otelcol-contrib\", \"service.version\": \"0.131.0\"}, \"otelcol.component.id\": \"debug\", \"otelcol.component.kind\": \"exporter\", \"otelcol.signal\": \"logs\", \"resource logs\": 3, \"log records\": 20}\n2025-08-19T18:28:43.985Z\tinfo\tLogs\t{\"resource\": {\"service.instance.id\": \"2e7d82f3-fb74-414f-a45c-c6b01d1a215a\", \"service.name\": \"otelcol-contrib\", \"service.version\": \"0.131.0\"}, \"otelcol.component.id\": \"debug\", \"otelcol.component.kind\": \"exporter\", \"otelcol.signal\": \"logs\", \"resource logs\": 13, \"log records\": 64}\n2025-08-19T18:28:44.023Z\tinfo\tinternal/retry_sender.go:133\tExporting failed. Will retry the request after interval.\t{\"resource\": {\"service.instance.id\": \"2e7d82f3-fb74-414f-a45c-c6b01d1a215a\", \"service.name\": \"otelcol-contrib\", \"service.version\": \"0.131.0\"}, \"otelcol.component.id\": \"otlphttp/prometheus\", \"otelcol.component.kind\": \"exporter\", \"otelcol.signal\": \"metrics\", \"error\": \"failed to make an HTTP request: Post \\\"http://prometheus:9090/api/v1/otlp/v1/metrics\\\": dial tcp 10.96.227.216:9090: connect: connection refused\", \"interval\": \"20.759344177s\"}\n2025-08-19T18:28:44.691Z\tinfo\tLogs\t{\"resource\": {\"service.instance.id\": \"2e7d82f3-fb74-414f-a45c-c6b01d1a215a\", \"service.name\": \"otelcol-contrib\", \"service.version\": \"0.131.0\"}, \"otelcol.component.id\": \"debug\", \"otelcol.component.kind\": \"exporter\", \"otelcol.signal\": \"logs\", \"resource logs\": 2, \"log records\": 25}\n2025-08-19T18:28:45.448Z\tinfo\tinternal/retry_sender.go:133\tExporting failed. Will retry the request after interval.\t{\"resource\": {\"service.instance.id\": \"2e7d82f3-fb74-414f-a45c-c6b01d1a215a\", \"service.name\": \"otelcol-contrib\", \"service.version\": \"0.131.0\"}, \"otelcol.component.id\": \"otlphttp/prometheus\", \"otelcol.component.kind\": \"exporter\", \"otelcol.signal\": \"metrics\", \"error\": \"failed to make an HTTP request: Post \\\"http://prometheus:9090/api/v1/otlp/v1/metrics\\\": dial tcp 10.96.227.216:9090: connect: connection refused\", \"interval\": \"22.953018265s\"}\n2025-08-19T18:28:47.918Z\tinfo\tMetrics\t{\"resource\": {\"service.instance.id\": \"2e7d82f3-fb74-414f-a45c-c6b01d1a215a\", \"service.name\": \"otelcol-contrib\", \"service.version\": \"0.131.0\"}, \"otelcol.component.id\": \"debug\", \"otelcol.component.kind\": \"exporter\", \"otelcol.signal\": \"metrics\", \"resource metrics\": 1, \"metrics\": 29, \"data points\": 34}\n2025-08-19T18:28:47.934Z\tinfo\tinternal/retry_sender.go:133\tExporting failed. Will retry the request after interval.\t{\"resource\": {\"service.instance.id\": \"2e7d82f3-fb74-414f-a45c-c6b01d1a215a\", \"service.name\": \"otelcol-contrib\", \"service.version\": \"0.131.0\"}, \"otelcol.component.id\": \"otlphttp/prometheus\", \"otelcol.component.kind\": \"exporter\", \"otelcol.signal\": \"metrics\", \"error\": \"failed to make an HTTP request: Post \\\"http://prometheus:9090/api/v1/otlp/v1/metrics\\\": dial tcp 10.96.227.216:9090: connect: connection refused\", \"interval\": \"4.521491303s\"}\n2025-08-19T18:28:49.892Z\tinfo\tTraces\t{\"resource\": {\"service.instance.id\": \"2e7d82f3-fb74-414f-a45c-c6b01d1a215a\", \"service.name\": \"otelcol-contrib\", \"service.version\": \"0.131.0\"}, \"otelcol.component.id\": \"debug\", \"otelcol.component.kind\": \"exporter\", \"otelcol.signal\": \"traces\", \"resource spans\": 6, \"spans\": 461}\n2025-08-19T18:28:50.119Z\tinfo\tLogs\t{\"resource\": {\"service.instance.id\": \"2e7d82f3-fb74-414f-a45c-c6b01d1a215a\", \"service.name\": \"otelcol-contrib\", \"service.version\": \"0.131.0\"}, \"otelcol.component.id\": \"debug\", \"otelcol.component.kind\": \"exporter\", \"otelcol.signal\": \"logs\", \"resource logs\": 15, \"log records\": 51}\n2025-08-19T18:28:50.317Z\tinfo\tTraces\t{\"resource\": {\"service.instance.id\": \"2e7d82f3-fb74-414f-a45c-c6b01d1a215a\", \"service.name\": \"otelcol-contrib\", \"service.version\": \"0.131.0\"}, \"otelcol.component.id\": \"debug\", \"otelcol.component.kind\": \"exporter\", \"otelcol.signal\": \"traces\", \"resource spans\": 4, \"spans\": 20}\n2025-08-19T18:28:50.541Z\tinfo\tTraces\t{\"resource\": {\"service.instance.id\": \"2e7d82f3-fb74-414f-a45c-c6b01d1a215a\", \"service.name\": \"otelcol-contrib\", \"service.version\": \"0.131.0\"}, \"otelcol.component.id\": \"debug\", \"otelcol.component.kind\": \"exporter\", \"otelcol.signal\": \"traces\", \"resource spans\": 6, \"spans\": 42}\n2025-08-19T18:28:50.665Z\tinfo\tLogs\t{\"resource\": {\"service.instance.id\": \"2e7d82f3-fb74-414f-a45c-c6b01d1a215a\", \"service.name\": \"otelcol-contrib\", \"service.version\": \"0.131.0\"}, \"otelcol.component.id\": \"debug\", \"otelcol.component.kind\": \"exporter\", \"otelcol.signal\": \"logs\", \"resource logs\": 1, \"log records\": 1}\n2025-08-19T18:28:50.759Z\tinfo\tTraces\t{\"resource\": {\"service.instance.id\": \"2e7d82f3-fb74-414f-a45c-c6b01d1a215a\", \"service.name\": \"otelcol-contrib\", \"service.version\": \"0.131.0\"}, \"otelcol.component.id\": \"debug\", \"otelcol.component.kind\": \"exporter\", \"otelcol.signal\": \"traces\", \"resource spans\": 6, \"spans\": 30}\n2025-08-19T18:28:50.993Z\tinfo\tTraces\t{\"resource\": {\"service.instance.id\": \"2e7d82f3-fb74-414f-a45c-c6b01d1a215a\", \"service.name\": \"otelcol-contrib\", \"service.version\": \"0.131.0\"}, \"otelcol.component.id\": \"debug\", \"otelcol.component.kind\": \"exporter\", \"otelcol.signal\": \"traces\", \"resource spans\": 5, \"spans\": 21}\n2025-08-19T18:28:51.000Z\tinfo\tLogs\t{\"resource\": {\"service.instance.id\": \"2e7d82f3-fb74-414f-a45c-c6b01d1a215a\", \"service.name\": \"otelcol-contrib\", \"service.version\": \"0.131.0\"}, \"otelcol.component.id\": \"debug\", \"otelcol.component.kind\": \"exporter\", \"otelcol.signal\": \"logs\", \"resource logs\": 11, \"log records\": 47}\n2025-08-19T18:28:51.115Z\twarn\tgrpc@v1.74.2/clientconn.go:1414\t[core] [Channel #1 SubChannel #37]grpc: addrConn.createTransport failed to connect to {Addr: \"10.96.63.99:4318\", ServerName: \"otel-collector:4318\", }. Err: connection error: desc = \"error reading server preface: http2: failed reading the frame payload: http2: frame too large, note that the frame header looked like an HTTP/1.1 header\"\t{\"resource\": {\"service.instance.id\": \"2e7d82f3-fb74-414f-a45c-c6b01d1a215a\", \"service.name\": \"otelcol-contrib\", \"service.version\": \"0.131.0\"}, \"grpc_log\": true}\n2025-08-19T18:28:51.210Z\tinfo\tTraces\t{\"resource\": {\"service.instance.id\": \"2e7d82f3-fb74-414f-a45c-c6b01d1a215a\", \"service.name\": \"otelcol-contrib\", \"service.version\": \"0.131.0\"}, \"otelcol.component.id\": \"debug\", \"otelcol.component.kind\": \"exporter\", \"otelcol.signal\": \"traces\", \"resource spans\": 2, \"spans\": 28}\n2025-08-19T18:28:51.317Z\tinfo\tLogs\t{\"resource\": {\"service.instance.id\": \"2e7d82f3-fb74-414f-a45c-c6b01d1a215a\", \"service.name\": \"otelcol-contrib\", \"service.version\": \"0.131.0\"}, \"otelcol.component.id\": \"debug\", \"otelcol.component.kind\": \"exporter\", \"otelcol.signal\": \"logs\", \"resource logs\": 1, \"log records\": 1}\n2025-08-19T18:28:51.454Z\tinfo\tTraces\t{\"resource\": {\"service.instance.id\": \"2e7d82f3-fb74-414f-a45c-c6b01d1a215a\", \"service.name\": \"otelcol-contrib\", \"service.version\": \"0.131.0\"}, \"otelcol.component.id\": \"debug\", \"otelcol.component.kind\": \"exporter\", \"otelcol.signal\": \"traces\", \"resource spans\": 6, \"spans\": 63}\n2025-08-19T18:28:51.634Z\tinfo\tLogs\t{\"resource\": {\"service.instance.id\": \"2e7d82f3-fb74-414f-a45c-c6b01d1a215a\", \"service.name\": \"otelcol-contrib\", \"service.version\": \"0.131.0\"}, \"otelcol.component.id\": \"debug\", \"otelcol.component.kind\": \"exporter\", \"otelcol.signal\": \"logs\", \"resource logs\": 2, \"log records\": 2}\n2025-08-19T18:28:51.697Z\tinfo\tTraces\t{\"resource\": {\"service.instance.id\": \"2e7d82f3-fb74-414f-a45c-c6b01d1a215a\", \"service.name\": \"otelcol-contrib\", \"service.version\": \"0.131.0\"}, \"otelcol.component.id\": \"debug\", \"otelcol.component.kind\": \"exporter\", \"otelcol.signal\": \"traces\", \"resource spans\": 7, \"spans\": 65}\n2025/08/19 18:28:51 failed to upload metrics: reader collect and export timeout: rpc error: code = Unavailable desc = connection error: desc = \"error reading server preface: http2: failed reading the frame payload: http2: frame too large, note that the frame header looked like an HTTP/1.1 header\"\n2025-08-19T18:28:51.935Z\tinfo\tLogs\t{\"resource\": {\"service.instance.id\": \"2e7d82f3-fb74-414f-a45c-c6b01d1a215a\", \"service.name\": \"otelcol-contrib\", \"service.version\": \"0.131.0\"}, \"otelcol.component.id\": \"debug\", \"otelcol.component.kind\": \"exporter\", \"otelcol.signal\": \"logs\", \"resource logs\": 9, \"log records\": 35}\n2025-08-19T18:28:52.125Z\tinfo\tTraces\t{\"resource\": {\"service.instance.id\": \"2e7d82f3-fb74-414f-a45c-c6b01d1a215a\", \"service.name\": \"otelcol-contrib\", \"service.version\": \"0.131.0\"}, \"otelcol.component.id\": \"debug\", \"otelcol.component.kind\": \"exporter\", \"otelcol.signal\": \"traces\", \"resource spans\": 3, \"spans\": 15}\n2025-08-19T18:28:52.427Z\tinfo\tTraces\t{\"resource\": {\"service.instance.id\": \"2e7d82f3-fb74-414f-a45c-c6b01d1a215a\", \"service.name\": \"otelcol-contrib\", \"service.version\": \"0.131.0\"}, \"otelcol.component.id\": \"debug\", \"otelcol.component.kind\": \"exporter\", \"otelcol.signal\": \"traces\", \"resource spans\": 10, \"spans\": 60}\n2025-08-19T18:28:52.641Z\tinfo\tLogs\t{\"resource\": {\"service.instance.id\": \"2e7d82f3-fb74-414f-a45c-c6b01d1a215a\", \"service.name\": \"otelcol-contrib\", \"service.version\": \"0.131.0\"}, \"otelcol.component.id\": \"debug\", \"otelcol.component.kind\": \"exporter\", \"otelcol.signal\": \"logs\", \"resource logs\": 2, \"log records\": 8}\n2025-08-19T18:28:52.967Z\tinfo\tLogs\t{\"resource\": {\"service.instance.id\": \"2e7d82f3-fb74-414f-a45c-c6b01d1a215a\", \"service.name\": \"otelcol-contrib\", \"service.version\": \"0.131.0\"}, \"otelcol.component.id\": \"debug\", \"otelcol.component.kind\": \"exporter\", \"otelcol.signal\": \"logs\", \"resource logs\": 11, \"log records\": 32}\n2025-08-19T18:28:53.515Z\tinfo\tLogs\t{\"resource\": {\"service.instance.id\": \"2e7d82f3-fb74-414f-a45c-c6b01d1a215a\", \"service.name\": \"otelcol-contrib\", \"service.version\": \"0.131.0\"}, \"otelcol.component.id\": \"debug\", \"otelcol.component.kind\": \"exporter\", \"otelcol.signal\": \"logs\", \"resource logs\": 1, \"log records\": 1}\n2025-08-19T18:28:54.014Z\tinfo\tLogs\t{\"resource\": {\"service.instance.id\": \"2e7d82f3-fb74-414f-a45c-c6b01d1a215a\", \"service.name\": \"otelcol-contrib\", \"service.version\": \"0.131.0\"}, \"otelcol.component.id\": \"debug\", \"otelcol.component.kind\": \"exporter\", \"otelcol.signal\": \"logs\", \"resource logs\": 11, \"log records\": 26}\n2025-08-19T18:28:57.833Z\tinfo\tMetrics\t{\"resource\": {\"service.instance.id\": \"2e7d82f3-fb74-414f-a45c-c6b01d1a215a\", \"service.name\": \"otelcol-contrib\", \"service.version\": \"0.131.0\"}, \"otelcol.component.id\": \"debug\", \"otelcol.component.kind\": \"exporter\", \"otelcol.signal\": \"metrics\", \"resource metrics\": 1, \"metrics\": 29, \"data points\": 34}\n2025-08-19T18:29:00.050Z\tinfo\tTraces\t{\"resource\": {\"service.instance.id\": \"2e7d82f3-fb74-414f-a45c-c6b01d1a215a\", \"service.name\": \"otelcol-contrib\", \"service.version\": \"0.131.0\"}, \"otelcol.component.id\": \"debug\", \"otelcol.component.kind\": \"exporter\", \"otelcol.signal\": \"traces\", \"resource spans\": 2, \"spans\": 351}\n2025-08-19T18:29:00.889Z\tinfo\tLogs\t{\"resource\": {\"service.instance.id\": \"2e7d82f3-fb74-414f-a45c-c6b01d1a215a\", \"service.name\": \"otelcol-contrib\", \"service.version\": \"0.131.0\"}, \"otelcol.component.id\": \"debug\", \"otelcol.component.kind\": \"exporter\", \"otelcol.signal\": \"logs\", \"resource logs\": 1, \"log records\": 2}\n2025-08-19T18:29:01.097Z\tinfo\tTraces\t{\"resource\": {\"service.instance.id\": \"2e7d82f3-fb74-414f-a45c-c6b01d1a215a\", \"service.name\": \"otelcol-contrib\", \"service.version\": \"0.131.0\"}, \"otelcol.component.id\": \"debug\", \"otelcol.component.kind\": \"exporter\", \"otelcol.signal\": \"traces\", \"resource spans\": 10, \"spans\": 50}\n2025-08-19T18:29:01.338Z\tinfo\tTraces\t{\"resource\": {\"service.instance.id\": \"2e7d82f3-fb74-414f-a45c-c6b01d1a215a\", \"service.name\": \"otelcol-contrib\", \"service.version\": \"0.131.0\"}, \"otelcol.component.id\": \"debug\", \"otelcol.component.kind\": \"exporter\", \"otelcol.signal\": \"traces\", \"resource spans\": 8, \"spans\": 55}\n2025-08-19T18:29:01.550Z\tinfo\tTraces\t{\"resource\": {\"service.instance.id\": \"2e7d82f3-fb74-414f-a45c-c6b01d1a215a\", \"service.name\": \"otelcol-contrib\", \"service.version\": \"0.131.0\"}, \"otelcol.component.id\": \"debug\", \"otelcol.component.kind\": \"exporter\", \"otelcol.signal\": \"traces\", \"resource spans\": 10, \"spans\": 50}\n2025-08-19T18:29:01.631Z\tinfo\tLogs\t{\"resource\": {\"service.instance.id\": \"2e7d82f3-fb74-414f-a45c-c6b01d1a215a\", \"service.name\": \"otelcol-contrib\", \"service.version\": \"0.131.0\"}, \"otelcol.component.id\": \"debug\", \"otelcol.component.kind\": \"exporter\", \"otelcol.signal\": \"logs\", \"resource logs\": 1, \"log records\": 1}\n2025/08/19 18:29:01 failed to upload metrics: reader collect and export timeout: rpc error: code = Unavailable desc = connection error: desc = \"error reading server preface: http2: failed reading the frame payload: http2: frame too large, note that the frame header looked like an HTTP/1.1 header\"\n2025-08-19T18:29:01.779Z\tinfo\tTraces\t{\"resource\": {\"service.instance.id\": \"2e7d82f3-fb74-414f-a45c-c6b01d1a215a\", \"service.name\": \"otelcol-contrib\", \"service.version\": \"0.131.0\"}, \"otelcol.component.id\": \"debug\", \"otelcol.component.kind\": \"exporter\", \"otelcol.signal\": \"traces\", \"resource spans\": 10, \"spans\": 49}\n2025-08-19T18:29:01.973Z\tinfo\tLogs\t{\"resource\": {\"service.instance.id\": \"2e7d82f3-fb74-414f-a45c-c6b01d1a215a\", \"service.name\": \"otelcol-contrib\", \"service.version\": \"0.131.0\"}, \"otelcol.component.id\": \"debug\", \"otelcol.component.kind\": \"exporter\", \"otelcol.signal\": \"logs\", \"resource logs\": 17, \"log records\": 78}\n2025-08-19T18:29:01.993Z\tinfo\tTraces\t{\"resource\": {\"service.instance.id\": \"2e7d82f3-fb74-414f-a45c-c6b01d1a215a\", \"service.name\": \"otelcol-contrib\", \"service.version\": \"0.131.0\"}, \"otelcol.component.id\": \"debug\", \"otelcol.component.kind\": \"exporter\", \"otelcol.signal\": \"traces\", \"resource spans\": 9, \"spans\": 63}\n2025-08-19T18:29:02.203Z\tinfo\tTraces\t{\"resource\": {\"service.instance.id\": \"2e7d82f3-fb74-414f-a45c-c6b01d1a215a\", \"service.name\": \"otelcol-contrib\", \"service.version\": \"0.131.0\"}, \"otelcol.component.id\": \"debug\", \"otelcol.component.kind\": \"exporter\", \"otelcol.signal\": \"traces\", \"resource spans\": 6, \"spans\": 26}\n2025-08-19T18:29:02.282Z\tinfo\tLogs\t{\"resource\": {\"service.instance.id\": \"2e7d82f3-fb74-414f-a45c-c6b01d1a215a\", \"service.name\": \"otelcol-contrib\", \"service.version\": \"0.131.0\"}, \"otelcol.component.id\": \"debug\", \"otelcol.component.kind\": \"exporter\", \"otelcol.signal\": \"logs\", \"resource logs\": 2, \"log records\": 5}\n2025-08-19T18:29:02.415Z\tinfo\tTraces\t{\"resource\": {\"service.instance.id\": \"2e7d82f3-fb74-414f-a45c-c6b01d1a215a\", \"service.name\": \"otelcol-contrib\", \"service.version\": \"0.131.0\"}, \"otelcol.component.id\": \"debug\", \"otelcol.component.kind\": \"exporter\", \"otelcol.signal\": \"traces\", \"resource spans\": 9, \"spans\": 74}\n2025-08-19T18:29:02.625Z\tinfo\tTraces\t{\"resource\": {\"service.instance.id\": \"2e7d82f3-fb74-414f-a45c-c6b01d1a215a\", \"service.name\": \"otelcol-contrib\", \"service.version\": \"0.131.0\"}, \"otelcol.component.id\": \"debug\", \"otelcol.component.kind\": \"exporter\", \"otelcol.signal\": \"traces\", \"resource spans\": 4, \"spans\": 34}\n2025-08-19T18:29:02.851Z\tinfo\tTraces\t{\"resource\": {\"service.instance.id\": \"2e7d82f3-fb74-414f-a45c-c6b01d1a215a\", \"service.name\": \"otelcol-contrib\", \"service.version\": \"0.131.0\"}, \"otelcol.component.id\": \"debug\", \"otelcol.component.kind\": \"exporter\", \"otelcol.signal\": \"traces\", \"resource spans\": 15, \"spans\": 37}\n2025-08-19T18:29:02.851Z\tinfo\tLogs\t{\"resource\": {\"service.instance.id\": \"2e7d82f3-fb74-414f-a45c-c6b01d1a215a\", \"service.name\": \"otelcol-contrib\", \"service.version\": \"0.131.0\"}, \"otelcol.component.id\": \"debug\", \"otelcol.component.kind\": \"exporter\", \"otelcol.signal\": \"logs\", \"resource logs\": 1, \"log records\": 1}\n2025-08-19T18:29:03.132Z\tinfo\tLogs\t{\"resource\": {\"service.instance.id\": \"2e7d82f3-fb74-414f-a45c-c6b01d1a215a\", \"service.name\": \"otelcol-contrib\", \"service.version\": \"0.131.0\"}, \"otelcol.component.id\": \"debug\", \"otelcol.component.kind\": \"exporter\", \"otelcol.signal\": \"logs\", \"resource logs\": 14, \"log records\": 56}\n2025-08-19T18:29:03.614Z\tinfo\tLogs\t{\"resource\": {\"service.instance.id\": \"2e7d82f3-fb74-414f-a45c-c6b01d1a215a\", \"service.name\": \"otelcol-contrib\", \"service.version\": \"0.131.0\"}, \"otelcol.component.id\": \"debug\", \"otelcol.component.kind\": \"exporter\", \"otelcol.signal\": \"logs\", \"resource logs\": 1, \"log records\": 1}\n2025-08-19T18:29:03.947Z\tinfo\tLogs\t{\"resource\": {\"service.instance.id\": \"2e7d82f3-fb74-414f-a45c-c6b01d1a215a\", \"service.name\": \"otelcol-contrib\", \"service.version\": \"0.131.0\"}, \"otelcol.component.id\": \"debug\", \"otelcol.component.kind\": \"exporter\", \"otelcol.signal\": \"logs\", \"resource logs\": 7, \"log records\": 28}\n2025-08-19T18:29:04.214Z\tinfo\tLogs\t{\"resource\": {\"service.instance.id\": \"2e7d82f3-fb74-414f-a45c-c6b01d1a215a\", \"service.name\": \"otelcol-contrib\", \"service.version\": \"0.131.0\"}, \"otelcol.component.id\": \"debug\", \"otelcol.component.kind\": \"exporter\", \"otelcol.signal\": \"logs\", \"resource logs\": 1, \"log records\": 1}\n2025-08-19T18:29:04.779Z\tinfo\tLogs\t{\"resource\": {\"service.instance.id\": \"2e7d82f3-fb74-414f-a45c-c6b01d1a215a\", \"service.name\": \"otelcol-contrib\", \"service.version\": \"0.131.0\"}, \"otelcol.component.id\": \"debug\", \"otelcol.component.kind\": \"exporter\", \"otelcol.signal\": \"logs\", \"resource logs\": 2, \"log records\": 20}\n2025-08-19T18:29:05.688Z\twarn\tgrpc@v1.74.2/clientconn.go:1414\t[core] [Channel #1 SubChannel #37]grpc: addrConn.createTransport failed to connect to {Addr: \"10.96.63.99:4318\", ServerName: \"otel-collector:4318\", }. Err: connection error: desc = \"error reading server preface: http2: failed reading the frame payload: http2: frame too large, note that the frame header looked like an HTTP/1.1 header\"\t{\"resource\": {\"service.instance.id\": \"2e7d82f3-fb74-414f-a45c-c6b01d1a215a\", \"service.name\": \"otelcol-contrib\", \"service.version\": \"0.131.0\"}, \"grpc_log\": true}\n"
        },
        {
            "role": "assistant",
            "content": "```\nsubmit(\"Yes\")\n```"
        },
        {
            "role": "env",
            "content": "1"
        }
    ],
    "results": {
        "Detection Accuracy": "Correct",
        "TTD": 31.81894588470459,
        "steps": 5,
        "in_tokens": 40162,
        "out_tokens": 106
    }
}